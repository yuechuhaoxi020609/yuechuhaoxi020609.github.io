
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":" I am currently studying at Huazhong University of Science and Technology, and also working as an undergraduate research assistant at the PKU CoRe Lab. My research interests include Computational Cognitive Science, Artificial Intelligence and Reinforcement Learning.\nDownload my Resumé here.\n","date":1681430400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1681430400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://example.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am currently studying at Huazhong University of Science and Technology, and also working as an undergraduate research assistant at the PKU CoRe Lab. My research interests include Computational Cognitive Science, Artificial Intelligence and Reinforcement Learning.","tags":null,"title":"Haofei Hou","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://example.com/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Haofei Hou"],"categories":["Summary"],"content":"Abstract humans possess a high social intelligence—the intelligence that senses social events, infers the goals and intents of others, and facilitates social interaction. cognitive science standpoint: social perception, theory of mind (ToM), and social interaction. Dawn of Artificial Social Intelligence efforts towards human-like intelligence can be divided into physical intelligence and social intelligence , analogous to the developmental psychology ideas of intuitive physics and intuitive psychology Unique challenges of context ASI is distinct and challenging compared to our physical understanding of the world; it is highly context-dependent Here, context could be as large as culture and common sense or as little as two friends’ shared experiences it begins with nonverbal communication Overview of the article In Section 2, which covers social perception, theory of mind, and social interaction, we begin with experimental evidence and theoretical hypotheses of human social intelligence from the standpoint of cognitive science. In Section 3, we present the AI community’s computational counterpart, focused on social perception, theory of mind, and social interaction, with an added topic on social robot and cognitive architectures. In Section 4, we explore significant challenges that impede the development of the ASI and recommend potential future trends. Human Social Intelligence We concentrate on the three most important aspects of social intelligence: social perception, ToM, and social interaction. Social perception is the basis for ToM and social interaction. It consists primarily of the perception of social features, such as animacy and agency, and provides low-level, automatic, instantaneous, and non-conscious visual perception. ToM, in contrast, is concerned with sophisticated, analytic, and logical cognitive reasoning, involving a general cognitive system with several essential components, including belief, intent, and desire. Social interaction emphasizes more multi-agent interactive activities, such as communication and cooperation, than social perception and ToM. Social perception Heider-Simmel stimuli Even when told explicitly that these are merely simple shapes, participants still make a rapid, spontaneous, and consistent perception of animate social agents with various complex mental states, including desires, goals, emotions, personalities, and coalitions. The Heider-Simmel experiment demonstrates two essential aspects of human social perception: the perception of animacy and agency. Animacy denotes that the perceived entities are animate as opposed to inanimate (e.g., physical objects), whereas agency refers to animate beings who are goal-oriented and capable of planning to achieve their goals rationally and efficiently. Animacy In this experiment, participants were shown two small squares separated by several inches and arranged in a line. In the first scenario, the first square (A) moves in a straight line until it reaches the second square (B), at which point A stops moving and B begins moving in the same direction (also called the launching effect). In case two, the first square (A) approaches the second square (B). While A approaches, B moves away from A quickly and stops when it is several inches away again. In the first instance, observers observe A physically causing B’s motion in the second case, A and B are perceived as alive with their own intentions. spatiotemporal parameters mediate causal perceptions, such as relative velocity, speed–mass interaction, path length, and spatial and temporal gap. spatiotemporal parameters mediate causal perceptions, such as relative velocity, speed–mass interaction, path length, and spatial and temporal gap. Agency An agent is rationally controlled because it has an internal energy source, whereas an object is not. The perception of agency is frequently studied in tandem with animacy for more complex social phenomena. Gao et al[81] study a particularly salient form of perceived animacy and agency via tasks based on dynamic visual search (the Find-the-Chase task) They used two cues to evaluate the objective accuracy of such perceptions: (1) chasing subtlety—the degree to which the wolf deviates from a perfectly heat-seeking pursuit, and (2) directionality—whether and how the shapes face each other. the researchers discovered that temporal dynamics could lead the visual system to either construct or actively reject interpretations of chasing van Buren et al[84] depict one disc (the “wolf”) pursuing another disc (the “sheep”) amidst several distractor discs that are moving. In the Unconnected condition, both lines connected distractors in pairs. In the Connected condition, however, one line connected the wolf to a distractor, and the other line connected the sheep to a different distractor. Observers in the Connected condition were markedly less likely to describe these behaviors in terms of mental state. According to the outcomes of their experiments, …","date":1681430400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1681430400,"objectID":"dd18eae3f9092f1c3bf8080798608bbd","permalink":"https://example.com/post/tom-summary/","publishdate":"2023-04-14T00:00:00Z","relpermalink":"/post/tom-summary/","section":"post","summary":"A quick read of ToM's literature around article \"Artificial Social Intelligence - A Comparative and Holistic View\", this blog is the summary. Humans are distinguished from their closest primate cousins by their social cognitive skills as opposed to their physical counterparts. Article believe that artificial social intelligence (ASI) will play a crucial role in shaping the future of artificial intelligence (AI). This article begins with a review of ASI from a cognitive science standpoint, including social perception, theory of mind (ToM), and social interaction. Next, article examine the recently-emerged computational counterpart in the AI community. Finally, this article provide an in-depth discussion on topics related to ASI.","tags":["ToM","AI","ASI"],"title":"Artificial Social Intelligence - A Comparative and Holistic View","type":"post"},{"authors":["Haofei Hou"],"categories":["Summary"],"content":"Policy Gradient $$G(\\tau)=\\sum_{t=0}^{T-1}\\gamma^{t}r_{t}$$ $$J(\\theta)=\\mathbb{E}_S\\Big[V_\\pi(S)\\Big].$$ Policy gradient theorem\n$$\\nabla J_\\theta=\\frac{1}{N}\\sum_{n=1}^N\\sum_{t=0}^{T_n-1}G_t(\\tau)\\nabla logp_\\theta(a_t|s_t)$$ SGD\nhigher variance\nno bias\nproof: PPOxFamily/chapter1_supp_trpo.pdf at main · opendilab/PPOxFamily (github.com)\nActor-Critic $$Q_\\phi\\left(s_r,a_t\\right)=E_{t-\\pi}\\left[\\sum_{l=0}^\\infty\\gamma^l r^{t+l}\\right]$$ $$\\nabla J_\\theta=\\dfrac{1}{N}\\sum_{n=1}^{N}\\sum_{t=0}^{T_a-1}Q_\\theta(s_t,a_t)\\nabla logp_\\theta(a_t|s_t)$$ not unbiased\nlower variance\nA2C $$\\nabla J_\\theta=\\frac{1}{N}\\sum_{n=1}^N\\sum_{t=0}^{T_n-1}(G_t(\\tau)-V_\\phi(s_t))\\nabla logp_\\theta(a_t|s_t)$$ no bias\nlower variance\nwhy cut the baseline function: PPOxFamily/chapter1_supp_a2c.pdf at main · opendilab/PPOxFamily (github.com)\nTRPO $$\\eta(\\pi)=E_{s_0,a_0,\\dots}\\left[\\sum\\limits_{t=0}^\\infty\\gamma^tr_t\\right]\\quad $$ $$\\rho_{\\pi}(s)=P\\left(s_{0}=s\\right)+\\gamma P\\left(s_{1}=s\\right)+{\\gamma}^{2}P\\left(s_{2}=s\\right)+\\ldots\\quad$$ $$\\eta(\\tilde{\\pi})=\\eta(\\pi)+\\sum_s\\rho_{\\tilde{\\pi}}(s)\\sum_a\\tilde{\\pi}(a\\mid s)A_{\\pi}(s,a)\t$$ $$\\eta(\\tilde{\\pi})\\approx L_{\\pi}(\\tilde{\\pi})=\\eta(\\pi)+\\sum_s\\rho_{\\pi}(s)\\sum_a\\tilde{\\pi}(a\\mid s)A_{\\pi}(s,a)$$ further\n$$\\eta(\\tilde{\\pi})\\geq L_{\\pi}(\\tilde{\\pi})-\\dfrac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha$$ $$where\\ \\alpha=\\operatorname*{max}_{s}D_{\\mathrm{KL}}(\\pi(\\cdot\\mid s)\\Vert{\\tilde{\\pi}}(\\cdot\\mid s)),\\epsilon=\\operatorname*{{max}}_{s,a}\\Big|A_{\\pi}(s,a)\\Big|$$ Another form\n$$\\underset{\\theta}{\\mathrm{maximize}}\\quad\\hat{\\mathbb{E}}_t\\bigg[\\dfrac{\\pi_\\theta(a_t\\mid s_t)}{\\pi_{\\theta_\\mathrm{old}}(a_t\\mid s_{t})}\\hat{A}_t\\bigg]\\quad$$ $$where\\ \\hat{\\mathbb{E}}_{t}[\\mathrm{KL}[\\pi_{\\theta\\text{old}}(\\cdot\\mid s_{t}),\\pi_{\\theta}(\\cdot\\mid s_t)]]\\leq\\delta.$$ proof: PPOxFamily/chapter1_supp_trpo.pdf at main · opendilab/PPOxFamily (github.com)\nPPO(Proximal Policy Optimization) $$\\mathbb{E}_t[\\min(\\dfrac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_k}(a_t|s_{t})}A^{\\theta_k}(s_r,a_{t}),\\text{clip}(\\dfrac{\\pi_\\vartheta(a_t|s_i)}{\\pi_{\\vartheta_k}(a_r|s_i)},1-\\epsilon,1+\\epsilon)A^{\\theta_4}(s_r,a_t))]$$ article: https://arxiv.org/pdf/1707.06347\nMoving towards Real decision Problems (Multi-Agent System) Biological swarm intelligence (population game, co-evolution) Machine swarm intelligence (division of labor and cooperation, each performs his own duties) GameAI (Cooperative mission objective) Loss Function The basic theory of multi-agent cooperation General setting for multi-agent decision making Joint action $(a^1, a^2, \\dots, a^N) \\in A^1 \\times A^2 \\times \\dots \\times A^N$ Agent-wise reward $R^i = R^i(s,\\ a^{-i},\\ a^i), \\ a^{-i}=(a^1,\\dots,a^N)\\setminus a^i$ Individual optimality: Maximizes the cumulative discount rewards of a single agent But team optimality is not a superposition of individual optimality, and collaboration, competition, and more complexity may occur. Formally, Dec-POMDP: $M = \u0026lt;I, S, {A_i}, P, R, {\\Omega_i}, O, h \u0026gt;$ $I$, the set of agents $I$ the set of states with initial state $s_0$ $A_i$, the set of actions for agent $i$, with $A = \\times_iA_i$ the set of joint actions $P$, the state transition probabilities: $P(s’| \\ s,\\ a)$, the probability of the environment transitioning to state $s’$ given it was in state $s$ and agents took actions $a$ $R$, the global reward function: $R(s,\\ a)$, the immediate reward the system receives for being in state $s$ and agents taking actions $a$ $\\Omega_i$, the set of observations for agent $i$, with $\\Omega = \\times_i\\Omega_i$ the set of joint observations $O$, the observation probabilities: $O(o| \\ s,\\ a)$, the probability of agents seeing observations $o$, given the state is $s$ and agents take actions $a$ $h$, the horizon, whether infinite or if finite, a positive integer when $h$ is infinite a discount factor, $0 ≤ \\gamma \u0026lt; 1$ , is used CTDE(centealized training and decentralized execution) Value decomposition IGM: individual -global -max\n$$\\operatorname{arg}\\operatorname*{max}_{a\\in\\mathcal{A}}Q_{t o v}(s,a)=\\left(\\underset{a_{1}\\in\\mathcal{A}}{arg\\operatorname*{max}}Q_{1}\\left(s_{1},a_{1}\\right),\\ldots,\\underset{a_{n}\\in\\mathbb{a}}{a_{n}}\\left(s_{n},a_{n}\\right)\\right)$$ TD-Learning\nVDN(value decomposition networks): The value function of each agent is integrated to obtain a joint action value function\nQMIX:\nIf $\\dfrac{\\partial Q_{tot}}{\\partial Q_a}\\geq0,\\forall a.$, IGM must be true. For each agent a, there is one agent network that represents its individual value function $Q_a(\\tau_a , u_a )$. We represent agent networks as DRQNs that receive the current individual observation $o^a_t$ and the last action $u^a_{t−1}$ as input at each time step, as shown in Figure c. The mixing network is a feed-forward neural network that takes the agent network outputs as input and mixes them monotonically, producing the values of $Q_{tot}$, as shown in Figure a. To enforce the monotonicity constraint of $\\dfrac{\\partial Q_{tot}}{\\partial …","date":1681257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1681257600,"objectID":"ecfb49b9196654e3e22aa3b9650e88fe","permalink":"https://example.com/post/ppoxfamliy/","publishdate":"2023-04-12T00:00:00Z","relpermalink":"/post/ppoxfamliy/","section":"post","summary":"Summary about MARL in class PPOxFamliy","tags":["MARL","PPO"],"title":"PPOxFamliy MARL","type":"post"},{"authors":["Haofei Hou"],"categories":["Summary"],"content":" Large Language Models: GPT BERT Multi-Modal Models: CLIP Reasoning RT-1: “RT-1: Robotics Transformer for Real-World Control at Scale”, arXiv, Dec 2022. [Paper] [GitHub] [Website]\nCode-As-Policies: “Code as Policies: Language Model Programs for Embodied Control”, arXiv, Sept 2022. [Paper] [Colab] [Website]\nSay-Can: “Do As I Can, Not As I Say: Grounding Language in Robotic Affordances”, arXiv, Apr 2021. [Paper] [Colab] [Website]\nSocratic: “Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language”, arXiv, Apr 2021. [Paper] [Pytorch Code] [Website]\nPIGLeT: “PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World”, ACL, Jun 2021. [Paper] [Pytorch Code] [Website]\nPlanning LM-Nav: “Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action”, arXiv, July 2022. [Paper] [Pytorch Code] [Website]\nLLM: GPT-3 parse textual instructions into a sequence of landmarks ($t$). VLM: CLIP compute likelihood that image $i_k$ corresponds to the string $t$ VNM: ViNG uses a goal-conditioned distance function to infer connectivity between the set of raw observations and constructs a topological graph Navigation InnerMonlogue: “Inner Monologue: Embodied Reasoning through Planning with Language Models”, arXiv, July 2022. [Paper] [Website]\nsystem Inner Monologue chains together these various components (perception models, robotic skills, and human feedback) in a shared language prompt, enabling it to successfully perform user instructions. Inner Monologue(Left): InstructGPT pick-and-place primitive: CLIPort Manipulation Housekeep: “Housekeep: Tidying Virtual Households using Commonsense Reasoning”, arXiv, May 2022. [Paper] [Pytorch Code] [Website]\nNew benchmark: In Housekeep, an embodied agent must tidy a house by rearranging misplaced objects without explicit instructions specifying which objects need to be rearranged A new dataset for an object (e.g. salt-shaker), a room (e.g. kitchen) in context, to classify into the following categories: misplaced, correct, implausible. Each Housekeep episode is created by instantiating 7-10 objects within a scene, out of which 3-5 objects are misplaced and the remaining are placed correct Object Room $[OR] – P(room|object)$ : Generate compatibility scores for rooms for a given object. Object Room Receptacle $[ORR] – P(receptacle|object, room)$: Generate compatibility scores for receptacles within a given room and for a given object Method1: Finetuning by Contrastive Matching (CM). We train a 3-layered MLP on top of language embeddings and compute pairwise cosine similarity between any two embeddings Method2: Zero-Shot Ranking via MLM (ZS-MLM). LLM: BERT for word embeddings MLM(Masked Language Model) Similarity calculation LID: “Pre-Trained Language Models for Interactive Decision-Making”, arXiv, Feb 2022. [Paper] [Pytorch Code] [Website]\nFine-tuning Mid-level actions LLM As a part of policy network. ZSP: “Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents”, ICML, Jan 2022. [Paper] [Pytorch Code] [Website]\nVirtualHome Executability Correctness Planning LM (Causal LLM): GPT-3 $\\hat{a}$ Translation LM(Masked LLM): BERT for each admissible environment action $a_e$ $max\\ C(f(\\hat{a}), f(a_e)) = \\frac{f(\\hat{a})\\cdot f(a_e)}{||f(\\hat{a})||\\ ||f(a_e)||}$ Mid-level actions Manipulation DIAL:“Robotic Skill Acquistion via Instruction Augmentation with Vision-Language Models”, “arXiv, Nov 2022”, [Paper] [Website]\nCLIP Model Few Shot/Fine-tuning Manipulation trajectories CLIP-Fields:“CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory”, “arXiv, Oct 2022”, [Paper] [PyTorch Code] [Website]\nweakly supervised semantic neural fields, called CLIP-Fields. dataset comes from LLM/MMM $g(x, y, z) : R_3 → R_d$ serves as a generic differentiable spatial database. trained. CLIP BERT EVALUATION: Instance and semantic segmentation in scene images. for visual encoder EVALUATION: Semantic Navigation on Robot with CLIP-Fields as Semantic-Spatial Memory. maximizing their similarity. LaTTe: “LaTTe: Language Trajectory TransformEr”, arXiv, Aug 2022. [Paper] [TensorFlow Code] [Website]\nBERT CLIP trained and evaluated the model described in Section III over a dataset containing 100k examples of procedurally generated trajectory modification. Trajectory reshaping obeying user’s constraints. Robots Enact Malignant Stereotypes: “Robots Enact Malignant Stereotypes”, FAccT, Jun 2022. [Paper] [Pytorch Code] [Website] [Washington Post] [Wired] (code access on request)\nexperiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy. Ethical experiments on LLM/MMM ATLA: “Leveraging Language for Accelerated Learning of Tool Manipulation”, CoRL, Jun 2022. [Paper]\ncombining linguistic information and meta-learning significantly accelerates tool learning tools $ \\mathscr{T} = { \\tau_i }{i=1}^K$ corresponding language descriptions $L_i = {l{ij}}{j=1}^{N_i}\\ l{ij} …","date":1681084800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1681084800,"objectID":"63c2f496d8a3605deef0eb2ba6524b45","permalink":"https://example.com/post/llm-for-rl/","publishdate":"2023-04-10T00:00:00Z","relpermalink":"/post/llm-for-rl/","section":"post","summary":"Articles Summary about LLM/MMM for RL","tags":["LLM","RL"],"title":"LLM/MMM for RL","type":"post"},{"authors":["Haofei Hou"],"categories":["Summary"],"content":"Abstract “small data for big tasks” paradigm models of common sense We identify functionality, physics, intent, causality, and utility (FPICU) as the five core domains of cognitive AI with humanlike common sense. FPICU is concerned with the questions of “why” and “how,” beyond the dominant “what” and “where” framework for understanding vision. They are invisible in terms of pixels but nevertheless drive the creation, maintenance, and development of visual scenes. A Call for a Paradigm Shift in Vision and AI The classic definition of computer vision proposed by the pioneer David Marr is to look at “what” is “where.” Here, “what” refers to object recognition (object vision), and “where” denotes three-dimensional (3D) reconstruction and object localization (spatial vision) require large sets of labeled training data designed for special tasks, and lack a general understanding of common facts—that is, facts that are obvious to the average human adult—that describe how our physical and social worlds work. missing dimensions and the potential benefits of joint representation and joint inference. The concept of “darkness” is perpendicular to and richer than the meanings of “latent” or “hidden” used in vision and probabilistic modeling; darkness” is a measure of the relative difficulty of classifying an entity or inferring about a relationship based on how much invisible common sense needed beyond the visible appearance or geometry. Section 2: paper starts by revisiting a classic view of computer vision in terms of “what” and “where”, task-driven Section 3: In order to use “small data” to solve “big tasks,” we then identify and review five crucial axes of visual common sense: Functionality, Physics, perceived Intent, Causality, and Utility (FPICU). Causality Section 4: The application of causality (i.e., intuitive physics; ) Section 5: Functionality Section 6: infer intent Section 7: utility-driven In a series of studies, we demonstrate that these five critical aspects of “dark entities” and “dark relationships” indeed support various visual tasks beyond just classification. Vision: From Data-driven to Task-driven From a biological perspective, the majority of living creatures use a single (with multiple components) vision system to perform thousands of tasks. these results indicate that our biological vision system possesses a mechanism for perceiving object functionality (i.e., how an object can be manipulated as a tool) that is independent of the mechanism governing face recognition (and recognition of other objects) “What”: Task-centered Visual Recognition these approaches have left unclear how classification interacts with scene semantics and enables cognitive reasoning human vision organizes representations during the inference process even for “simple” categorical recognition tasks. scene categorization and the information-gathering process are constrained by these categorization tasks, suggesting a bidirectional interplay between the visual input and the viewer’s needs/tasks the representation of the same object can vary according to the planned task task-driven nature of scene categorization. “Where”: Constructing 3D Scenes as a Series of Tasks scene reconstruction from a single two-dimensional (2D) image is a well-known illposed problem; there may exist an infinite number of possible 3D configurations that match the projected 2D observed images enable agents to perform tasks by generating the best possible configuration in terms of functionality, physics, and object relationships. there is now abundant evidence that humans represent the 3D layout of a scene in a way that fundamentally differs from any current computer vision algorithms human vision is error-prone and distorted in terms of localization Grid cells encode a cognitive representation of Euclidean space, implying a different mechanism for perceiving and processing locations and directions. Xie et al. proposed a representational model for grid cells, in which the 2D self-position of an agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. how we navigate complex environments while remaining able at all times to return to an original location (i.e., homing) remains a mystery in biology and neuroscience. the task-dependent representation of space can shed some light. neither based on a stable 3D model of a scene nor a distorted one; instead, participants seemed to form a flat and task-dependent representation Beyond “What” and “Where”: Towards Scene Understanding with Humanlike Common Sense rich as videos and much sparser visual inputs To enable an artificial agent with similar capabilities, we call for joint reasoning algorithms on a joint representation that integrates (i) the “visible” traditional recognition and categorization of objects, scenes, actions, events, and so forth; and (ii) the “dark” higher level concepts of fluent, causality, …","date":1680393600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680393600,"objectID":"9312a8cd03c12aeaf4411e7c3bd91bee","permalink":"https://example.com/post/dark-beyond-deep/","publishdate":"2023-04-02T00:00:00Z","relpermalink":"/post/dark-beyond-deep/","section":"post","summary":"Propose a “small data for big tasks” paradigm, wherein a single artificial intelligence (AI) system is challenged to develop “common sense,” enabling it to solve a wide range of tasks with little training data. Identify functionality, physics, intent, causality, and utility (FPICU) as the five core domains of cognitive AI with humanlike common sense","tags":["CV","CoCoSci"],"title":"Dark, Beyond Deep - A Paradigm Shift to Cognitive AI with Humanlike Common Sense","type":"post"},{"authors":["Haofei Hou"],"categories":["Summary"],"content":"Introduction two different computational approaches to intelligence pattern recognition approach treats prediction as primary, usually in the context of a specific classification, regression, or control task. discovering features that have high-value states in common across a large, diverse set of training data. model building. Cognition is about using these models to understand the world, to explain, to imagine what could have happened that didn’t, or what could be true that isn’t, and then planning. prediction and explanation, is central to our view of human intelligence. pattern recognition can support model building, through “model-free” algorithms that learn through experience how to make essential inferences more computationally efficient What this article is not we believe that reverse engineering human intelligence can usefully inform AI and machine learning. avoiding cognitive or neural inspiration as well as claims of cognitive or neural plausibility is a approach to developing AI. But this article has little pertinence to this approach. Overview of the key ideas propose a set of core ingredients for building more human-like learning and thinking machines mainly in Section 4 Core ingredients of human intelligence Cognitive and neural inspiration in artificial intelligence behaviorist view Cognitive science PDP approach knowledge is thus distributed across the collection of units rather than localized as in most symbolic data structures. Neural network models and the PDP approach offer a view of the mind (and intelligence more broadly) that is sub-symbolic and often populated with minimal constraints and inductive biases to guide learning. Proponents of this approach maintain that many classic types of structured knowledge, such as graphs, grammars can be useful yet misleading metaphors for characterizing thought. Question neural networks have such broad application in machine vision, language, and control, and they can be trained to emulate the rule-like and structured behaviors that characterize cognition do we need more to develop truly human-like learning and thinking machines? How far can relatively generic neural networks bring us toward this goal? Challenges for building more human-like machines The Characters Challenge learning simple visual concepts People learn a lot more from a lot less, and capturing these human-level learning abilities in machines is the Characters Challenge Although humans and neural networks may perform equally well on the MNIST digit recognition task and other large-scale image classification tasks, it does not mean that they learn and think in the same way. two important differences between CNN and human in learning simple visual concepts people learn from fewer examples and they learn richer representations people learn more than how to do pattern recognition: they learn a concept, that is, a model of the class that allows their acquired knowledge to be flexibly applied in new ways. Some difficulty A single example of a new visual concept (red box) can be enough information to support the classification of new examples generation of new examples parsing an object into parts and relations generation of new concepts from related concepts. The Frostbite Challenge learning to play the Atari game Frostbite Failed on accomplishing a sub-goal (such as reaching an ice floe) and then safely proceed to the next sub-goal the policy are highly specialized for the games it was trained on considering the amount of experience required for learning non-professional humans can grasp the basics of the game after just a few minutes of play. people do this by inferring a general schema to describe the goals of the game and the object types and their interactions, using the kinds of intuitive theories, model-building abilities and model-based planning mechanisms we describe below. the game of Frostbite provides incremental rewards for reaching each active ice floe, providing the DQN with the relevant sub-goals for completing the larger task of building an igloo. Without these sub-goals, the DQN would have to take random actions until it accidentally builds an igloo and is rewarded for completing the entire level. Human is possible to figure out the higher-level goal of building an igloo without incremental feedback; sparse feedback is a source of difficulty in other Atari 2600 games such as Montezuma’s Revenge inflexible to changes in its inputs and goals. Changing the color or appearance of objects or changing the goals of the network would have devastating consequences on performance if the network is not retrained In contrast, people require little or no retraining or reconfiguration, adding new tasks and goals to their repertoire with relative ease. Humans as a result often have important domain-specific knowledge for these tasks, even before they ‘begin.’ The DQN is starting completely from scratch. How do we bring to bear rich prior knowledge to learn new tasks and solve new …","date":1679270400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1679270400,"objectID":"2f3eb87357daaf9b6e2c26b1090e73a2","permalink":"https://example.com/post/build-humanlike-machine/","publishdate":"2023-03-20T00:00:00Z","relpermalink":"/post/build-humanlike-machine/","section":"post","summary":"(1) build causal models of the world (2) ground learning in intuitive theories of physics and psychology (3) harness compositionality and learning-to-learn. Authors suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.","tags":["CoCoSci","AI"],"title":"Building machines that learn and think like people","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://example.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]