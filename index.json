
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":" I am currently studying at the College of Engineering, Peking University, under the supervision of Prof. Lecheng Ruan and Prof. Qining Wang. My academic work centers on Robotics and Artificial Intelligence, with a particular emphasis on their interdisciplinary applications. ","date":1745719200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1745719200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://yuechuhaoxi020609.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am currently studying at the College of Engineering, Peking University, under the supervision of Prof. Lecheng Ruan and Prof. Qining Wang. My academic work centers on Robotics and Artificial Intelligence, with a particular emphasis on their interdisciplinary applications.","tags":null,"title":"Haofei Hou","type":"authors"},{"authors":null,"categories":null,"content":"","date":1745719200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1745719200,"objectID":"e657abd0c2eddfabc9f3a385fbb1cb27","permalink":"https://yuechuhaoxi020609.github.io/authors/lechengruan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/lechengruan/","section":"authors","summary":"","tags":null,"title":"Lecheng Ruan","type":"authors"},{"authors":null,"categories":null,"content":"","date":1745719200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1745719200,"objectID":"0db70c57adf3bf9d10010e2e0eb10992","permalink":"https://yuechuhaoxi020609.github.io/authors/mengchencai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mengchencai/","section":"authors","summary":"","tags":null,"title":"Mengchen Cai","type":"authors"},{"authors":null,"categories":null,"content":"","date":1745719200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1745719200,"objectID":"e6fce986207ddcb35dd12e27cd52c9d4","permalink":"https://yuechuhaoxi020609.github.io/authors/qiningwang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/qiningwang/","section":"authors","summary":"","tags":null,"title":"Qining Wang","type":"authors"},{"authors":null,"categories":null,"content":"","date":1745719200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1745719200,"objectID":"da337905fdc5b1e03d71d6661d5d95a9","permalink":"https://yuechuhaoxi020609.github.io/authors/shunyizhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shunyizhao/","section":"authors","summary":"","tags":null,"title":"Shunyi Zhao","type":"authors"},{"authors":null,"categories":null,"content":" ","date":1745719200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1745719200,"objectID":"220bd4fe9b15dd8139c2a8f356fb1e68","permalink":"https://yuechuhaoxi020609.github.io/authors/wenduozhu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/wenduozhu/","section":"authors","summary":" ","tags":null,"title":"Wenduo Zhu","type":"authors"},{"authors":null,"categories":null,"content":" I received my B.Eng. in Biomedical Engineering from the Beijing Institute of Technology, Beijing, China, in 2020. I completed my M.Eng. in Biomedical Engineering at the same institution in 2023, focusing on vascular interventional surgical robots. And I am currently pursuing a Ph.D. at the College of Engineering, Peking University, with research interests in knowledge representation in robotics and predictive problems in orthopedics. In addition to my academic pursuits, I enjoy playing badminton, long-distance running, and basketball in my free time. ","date":1727316000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1727316000,"objectID":"2b74dbc0e3be0a39c6789c0c3889ebb5","permalink":"https://yuechuhaoxi020609.github.io/authors/fanxumeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/fanxumeng/","section":"authors","summary":"I received my B.Eng. in Biomedical Engineering from the Beijing Institute of Technology, Beijing, China, in 2020. I completed my M.Eng. in Biomedical Engineering at the same institution in 2023, focusing on vascular interventional surgical robots.","tags":null,"title":"Fanxu Meng","type":"authors"},{"authors":null,"categories":null,"content":"","date":1727316000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1727316000,"objectID":"eb4560783b83012883bfff7d18f1ef3e","permalink":"https://yuechuhaoxi020609.github.io/authors/qiaoxu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/qiaoxu/","section":"authors","summary":"","tags":null,"title":"Qiao Xu","type":"authors"},{"authors":null,"categories":null,"content":"","date":1727316000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1727316000,"objectID":"5670d199fb96816e9a31053d88c998fc","permalink":"https://yuechuhaoxi020609.github.io/authors/yuzheshi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuzheshi/","section":"authors","summary":"","tags":null,"title":"Yu-Zhe Shi","type":"authors"},{"authors":null,"categories":null,"content":"","date":1727316000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1727316000,"objectID":"9603a3a59e25b0bf2922859ce96dfd5b","permalink":"https://yuechuhaoxi020609.github.io/authors/zhangqianbi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhangqianbi/","section":"authors","summary":"","tags":null,"title":"Zhangqian Bi","type":"authors"},{"authors":null,"categories":null,"content":"","date":1714942800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1714942800,"objectID":"f29452ac48eb32fb1be70d4cc9e13f9f","permalink":"https://yuechuhaoxi020609.github.io/authors/xiangwei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xiangwei/","section":"authors","summary":"","tags":null,"title":"Xiang Wei","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://yuechuhaoxi020609.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Wenduo Zhu","Mengchen Cai","Haofei Hou","Shunyi Zhao","Lecheng Ruan","Qining Wang"],"categories":null,"content":"","date":1745719200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1745719200,"objectID":"39119161f446b9ffcfeaf1728122185f","permalink":"https://yuechuhaoxi020609.github.io/publication/mpc_prosthetic/","publishdate":"2025-04-27T02:00:00Z","relpermalink":"/publication/mpc_prosthetic/","section":"publication","summary":"This paper presents a novel framework for powered knee prostheses to achieve obstacle avoidance by directly integrating environmental information.","tags":["Prosthesis","HMI","Control"],"title":"Obstacle Avoidance for Knee Prostheses via Direct Integration of Environment Information","type":"publication"},{"authors":["Haofei Hou","Wenduo Zhu","Lecheng Ruan","Qining Wang"],"categories":null,"content":"","date":1745028000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1745028000,"objectID":"0efa3766959fbcb9bae2222f542dad30","permalink":"https://yuechuhaoxi020609.github.io/publication/humanipro/","publishdate":"2025-04-19T02:00:00Z","relpermalink":"/publication/humanipro/","section":"publication","summary":"We develop a model-free reinforcement learning framework that enables the prosthesis to adapt to diverse human movement patterns through cooperative policy learning.","tags":["Prosthesis","HMI"],"title":"Prosthetic Control by Learning: A Multi-Agent Cooperative Game Framework","type":"publication"},{"authors":["Yu-Zhe Shi","Fanxu Meng","Haofei Hou","Zhangqian Bi","Qiao Xu","Lecheng Ruan","Qining Wang"],"categories":null,"content":"","date":1727316000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1727316000,"objectID":"b603b9ec3606b91d5952217f4f5eaf87","permalink":"https://yuechuhaoxi020609.github.io/publication/autodsl_trans/","publishdate":"2024-09-26T02:00:00Z","relpermalink":"/publication/autodsl_trans/","section":"publication","summary":"We propose a framework to automate protocol translation for self-driving laboratories.","tags":["Embodied-AI","Knowledge","DSL","Lab-automation"],"title":"Expert-level protocol translation for self-driving labs","type":"publication"},{"authors":["Yu-Zhe Shi","Haofei Hou","Zhangqian Bi","Fanxu Meng","Xiang Wei","Lecheng Ruan","Qining Wang"],"categories":null,"content":"","date":1714942800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714942800,"objectID":"e19954b761e71445c7dac04a60dc2f3e","permalink":"https://yuechuhaoxi020609.github.io/publication/autodsl/","publishdate":"2024-08-05T00:00:00Z","relpermalink":"/publication/autodsl/","section":"publication","summary":"We introduce the AutoDSL framework to automate DSL-based constraint design across various domains.","tags":["Embodied-AI","Knowledge","DSL"],"title":"AutoDSL: Automated domain-specific language design for structural representation of procedures with constraints","type":"publication"},{"authors":null,"categories":null,"content":"Humans solve problems with ease for two primary reasons. For one, we can infer problem context by leveraging object-level prior (henceforth prior for brevity), such as semantics, affordance, and physics. For another, we can simplify the problem context to a task-oriented abstract representation (henceforth task abstraction for brevity), such as only considering the routine graph instead of the entire city map when traveling by subway.\nUnfortunately, priors are latent or inaccessible in many real-world problems, thus difficult to separate them from task abstraction. Here we introduce an experimental procedure to probe how people generate task abstraction without priors.\nWe devise a hierarchical generative model that accounts for human behavior of task abstraction from the first principles. We design experiments that require participants to solve different problems instructed with a goal (Exp1) or goal and constraint (Exp2) to testify model hypothesis. Model fitting on data of Exp1 significantly shows that task abstractions converge on the same family of goals regardless of variants, yet the policies for solving are diverse.\nThus, we further show that the diversity of policies may come from personal preference, independent of the generation of task abstraction, echoing the hierarchy of our model. In Exp2, variants of constraint generally do not transform task abstraction, defending the generality of our findings.\nWith the model hypothesis checked and the comparison with alternate accounts, we suggest that with the absence of priors, people may construct task abstraction from common sense on tasks in a top-down manner. Our findings may lead to general discussions on \u0026#34;knowing how to solve an unknown problem before starting to solve it\u0026#34;.\nMetaQ and PPO are chosen to be taught from scratch in the ProbSol environment. The MetaQ agent fails to complete all tasks across all trajectories, but the PPO agent succeeds in the two tasks. The success rate indicates that the reinforcement learning agent is incapable of learning or producing the appropriate strategy to complete the task. It does not learn anything but rather remembers the answer based on the gradient. Thus, we conclude that the gradient-learning-based RL cannot generate task abstraction for the ProbSol environment, let alone produce viable strategies.\nBesides hierarchical modeling, we employ imitation learning to study the properties of task abstraction, to maintain the generality of our analysis. We find that the behavior of BC agent is to some extent consistent with that of human subjects. We view the activation vector of the penultimate hidden layer as the feature of task abstraction extracted by the BC agent. In the same task, features of different policies cannot be separated from each other. These observations by imitation learning echo our findings by the hierarchical modeling. ","date":1677628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677628800,"objectID":"d4e519e09c8e215f37b62127840fc1c8","permalink":"https://yuechuhaoxi020609.github.io/project/probsol/","publishdate":"2023-03-01T00:00:00Z","relpermalink":"/project/probsol/","section":"project","summary":"Experiments show task abstraction relies on top-down common sense, independent of personal problem-solving strategies, suggesting we can approach unknown problems before solving them.","tags":["Computational Cognitive Science"],"title":"Abductive task abstractions in physical problem-solving","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://yuechuhaoxi020609.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]