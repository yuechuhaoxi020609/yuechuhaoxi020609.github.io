
    
    
    
    
    [{"authors":null,"categories":null,"content":" I am currently studying at Huazhong University of Science and Technology, and also working as an undergraduate research assistant at the PKU CoRe Lab under the supervision of Dr. Yixin-Zhu. My research interests include Computational Cognitive Science, Artificial Intelligence and Reinforcement Learning.\nDownload my Resumé here.\n","date":1680393600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1680393600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://example.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am currently studying at Huazhong University of Science and Technology, and also working as an undergraduate research assistant at the PKU CoRe Lab under the supervision of Dr. Yixin-Zhu.","tags":null,"title":"Haofei Hou","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://example.com/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Haofei Hou"],"categories":["Summary"],"content":"Abstract “small data for big tasks” paradigm models of common sense We identify functionality, physics, intent, causality, and utility (FPICU) as the five core domains of cognitive AI with humanlike common sense. FPICU is concerned with the questions of “why” and “how,” beyond the dominant “what” and “where” framework for understanding vision. They are invisible in terms of pixels but nevertheless drive the creation, maintenance, and development of visual scenes. A Call for a Paradigm Shift in Vision and AI The classic definition of computer vision proposed by the pioneer David Marr is to look at “what” is “where.” Here, “what” refers to object recognition (object vision), and “where” denotes three-dimensional (3D) reconstruction and object localization (spatial vision) require large sets of labeled training data designed for special tasks, and lack a general understanding of common facts—that is, facts that are obvious to the average human adult—that describe how our physical and social worlds work. missing dimensions and the potential benefits of joint representation and joint inference. The concept of “darkness” is perpendicular to and richer than the meanings of “latent” or “hidden” used in vision and probabilistic modeling; darkness” is a measure of the relative difficulty of classifying an entity or inferring about a relationship based on how much invisible common sense needed beyond the visible appearance or geometry. Section 2: paper starts by revisiting a classic view of computer vision in terms of “what” and “where”, task-driven Section 3: In order to use “small data” to solve “big tasks,” we then identify and review five crucial axes of visual common sense: Functionality, Physics, perceived Intent, Causality, and Utility (FPICU). Causality Section 4: The application of causality (i.e., intuitive physics; ) Section 5: Functionality Section 6: infer intent Section 7: utility-driven In a series of studies, we demonstrate that these five critical aspects of “dark entities” and “dark relationships” indeed support various visual tasks beyond just classification. Vision: From Data-driven to Task-driven From a biological perspective, the majority of living creatures use a single (with multiple components) vision system to perform thousands of tasks. these results indicate that our biological vision system possesses a mechanism for perceiving object functionality (i.e., how an object can be manipulated as a tool) that is independent of the mechanism governing face recognition (and recognition of other objects) “What”: Task-centered Visual Recognition these approaches have left unclear how classification interacts with scene semantics and enables cognitive reasoning human vision organizes representations during the inference process even for “simple” categorical recognition tasks. scene categorization and the information-gathering process are constrained by these categorization tasks, suggesting a bidirectional interplay between the visual input and the viewer’s needs/tasks the representation of the same object can vary according to the planned task task-driven nature of scene categorization. “Where”: Constructing 3D Scenes as a Series of Tasks scene reconstruction from a single two-dimensional (2D) image is a well-known illposed problem; there may exist an infinite number of possible 3D configurations that match the projected 2D observed images enable agents to perform tasks by generating the best possible configuration in terms of functionality, physics, and object relationships. there is now abundant evidence that humans represent the 3D layout of a scene in a way that fundamentally differs from any current computer vision algorithms human vision is error-prone and distorted in terms of localization Grid cells encode a cognitive representation of Euclidean space, implying a different mechanism for perceiving and processing locations and directions. Xie et al. proposed a representational model for grid cells, in which the 2D self-position of an agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. how we navigate complex environments while remaining able at all times to return to an original location (i.e., homing) remains a mystery in biology and neuroscience. the task-dependent representation of space can shed some light. neither based on a stable 3D model of a scene nor a distorted one; instead, participants seemed to form a flat and task-dependent representation Beyond “What” and “Where”: Towards Scene Understanding with Humanlike Common Sense rich as videos and much sparser visual inputs To enable an artificial agent with similar capabilities, we call for joint reasoning algorithms on a joint representation that integrates (i) the “visible” traditional recognition and categorization of objects, scenes, actions, events, and so forth; and (ii) the “dark” higher level concepts of fluent, causality, …","date":1680393600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680393600,"objectID":"9312a8cd03c12aeaf4411e7c3bd91bee","permalink":"https://example.com/post/dark-beyond-deep/","publishdate":"2023-04-02T00:00:00Z","relpermalink":"/post/dark-beyond-deep/","section":"post","summary":"Propose a “small data for big tasks” paradigm, wherein a single artificial intelligence (AI) system is challenged to develop “common sense,” enabling it to solve a wide range of tasks with little training data. Identify functionality, physics, intent, causality, and utility (FPICU) as the five core domains of cognitive AI with humanlike common sense","tags":["Academic","CV","CoCoSci"],"title":"Dark, Beyond Deep - A Paradigm Shift to Cognitive AI with Humanlike Common Sense","type":"post"},{"authors":["Haofei Hou"],"categories":["Summary"],"content":"Introduction two different computational approaches to intelligence pattern recognition approach treats prediction as primary, usually in the context of a specific classification, regression, or control task. discovering features that have high-value states in common across a large, diverse set of training data. model building. Cognition is about using these models to understand the world, to explain, to imagine what could have happened that didn’t, or what could be true that isn’t, and then planning. prediction and explanation, is central to our view of human intelligence. pattern recognition can support model building, through “model-free” algorithms that learn through experience how to make essential inferences more computationally efficient What this article is not we believe that reverse engineering human intelligence can usefully inform AI and machine learning. avoiding cognitive or neural inspiration as well as claims of cognitive or neural plausibility is a approach to developing AI. But this article has little pertinence to this approach. Overview of the key ideas propose a set of core ingredients for building more human-like learning and thinking machines mainly in Section 4 Core ingredients of human intelligence Cognitive and neural inspiration in artificial intelligence behaviorist view Cognitive science PDP approach knowledge is thus distributed across the collection of units rather than localized as in most symbolic data structures. Neural network models and the PDP approach offer a view of the mind (and intelligence more broadly) that is sub-symbolic and often populated with minimal constraints and inductive biases to guide learning. Proponents of this approach maintain that many classic types of structured knowledge, such as graphs, grammars can be useful yet misleading metaphors for characterizing thought. Question neural networks have such broad application in machine vision, language, and control, and they can be trained to emulate the rule-like and structured behaviors that characterize cognition do we need more to develop truly human-like learning and thinking machines? How far can relatively generic neural networks bring us toward this goal? Challenges for building more human-like machines The Characters Challenge learning simple visual concepts People learn a lot more from a lot less, and capturing these human-level learning abilities in machines is the Characters Challenge Although humans and neural networks may perform equally well on the MNIST digit recognition task and other large-scale image classification tasks, it does not mean that they learn and think in the same way. two important differences between CNN and human in learning simple visual concepts people learn from fewer examples and they learn richer representations people learn more than how to do pattern recognition: they learn a concept, that is, a model of the class that allows their acquired knowledge to be flexibly applied in new ways. Some difficulty A single example of a new visual concept (red box) can be enough information to support the classification of new examples generation of new examples parsing an object into parts and relations generation of new concepts from related concepts. The Frostbite Challenge learning to play the Atari game Frostbite Failed on accomplishing a sub-goal (such as reaching an ice floe) and then safely proceed to the next sub-goal the policy are highly specialized for the games it was trained on considering the amount of experience required for learning non-professional humans can grasp the basics of the game after just a few minutes of play. people do this by inferring a general schema to describe the goals of the game and the object types and their interactions, using the kinds of intuitive theories, model-building abilities and model-based planning mechanisms we describe below. the game of Frostbite provides incremental rewards for reaching each active ice floe, providing the DQN with the relevant sub-goals for completing the larger task of building an igloo. Without these sub-goals, the DQN would have to take random actions until it accidentally builds an igloo and is rewarded for completing the entire level. Human is possible to figure out the higher-level goal of building an igloo without incremental feedback; sparse feedback is a source of difficulty in other Atari 2600 games such as Montezuma’s Revenge inflexible to changes in its inputs and goals. Changing the color or appearance of objects or changing the goals of the network would have devastating consequences on performance if the network is not retrained In contrast, people require little or no retraining or reconfiguration, adding new tasks and goals to their repertoire with relative ease. Humans as a result often have important domain-specific knowledge for these tasks, even before they ‘begin.’ The DQN is starting completely from scratch. How do we bring to bear rich prior knowledge to learn new tasks and solve new …","date":1679270400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1679270400,"objectID":"2f3eb87357daaf9b6e2c26b1090e73a2","permalink":"https://example.com/post/build-humanlike-machine/","publishdate":"2023-03-20T00:00:00Z","relpermalink":"/post/build-humanlike-machine/","section":"post","summary":"(1) build causal models of the world (2) ground learning in intuitive theories of physics and psychology (3) harness compositionality and learning-to-learn. Authors suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.","tags":["Academic","CoCoSci"],"title":"Building machines that learn and think like people","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://example.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]