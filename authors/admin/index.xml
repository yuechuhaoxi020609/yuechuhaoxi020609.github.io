<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Haofei Hou | Haofei Hou | HUST</title>
    <link>https://example.com/authors/admin/</link>
      <atom:link href="https://example.com/authors/admin/index.xml" rel="self" type="application/rss+xml" />
    <description>Haofei Hou</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 20 Mar 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/authors/admin/avatar_hu78482ed6fd3911eb675c3aef45a3fb83_32147_270x270_fill_q75_lanczos_center.jpg</url>
      <title>Haofei Hou</title>
      <link>https://example.com/authors/admin/</link>
    </image>
    
    <item>
      <title>Building machines that learn and think like people</title>
      <link>https://example.com/post/build-humanlike-machine/</link>
      <pubDate>Mon, 20 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/build-humanlike-machine/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;h3 id=&#34;two-different-computational-approaches-to-intelligence&#34;&gt;two different computational approaches to intelligence&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;pattern recognition approach treats prediction as primary, usually in the context of a specific classification, regression, or control task.  discovering features that have high-value states in common across a large, diverse set of training data.&lt;/li&gt;
&lt;li&gt;model building. Cognition is about using these models to understand the world, to explain, to imagine what could have happened that didn’t, or what could be true that isn’t, and then planning.&lt;/li&gt;
&lt;li&gt;prediction and explanation, is central to our view of human intelligence.  pattern recognition can support model building, through “model-free” algorithms that learn through experience how to make essential inferences more computationally efficient&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;what-this-article-is-not&#34;&gt;What this article is not&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we believe that reverse engineering human intelligence  can usefully inform AI and machine learning.&lt;/li&gt;
&lt;li&gt;avoiding cognitive or neural inspiration as well as claims of cognitive or neural plausibility  is a  approach to developing AI. But this article has little pertinence to this approach.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;overview-of-the-key-ideas&#34;&gt;Overview of the key ideas&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;propose a set of core ingredients for building more human-like learning and thinking machines&lt;/li&gt;
&lt;li&gt;mainly in Section 4 Core ingredients of human intelligence&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;cognitive-and-neural-inspiration-in-artificial-intelligence&#34;&gt;Cognitive and neural inspiration in artificial intelligence&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;behaviorist view&lt;/li&gt;
&lt;li&gt;Cognitive science&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pdp-approach&#34;&gt;PDP approach&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;knowledge is thus distributed across the collection of units rather than localized as in most symbolic data structures.&lt;/li&gt;
&lt;li&gt;Neural network models and the PDP approach offer a view of the mind (and intelligence more broadly) that is sub-symbolic and often populated with minimal constraints and inductive biases to guide learning.&lt;/li&gt;
&lt;li&gt;Proponents of this approach maintain that many classic types of structured knowledge, such as graphs, grammars can be useful yet misleading metaphors for characterizing thought.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;question&#34;&gt;Question&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;neural networks have such broad application in machine vision, language, and control, and they can be trained to emulate the rule-like and structured behaviors that characterize cognition&lt;/li&gt;
&lt;li&gt;do we need more to develop truly human-like learning and thinking machines?&lt;/li&gt;
&lt;li&gt;How far can relatively generic neural networks bring us toward this goal?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;challenges-for-building-more-human-like-machines&#34;&gt;Challenges for building more human-like machines&lt;/h2&gt;
&lt;h3 id=&#34;the-characters-challenge&#34;&gt;The Characters Challenge&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;learning simple visual concepts&lt;/li&gt;
&lt;li&gt;People learn a lot more from a lot less, and capturing these human-level learning abilities in machines is the Characters Challenge&lt;/li&gt;
&lt;li&gt;Although humans and neural networks may perform equally well on the MNIST digit recognition task and other large-scale image classification tasks, it does not mean that they learn and think in the same way.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;two-important-differences-between-cnn-and-human-in-learning-simple-visual-concepts&#34;&gt;two important differences between CNN and human in learning simple visual concepts&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;people learn from fewer examples and they learn richer representations&lt;/li&gt;
&lt;li&gt;people learn more than how to do pattern recognition: they learn a concept, that is, a model of the class that allows their acquired knowledge to be flexibly applied in new ways.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;some-difficulty&#34;&gt;Some difficulty&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A single example of a new visual concept (red box) can be enough information to support the&lt;/li&gt;
&lt;li&gt;classification of new examples&lt;/li&gt;
&lt;li&gt;generation of new examples&lt;/li&gt;
&lt;li&gt;parsing an object into parts and relations&lt;/li&gt;
&lt;li&gt;generation of new concepts from related concepts.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-frostbite-challenge&#34;&gt;The Frostbite Challenge&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;learning to play the Atari game Frostbite&lt;/li&gt;
&lt;li&gt;Failed on accomplishing a sub-goal (such as reaching an ice floe) and then safely proceed to the next sub-goal&lt;/li&gt;
&lt;li&gt;the policy are highly specialized for the games it was trained on&lt;/li&gt;
&lt;li&gt;considering the amount of experience required for learning&lt;/li&gt;
&lt;li&gt;non-professional humans can grasp the basics of the game after just a few minutes of play.&lt;/li&gt;
&lt;li&gt;people do this by inferring a general schema to describe the goals of the game and the object types and their interactions, using the kinds of intuitive theories, model-building abilities and model-based planning mechanisms we describe below.&lt;/li&gt;
&lt;li&gt;the game of Frostbite provides incremental rewards for reaching each active ice floe, providing the DQN with the relevant sub-goals for completing the larger task of building an igloo.&lt;/li&gt;
&lt;li&gt;Without these sub-goals, the DQN would have to take random actions until it accidentally builds an igloo and is rewarded for completing the entire level.&lt;/li&gt;
&lt;li&gt;Human is possible to figure out the higher-level goal of building an igloo without incremental feedback;&lt;/li&gt;
&lt;li&gt;sparse feedback is a source of difficulty in other Atari 2600 games such as Montezuma’s Revenge&lt;/li&gt;
&lt;li&gt;inflexible to changes in its inputs and goals. Changing the color or appearance of objects or changing the goals of the network would have devastating consequences on performance if the network is not retrained&lt;/li&gt;
&lt;li&gt;In contrast, people require little or no retraining or reconfiguration, adding new tasks and goals to their repertoire with relative ease.&lt;/li&gt;
&lt;li&gt;Humans as a result often have important domain-specific knowledge for these tasks, even before they ‘begin.’ The DQN is starting completely from scratch.&lt;/li&gt;
&lt;li&gt;How do we bring to bear rich prior knowledge to learn new tasks and solve new problems so quickly?&lt;/li&gt;
&lt;li&gt;What form does that prior knowledge take, and how is it constructed, from some combination of inbuilt capacities and previous experience?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;core-ingredients-of-human-intelligence&#34;&gt;Core ingredients of human intelligence&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Whether learned, built in, or enriched, the key claim is that these ingredients play an active and important role in producing human-like learning and thought, in ways contemporary machine learning has yet to capture.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;developmental-start-up-software&#34;&gt;Developmental start-up software&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Early in development, humans have a foundational understanding of several core domains including number (numerical and set operations), space (geometry and navigation), physics (inanimate objects and mechanics), and psychology (agents and groups).&lt;/li&gt;
&lt;li&gt;The underlying cognitive representations can be understood as “intuitive theories,” with a causal structure resembling a scientific theory&lt;/li&gt;
&lt;li&gt;focus on the early understanding of objects and agents.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;intuitive-physics&#34;&gt;Intuitive physics&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;At the age of 2 months, and possibly earlier, human infants expect inanimate objects to follow principles of persistence, continuity, cohesion, and solidity.&lt;/li&gt;
&lt;li&gt;At around 6 months, infants have already developed different expectations for rigid bodies, soft bodies, and liquids&lt;/li&gt;
&lt;li&gt;By their first birthday, infants have gone through several transitions of comprehending basic physical concepts such as inertia, support, containment, and collisions&lt;/li&gt;
&lt;li&gt;There is no single agreed-upon computational account of these early physical principles and concepts, and previous suggestions have ranged from decision trees, to cues, to lists of rules&lt;/li&gt;
&lt;li&gt;A promising recent approach sees intuitive physical reasoning as similar to inference over a physics software engine&lt;/li&gt;
&lt;li&gt;This “intuitive physics engine” approach enables flexible adaptation to a wide range of everyday scenarios and judgments in a way that goes beyond perceptual cues.&lt;/li&gt;
&lt;li&gt;What are the prospects for embedding or acquiring this kind of intuitive physics in deep learning systems?&lt;/li&gt;
&lt;li&gt;Connectionist models in psychology have previously been applied to physical reasoning tasks such as balance-beam rules (McClelland 1988; Shultz 2003) or rules relating to distance, velocity, and time in motion&lt;/li&gt;
&lt;li&gt;PhysNet In contrast, people require far less experience to perform any particular task, and can generalize to many novel judgments and complex scenes with no new training required (although they receive large amounts of physics experience through interacting with the world more generally).&lt;/li&gt;
&lt;li&gt;Could deep learning systems such as PhysNet capture this flexibility, without explicitly simulating the causal interactions between objects in three dimensions?&lt;/li&gt;
&lt;li&gt;Whether such models can be learned with the kind (and quantity) of data available to human infants is not clear&lt;/li&gt;
&lt;li&gt;But incorporating a physics-engine–based representation could help DQNs learn to play games such as Frostbite in a faster and more general way, whether the physics knowledge is captured implicitly in a neural network or more explicitly in a simulator.&lt;/li&gt;
&lt;li&gt;When a new object type such as a bear is introduced, as in the later levels of Frostbite (Fig. 2D), a network endowed with intuitive physics would also have an easier time adding this object type to its knowledge&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;challenges&#34;&gt;Challenges&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;For networks trained on object classification,deeper layers often become sensitive to successively higher-level features, from edges to textures to shapeparts to full objects.&lt;/li&gt;
&lt;li&gt;For deep networks trained on physics-related data,it remains to be seen whether higher layers will encode objects, general physical properties, forces, and approximately Newtonian dynamics.&lt;/li&gt;
&lt;li&gt;would it generalize broadly beyond training contexts as people’s more explicit physical concepts do?&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;intuitive-psychology&#34;&gt;Intuitive psychology&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Pre-verbal infants distinguish animate agents from inanimate objects. This distinction is partially based on innate or early-present detectors for low-level cues, such as the presence of eyes, motion initiated from rest, and biological motion&lt;/li&gt;
&lt;li&gt;Beyond these low-level cues, infants also expect agents to act contingently and reciprocally, to have goals, and to take efficient actions toward those goals subject to constraints&lt;/li&gt;
&lt;li&gt;It is generally agreed that infants expect agents to act in a goal-directed, efficient, and socially sensitive fashion&lt;/li&gt;
&lt;li&gt;One possibility is that intuitive psychology is simply cues “all the way down” This inference could be captured by a cue that states &amp;ldquo;If an agent’s expected trajectory is prevented from completion, the blocking agent is given some negative association.&lt;/li&gt;
&lt;li&gt;One alternative to a cue-based account is to use generative models of action choice, as in the Bayesian inverse planning, or Bayesian theory of mind (ToM), models of Baker et al. (2009) or the naive utility calculus models of Jara-Ettinger et al.&lt;/li&gt;
&lt;li&gt;These models formalize explicitly mentalistic concepts such as “goal,” “agent,”  “planning,” “cost,” “efficiency,” and “belief,” used to describe core psychological  reasoning in infancy.&lt;/li&gt;
&lt;li&gt;They assume adults and children treat agents as approximately rational planners who choose the most efficient means to their goals.&lt;/li&gt;
&lt;li&gt;By simulating these planning processes, people can predict what agents might do next, or use inverse reasoning from observing a series of actions to infer the utilities and beliefs of agents in a scene.&lt;/li&gt;
&lt;li&gt;Importantly, unlike in intuitive physics, simulation-based reasoning in intuitive psychology can be nested recursively to understand social interactions. We can think about agents thinking about other agents.&lt;/li&gt;
&lt;li&gt;Although deep networks have not yet been applied to scenarios involving theory of mind and intuitive psychology, they could probably learn visual cues, heuristics and summary statistics of a scene that happens to involve agents.&lt;/li&gt;
&lt;li&gt;However, it seems to us that any full formal account of intuitive psychological reasoning needs to include representations of agency, goals, efficiency, and reciprocal relations.&lt;/li&gt;
&lt;li&gt;Connectionists have argued that innate constraints in the form of hard-wired cortical circuits are unlikely , but a simple inductive bias, for example, the tendency to notice things that move other things, can bootstrap reasoning about more abstract concepts of agency&lt;/li&gt;
&lt;li&gt;Similarly, a great deal of goal-directed and socially directed actions can also be boiled down to a simple utility calculus, in a way that could be shared with other cognitive abilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;learning-as-rapid-model-building&#34;&gt;Learning as rapid model building&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;There are many learning algorithms for neural networks, including the perceptron algorithm (Rosenblatt 1958), Hebbian learning (Hebb 1949), the BCM rule (Bienenstock et al. 1982), backpropagation (Rumelhart et al. 1986a), the wake-sleep algorithm (Hinton et al. 1995), and contrastive divergence (Hinton 2002).&lt;/li&gt;
&lt;li&gt;Human “one-shot” learning&lt;/li&gt;
&lt;li&gt;Neural network sdata hungry&lt;/li&gt;
&lt;li&gt;the types of things that children learn as the meanings of words – people are still far better learners than machines.&lt;/li&gt;
&lt;li&gt;Even with just a few examples, people can learn remarkably rich conceptual models.&lt;/li&gt;
&lt;li&gt;Beyond classification, concepts support prediction, action, communication, imagination , explanation, and composition.&lt;/li&gt;
&lt;li&gt;In addition to evaluating several types of deep learning models, we developed an algorithm using Bayesian program learning (BPL) that represents concepts as simple stochastic programs: structured procedures that generate new examples of a concept when executed&lt;/li&gt;
&lt;li&gt;These programs allow the model to express causal knowledge about how the raw data are formed, and the probabilistic semantics allow the model to handle noise and perform creative tasks. Structure sharing across concepts is accomplished by the compositional re-use of stochastic primitives that can combine in new ways to create new concepts.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;compositionality&#34;&gt;Compositionality&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Compositionality is the classic idea that new representations can be constructed through the combination of primitive elements.&lt;/li&gt;
&lt;li&gt;Compositionality is also at the core of productivity: an infinite number of representations can be constructed from a finite set of primitives&lt;/li&gt;
&lt;li&gt;To capture the full extent of the mind’s compositionality, a model must include explicit representations of objects, identity, and relations, all while maintaining a notion of “coherence” when understanding novel configurations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;causality&#34;&gt;Causality&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;To capture the full extent of the mind’s compositionality, a model must include explicit representations of objects, identity, and relations, all while maintaining a notion of “coherence” when understanding novel configurations.&lt;/li&gt;
&lt;li&gt;Concept learning and vision models that use causality are usually generative but not every generative model is also causal.&lt;/li&gt;
&lt;li&gt;Causality has been influential in theories of perception. “Analysis-by-synthesis” theories of perception maintain that sensory data can be more richly represented by modeling the process that generated it&lt;/li&gt;
&lt;li&gt;Causal knowledge has also been shown to influence how people learn new concepts; providing a learner with different types of causal knowledge changes how he or she learns and generalizes.&lt;/li&gt;
&lt;li&gt;To explain the role of causality in learning, conceptual representations have been likened to intuitive theories or explanations, providing the glue that lets core features stick, whereas other equally applicable features wash away.&lt;/li&gt;
&lt;li&gt;Beyond concept learning, people also understand scenes by building causal models.&lt;/li&gt;
&lt;li&gt;There have been steps toward deep neural networks and related approaches that learn causal models.&lt;/li&gt;
&lt;li&gt;Incorporating causality may greatly improve these deep learning models; they were trained without access to causal data about how characters are actually produced, and without any incentive to learn the true causal process.&lt;/li&gt;
&lt;li&gt;A causal model of Frostbite would have to be more complex, gluing together object representations and explaining their interactions with intuitive physics and intu-
itive psychology, much like the game engine that generates the game dynamics and, ultimately, the frames of pixel images.&lt;/li&gt;
&lt;li&gt;Inference is the process of inverting this causal generative model, explaining the raw pixels as objects and their interactions, such as the agent stepping on an ice floe to deactivate it or a crab pushing the agent into the water.&lt;/li&gt;
&lt;li&gt;Deep neural networks could play a role in two ways: by serving as a bottom-up proposer to make probabilistic inference more tractable in a structured generative model or by serving as the causal generative model if imbued with the right set of ingredients.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;learning-to-learn&#34;&gt;Learning-to-learn&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;When humans or machines make inferences that go far beyond the data, strong prior knowledge (or inductive biases or constraints) must be making up the difference.&lt;/li&gt;
&lt;li&gt;One way people acquire this prior knowledge is through “learning-to-learn,” a term introduced by Harlow (1949) and closely related to the machine learning notions of “transfer learning,” “multitask learning,” and “representation learning.”&lt;/li&gt;
&lt;li&gt;The strong priors, constraints, or inductive bias needed to learn a particular task quickly are often shared to some extent with other related tasks.&lt;/li&gt;
&lt;li&gt;In hierarchical Bayesian modeling, a general prior on concepts is shared by multiple specific concepts, and the prior itself is learned over the course of learning the specific concepts&lt;/li&gt;
&lt;li&gt;In machine vision, for deep convolutional networks or other discriminative methods that form the core of recent recognition systems, learning-to-learn can occur through the sharing of features between the models learned for old objects or old tasks and the models learned for new objects or new tasks&lt;/li&gt;
&lt;li&gt;We cannot be sure how people get to the knowledge they have in this domain, but we do understand how this works in BPL, and we think people might be similar.&lt;/li&gt;
&lt;li&gt;BPL transfers readily to new concepts because it learns about object parts, sub-parts, and relations, capturing learning about what each concept is like and what concepts are like in general.  It is crucial that learning-to-learn occurs at multiple levels of the hierarchical generative process.&lt;/li&gt;
&lt;li&gt;Further transfer occurs by learning about the typical levels of variability within a typical generative model. This provides knowledge about how far and in what ways to generalize when we have seen only one example of a new character, which on its own could not possibly carry any information about variance.&lt;/li&gt;
&lt;li&gt;In the Frostbite Challenge, and in video games more generally, there is a similar interdependence between the form of the representation and the effectiveness of learning-to-learn.&lt;/li&gt;
&lt;li&gt;general world knowledge and previous video games may help inform exploration and generalization in new scenarios, helping people learn maximally from a single mistake or avoid mistakes altogether&lt;/li&gt;
&lt;li&gt;Deep reinforcement learning systems for playing Atari games have had some impressive successes in transfer learning, but they still have not come close to learning to play new games as quickly as humans can. For example, Parisotto et al. (2016) present the “actor-mimic” algorithm&lt;/li&gt;
&lt;li&gt;In sum, the interaction between representation and previous experience may be key to building machines that learn as fast as people.&lt;/li&gt;
&lt;li&gt;if such a system aims to learn compositionally structured causal models of each game – built on a foundation of intuitive physics and psychology – it could transfer knowledge more efficiently and thereby learn new games much more quickly.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;thinking-fast&#34;&gt;Thinking Fast&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In general, richer and more structured models require more complex and slower inference algorithms, similar to how complex models require more data, making the speed of perception and thought all the more remarkable.&lt;/li&gt;
&lt;li&gt;The combination of rich models with efficient inference suggests another way psychology and neuroscience may usefully inform AI.&lt;/li&gt;
&lt;li&gt;This section discusses possible paths toward resolving the conflict between fast inference and structured representations, including Helmholtz machine–style approximate inference in generative models and cooperation between model-free and model-based reinforcement learning systems&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;approximate-inference-in-structured-models&#34;&gt;Approximate inference in structured models&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;In contrast, whereas representing intuitive theories and structured causal models is less natural in deep neural networks, recent progress has demonstrated the remarkable effectiveness of gradient-based learning in high-dimensional parameter spaces.&lt;/li&gt;
&lt;li&gt;A complete account of learning and inference must explain how the brain does so much with limited computational resources&lt;/li&gt;
&lt;li&gt;Popular algorithms for approximate inference in probabilistic machine learning have been proposed as psychological models&lt;/li&gt;
&lt;li&gt;Most prominently, it has been proposed that humans can approximate Bayesian inference using Monte Carlo methods, which stochastically sample the space of possible hypotheses and evaluate these samples according to their consistency with the data and prior knowledge&lt;/li&gt;
&lt;li&gt;Although Monte Carlo methods are powerful and come with asymptotic guarantees, it is challenging to make them work on complex problems like program induction and theory learning.&lt;/li&gt;
&lt;li&gt;Discovering new theories can be slow and arduous, as testified by the long time scale of cognitive development, and learning in a saltatory fashion (rather than through gradual adaptation) is characteristic of aspects of human intelligence, including discovery and insight during development (Schulz 2012b), problem-solving (Sternberg &amp;amp; Davidson 1995), and epoch-making discoveries in scientific research (Langley et al. 1987).&lt;/li&gt;
&lt;li&gt;Dis-covering new theories can also occur much more quickly. A person learning the rules of Frostbite will probably undergo a loosely ordered sequence of “Aha!” moments:&lt;/li&gt;
&lt;li&gt;These little fragments of a “Frostbite theory” are assembled to form a causal understanding of the game relatively quickly, in what seems more like a guided process than arbitrary proposals in a Monte Carlo inference scheme&lt;/li&gt;
&lt;li&gt;For domains where program or theory learning occurs quickly, it is possible that people employ inductive biases not only to evaluate hypotheses, but also to guide hypothesis selection.&lt;/li&gt;
&lt;li&gt;How might efficient mappings from questions to a plausible subset of answers be learned?  spanning both deep learning and graphical models, has attempted to tackle this challenge by “amortizing” probabilistic inference computations into an efficient feed-forward mapping&lt;/li&gt;
&lt;li&gt;These feed-forward mappings can be learned in various ways, for example, using paired generative/recognition networks and variational optimization, or nearest-neighbor density estimation&lt;/li&gt;
&lt;li&gt;One implication of amortization is that solutions to different problems will become correlated because of the sharing of amortized computations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;model-based-and-model-free-reinforcement-learning&#34;&gt;Model-based and model-free reinforcement learning&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;There is indeed substantial evidence that the brain uses similar model-free learning algorithms in simple associative learning or discrimination learning tasks&lt;/li&gt;
&lt;li&gt;Considerable evidence suggests that the brain also has a model-based learning system, responsible for building a “cognitive map” of the environment and using it to plan action sequences for more complex tasks Model-based planning is an essential ingredient of human intelligence, enabling flexible adaptation to new tasks and goals;&lt;/li&gt;
&lt;li&gt;One boundary condition on this flexibility is the fact that the skills become “habitized” with routine application,  possibly reflecting a shift from model-based to model-free control. This shift may arise from a rational arbitration between learning systems to balance the trade-off between flexibility and speed&lt;/li&gt;
&lt;li&gt;Similarly to how probabilistic computations can be amortized for efficiency (see previous section), plans can be amortized into cached values by allowing the model-based system to simulate training data for the model-free system&lt;/li&gt;
&lt;li&gt;Intrinsic motivation also plays an important role in human learning and behavior&lt;/li&gt;
&lt;li&gt;all externally provided rewards are reinterpreted according to the “internal value” of the agent,&lt;/li&gt;
&lt;li&gt;There may also be an intrinsic drive to reduce uncertainty and construct models of the 8environment&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;responses-to-common-questionswhat-we-believe&#34;&gt;Responses to common questions(What we believe)&lt;/h2&gt;
&lt;h3 id=&#34;comparing-the-learning-speeds-of-humans-and-neural-networks-on-specific-tasks-is-not-meaningful-because-humans-have-extensive-prior-experience&#34;&gt;Comparing the learning speeds of humans and neural networks on specific tasks is not meaningful, because humans have extensive prior experience&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If deep learning researchers see themselves as trying to capture the equivalent of humans’ collective evolutionary experience, this would be equivalent to a truly immense “pre-training” phase.&lt;/li&gt;
&lt;li&gt;We are less committed to a particular story regarding the origins of the ingredients,&lt;/li&gt;
&lt;li&gt;successful learning-to-learn – or, at least, human-level transfer learning – is enabled by having models with the right representational structure, including the other building blocks discussed in this article.&lt;/li&gt;
&lt;li&gt;To build these representations from scratch might require exploring fundamental structural variations in the network’s architecture, which gradient-based learning in weight space is not prepared to do. Although deep learning researchers do explore many such architectural variations&lt;/li&gt;
&lt;li&gt;dynamics of structure search may look much more like the slow random hill climbing of evolution than the smooth, methodical progress of stochastic gradient descent.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;biological-plausibility-suggests-theories-of-intelligence-should-start-with-neural-networks&#34;&gt;Biological plausibility suggests theories of intelligence should start with neural networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We have focused on how cognitive science can motivate and guide efforts to engineer human-like AI, in contrast to some advocates of deep neural networks who cite neuro-science for inspiration.&lt;/li&gt;
&lt;li&gt;Unfortunately, what we “know” about the brain is not all that clear-cut.&lt;/li&gt;
&lt;li&gt;Hebbian learning is another case in point. In the form of long-term potentiation (LTP) and spike-timing dependent plasticity (STDP), Hebbian learning mechanisms are often cited as biologically supported.&lt;/li&gt;
&lt;li&gt;Most relevantly for our focus, it would be especially challenging to try to implement the ingredients described in this article using purely Hebbian mechanisms.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;language-is-essential-for-human-intelligence-why-is-it-not-more-prominent-here&#34;&gt;Language is essential for human intelligence. Why is it not more prominent here?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We have said little in this article about people’s ability to communicate and think in natural language, a distinctively human cognitive capacity where machine capabilities strikingly lag.&lt;/li&gt;
&lt;li&gt;We believe that understanding language and its role in intelligence goes hand-in-hand with understanding the building blocks discussed in this article.&lt;/li&gt;
&lt;li&gt;These capacities are in place before children master language, and they provide the building blocks for linguistic meaning and language acquisition&lt;/li&gt;
&lt;li&gt;Is it recursion, or some new kind of recursive structure uilding ability? Is it the ability to re-use symbols by name ? Is it the ability to understand others intentionally and build shared intentionality ? Is it some new version of these things, or is it just more of the aspects of these capacities that are already present in infants?&lt;/li&gt;
&lt;li&gt;But with language, older children become able to reason about a much wider range of physical and psychological situations . Language also facilitates more powerful learning-to-learn and compositionality.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;looking-forward&#34;&gt;Looking forward&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Machine performance may rival or exceed human performance on particular tasks, and algorithms may take inspiration from neuroscience or aspects of psychology, but it does not follow that the algorithm learns or thinks like a person. This is a higher bar worth reaching for, potentially leading to more powerful algorithms, while also helping unlock the mysteries of the human mind.&lt;/li&gt;
&lt;li&gt;When comparing people with the current best algorithms in AI and machine learning, people learn from fewer data and generalize in richer and more flexible ways.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;promising-directions-in-deep-learning&#34;&gt;Promising directions in deep learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;There has been recent interest in integrating psychological ingredients with deep neural networks, especially selective   attention, augmented working memory, and experience replay.&lt;/li&gt;
&lt;li&gt;Paralleling the human perceptual apparatus, selective  attention forces deep learning models to process raw, perceptual data as a series of high-resolution “foveal glimpses” rather than all at once.&lt;/li&gt;
&lt;li&gt;Attention may help these models in several ways. It helps to coordinate complex, often sequential, outputs by attending to only specific aspects of the input, allowing the model to focus on smaller sub-tasks rather than solving an entire problem in one shot.&lt;/li&gt;
&lt;li&gt;Attention also allows larger models to be trained without requiring every model parameter to affect every output or action.&lt;/li&gt;
&lt;li&gt;Researchers are also developing neural networks with “working memories” that augment the shorter-term memory provided by unit activation and the longer-term memory provided by the connection weights&lt;/li&gt;
&lt;li&gt;Each model seems to learn genuine programs from examples, albeit in a representation more like assembly language than a high-level programming language.&lt;/li&gt;
&lt;li&gt;differentiable programming suggests the intriguing possibility of combining the best of program induction and deep learning.&lt;/li&gt;
&lt;li&gt;Another example of combining pattern recognition and model-based search comes from recent AI research into the game Go.&lt;/li&gt;
&lt;li&gt;One worthy goal would be to build an AI system that beats a world-class player with the amount and kind of training human champions receive, rather than overpowering them with Google-scale computational resources.&lt;/li&gt;
&lt;li&gt;Although techniques for handling variable-sized inputs in ConvNets may help in playing on different board sizes, the value functions and policies that AlphaGo learns seem unlikely to generalize as flexibly and automatically as people.&lt;/li&gt;
&lt;li&gt;the fact that it cannot even conceive of these variants, let alone adapt to them autonomously, is a sign that it does not understand the game as humans do.&lt;/li&gt;
&lt;li&gt;Humans represent their strategies as a response to these constraints, such that if the game changes, they can begin to adjust their strategies accordingly.&lt;/li&gt;
&lt;li&gt;We believe it would be richly rewarding for AI and cognitive science to pursue this challenge together and that such systems could be a compelling testbed for the principles this article suggests, as well as building on all of the progress to date that AlphaGo represents.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;future-applications-to-practical-ai-problems&#34;&gt;Future applications to practical AI problems&lt;/h3&gt;
&lt;h4 id=&#34;scene-understanding&#34;&gt;Scene understanding&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Deep learning is moving beyond object recognition and toward scene understanding, as evidenced by a flurry of recent work focused on generating natural language captions for images&lt;/li&gt;
&lt;li&gt;Yet current algorithms are still better at recognizing objects than understanding scenes, often getting the key objects right but their causal relationships wrong&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;autonomous-agents-and-intelligent-devices&#34;&gt;Autonomous agents and intelligent devices&lt;/h4&gt;
&lt;h4 id=&#34;autonomous-driving&#34;&gt;Autonomous driving&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Similarly, other drivers on the road have similarly complex mental states underlying their behavior&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;creative-design&#34;&gt;Creative design&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Although we are still far from developing AI systems that can tackle these types of tasks, we see compositionality and causality as central to this goal.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;toward-more-human-like-learning-and-thinking-machines&#34;&gt;Toward more human-like learning and thinking machines&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we suggest that deep learning and other computational paradigms should aim to tackle these tasks using as few training data as people need, and also to evaluate models on a range of human-like generalizations beyond the one task on which the model was trained.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;note&#34;&gt;Note&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The Atari games are deterministic, raising the possibility that a learner can succeed by memorizing long sequences of actions without learning to generalize&lt;/li&gt;
&lt;li&gt;Although it is unclear if the DQN also memorizes action sequences, an alternative “human starts” metric provides a stronger test of generalization&lt;/li&gt;
&lt;li&gt;Although connectionist networks have been used to model the general transition that children undergo between the ages of 3 and 4 regarding false belief,&lt;/li&gt;
&lt;li&gt;A new approach using convolutional “matching networks” achieves good one-shot classification performance when discriminating between characters from different alphabets&lt;/li&gt;
&lt;li&gt;In the interest of brevity, we do not discuss here another important vein of work linking neural circuits to variational approximations (Bastos et al. 2012), which have received less attention in the psychological literature.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-three-major-paradigms-of-ai-symbolicism-connectionism-behaviorism&#34;&gt;The three major paradigms of AI: symbolicism, connectionism, behaviorism&lt;/h2&gt;
&lt;h3 id=&#34;symbolicism&#34;&gt;Symbolicism&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;symbolicism=Logicism=Psychlogism=Computerism&lt;/li&gt;
&lt;li&gt;The main principles are the hypothesis of a physical symbol system (i.e., a system that manipulates symbols) and the principle of limited rationality.&lt;/li&gt;
&lt;li&gt;This category included most of the pioneers of artificial intelligence research.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;main-points&#34;&gt;Main points&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Symbolism is the foundation of human cognitive and mental activities&lt;/li&gt;
&lt;li&gt;Computers operate as physical systems that manipulate symbols&lt;/li&gt;
&lt;li&gt;Cognition involves performing computations on symbolic representations&lt;/li&gt;
&lt;li&gt;Computers can emulate or approximate human cognitive functions.&lt;/li&gt;
&lt;li&gt;In 1957, Newell, Simon and their colleagues created a program named &amp;ldquo;Logic Theorist&amp;rdquo; that could prove mathematical theorems. It verified 38 out of the first 52 propositions in Whitehead and Russell&amp;rsquo;s &amp;ldquo;Principia Mathematica&amp;rdquo;, and subsequently confirmed some more.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;connectionism&#34;&gt;Connectionism&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Connectionism is an approach in cognitive science that hopes to explain mental phenomena with artificial neural networks (ANNs).&lt;/li&gt;
&lt;li&gt;The central principle of connectionism is to use simple and often consistent units interconnected networks, to describe psychological phenomena. Different models of connections and unit forms may vary. For example, the units and connections of the network can represent neurons and synapses, as in the human brain.&lt;/li&gt;
&lt;li&gt;Lack of neuroscientific rationality.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;parallel-distributed-processing-pdp&#34;&gt;Parallel distributed processing, PDP&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;It is a method of artificial neural networks that emphasizes the parallelism of neural processing and the distributedness of neural representations, providing researchers with a general mathematical framework. It mainly includes eight aspects:&lt;/li&gt;
&lt;li&gt;A set of processing units, represented by a set of integers.&lt;/li&gt;
&lt;li&gt;The activation of the units, represented by a vector of time-dependent functions.&lt;/li&gt;
&lt;li&gt;The output function of the units, represented by a vector of activation functions.&lt;/li&gt;
&lt;li&gt;The connectivity pattern between units, represented by a real matrix indicating connection strengths.&lt;/li&gt;
&lt;li&gt;The propagation rule for propagating activation through connections, expressed as a function on unit outputs.&lt;/li&gt;
&lt;li&gt;The activation rule for combining inputs sent to units to determine new activations for units, represented by current activations and propagation functions.&lt;/li&gt;
&lt;li&gt;The learning rule for modifying connections based on experience, expressed as weight changes based on any number of variables.&lt;/li&gt;
&lt;li&gt;The environment that provides experience for the system, represented by a set of activation vectors for some subsets of units.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;experimentalism&#34;&gt;Experimentalism&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;We can learn almost everything we know from the statistical patterns of sensory input.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;actionism&#34;&gt;Actionism&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Actionism (Behaviorism), also known as evolutionary or cybernetic school, is based on cybernetics and perception-action control systems.&lt;/li&gt;
&lt;li&gt;It argues that artificial intelligence originates from cybernetics.&lt;/li&gt;
&lt;li&gt;Cybernetic ideas became an important part of the zeitgeist in the 1940s and 1950s, influencing early artificial intelligence researchers. The cybernetics and self-organizing systems proposed by Wiener, McCulloch and others, as well as the engineering cybernetics and biological cybernetics proposed by Qian Xuesen and others, affected many fields. Cybernetics linked the working principles of neural systems with information theory, control theory, logic and computer science.&lt;/li&gt;
&lt;li&gt;And it assumes that all behaviors are produced by stimuli from the environment or shaped by individual life history; especially individual punishment, incentives, stimuli and behavioral outcomes caused by reinforcement in environment and life history. Therefore, although behaviorists generally accept that genetic factors are important determinants of behavior, they still pay more attention to environmental influences.&lt;/li&gt;
&lt;li&gt;The early research work focused on simulating human intelligent behavior and role in control processes, such as research on cybernetic systems such as self-optimization, self-adaptation, self-stabilization, self-organization and self-learning , And carried out research on &amp;ldquo;cybernetic animals&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;By the 1960s and 1970s , some progress had been made in these studies of cybernetic systems , sowing seeds for intelligent control and intelligent robots , which gave birth to intelligent control systems . Intelligent robot system .&lt;/li&gt;
&lt;li&gt;Behaviorism did not appear until the end of the 20th century as a new school of artificial intelligence , attracting many people&amp;rsquo;s interest . The representative author of this school was Brooks&amp;rsquo;s six-legged walking robot , which was regarded as a new generation of &amp;ldquo;cybernetic animals&amp;rdquo; . It is a control system based on perception-action mode to simulate insect behavior .&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
