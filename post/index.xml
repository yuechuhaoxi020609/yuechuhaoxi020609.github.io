<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Haofei Hou | HUST</title>
    <link>https://example.com/post/</link>
      <atom:link href="https://example.com/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 14 Apr 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu88cdd4b7a12988e7d5bb56be50e4f21a_10097_512x512_fill_lanczos_center_3.png</url>
      <title>Posts</title>
      <link>https://example.com/post/</link>
    </image>
    
    <item>
      <title>Artificial Social Intelligence - A Comparative and Holistic View</title>
      <link>https://example.com/post/tom-summary/</link>
      <pubDate>Fri, 14 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/tom-summary/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;humans possess a high social intelligence—the intelligence that senses social events, infers the goals and intents of others, and facilitates social interaction.&lt;/li&gt;
&lt;li&gt;cognitive science standpoint: social perception, theory of mind (ToM), and social interaction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;dawn-of-artificial-social-intelligence&#34;&gt;Dawn of Artificial Social Intelligence&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;efforts towards human-like intelligence can be divided into physical intelligence and social intelligence , analogous to the developmental psychology ideas of intuitive physics and intuitive psychology&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;unique-challenges-of-context&#34;&gt;Unique challenges of context&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ASI is distinct and challenging compared to our physical understanding of the world; it is highly context-dependent&lt;/li&gt;
&lt;li&gt;Here, context could be as large as culture and common sense or as little as two friends&amp;rsquo; shared experiences&lt;/li&gt;
&lt;li&gt;it begins with nonverbal communication&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview-of-the-article&#34;&gt;Overview of the article&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In Section 2, which covers social perception, theory of mind, and social interaction, we begin with experimental evidence and theoretical hypotheses of human social intelligence from the standpoint of cognitive science.&lt;/li&gt;
&lt;li&gt;In Section 3, we present the AI community’s computational counterpart, focused on social perception, theory of mind, and social interaction, with an added topic on social robot and cognitive architectures.&lt;/li&gt;
&lt;li&gt;In Section 4, we explore significant challenges that impede the development of the ASI and recommend potential future trends.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;human-social-intelligence&#34;&gt;Human Social Intelligence&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;We concentrate on the three most important aspects of social intelligence: social perception, ToM, and social interaction.&lt;/li&gt;
&lt;li&gt;Social perception is the basis for ToM and social interaction. It consists primarily of the perception of social features, such as animacy and agency, and provides low-level, automatic, instantaneous, and non-conscious visual perception. ToM, in contrast, is concerned with sophisticated, analytic, and logical cognitive reasoning, involving a general cognitive system with several essential components, including belief, intent, and desire. Social interaction emphasizes more multi-agent interactive activities, such as communication and cooperation, than social perception and ToM.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;social-perception&#34;&gt;Social perception&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Heider-Simmel stimuli&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230412213753647.png&#34; alt=&#34;image-20230412213753647&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Even when told explicitly that these are merely simple shapes, participants still make a rapid, spontaneous, and consistent perception of animate social agents with various complex mental states, including desires, goals, emotions, personalities, and coalitions.&lt;/li&gt;
&lt;li&gt;The Heider-Simmel experiment demonstrates two essential aspects of human social perception: the perception of animacy and agency. Animacy denotes that the perceived entities are animate as opposed to inanimate (e.g., physical objects), whereas agency refers to animate beings who are goal-oriented and capable of planning to achieve their goals rationally and efficiently.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;animacy&#34;&gt;Animacy&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In this experiment, participants were shown two small squares separated by several inches and arranged in a line.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230412215127371.png&#34; alt=&#34;image-20230412215127371&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;In the first scenario, the first square (A) moves in a straight line until it reaches the second square (B), at which point A stops moving and B begins moving in the same direction (also called the launching effect).&lt;/li&gt;
&lt;li&gt;In case two, the first square (A) approaches the second square (B). While A approaches, B moves away from A quickly and stops when it is several inches away again.&lt;/li&gt;
&lt;li&gt;In the first instance, observers observe A physically causing B&amp;rsquo;s motion&lt;/li&gt;
&lt;li&gt;in the second case, A and B are perceived as alive with their own intentions.&lt;/li&gt;
&lt;li&gt;spatiotemporal parameters mediate causal perceptions, such as relative velocity, speed–mass interaction, path length, and spatial and temporal gap.&lt;/li&gt;
&lt;li&gt;spatiotemporal parameters mediate causal perceptions, such as relative velocity, speed–mass interaction, path length, and spatial and temporal gap.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;agency&#34;&gt;Agency&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An agent is rationally controlled because it has an internal energy source, whereas an object is not.&lt;/li&gt;
&lt;li&gt;The perception of agency is frequently studied in tandem with animacy for more complex social phenomena.&lt;/li&gt;
&lt;li&gt;Gao et al[81] study a particularly salient form of perceived animacy and agency via tasks based on dynamic visual search (the Find-the-Chase task)&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230412215212952.png&#34; alt=&#34;image-20230412215212952&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;They used two cues to evaluate the objective accuracy of such perceptions: (1) chasing subtlety—the degree to which the wolf deviates from a perfectly heat-seeking pursuit, and (2) directionality—whether and how the shapes face each other.&lt;/li&gt;
&lt;li&gt;the researchers discovered that temporal dynamics could lead the visual system to either construct or actively reject interpretations of chasing&lt;/li&gt;
&lt;li&gt;van Buren et al[84] depict one disc (the “wolf”) pursuing another disc (the “sheep”) amidst several distractor discs that are moving.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230412215259122.png&#34; alt=&#34;image-20230412215259122&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;In the Unconnected condition, both lines connected distractors in pairs. In the Connected condition, however, one line connected the wolf to a distractor, and the other line connected the sheep to a different distractor. Observers in the Connected condition were markedly less likely to describe these behaviors in terms of mental state.&lt;/li&gt;
&lt;li&gt;According to the outcomes of their experiments, discrete visual objects are the fundamental units of social perception.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;specific bottomup perceptual processing is specialized and difficult to be “penetrated” by higher-level cognition. This type of social perception may be at the intersection of perceptual and cognitive processing, where basic stimuli are transformed into causal, animate, or even intentional qualities, which are strongly linked to higher-level cognitive processing.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tom&#34;&gt;ToM&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;chimpanzee Sarah was shown a brief clip of an experimenter attempting to perform simple tasks. Subsequently, Sarah observed images of several objects, one of which solved the experimenter’s&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Their findings highlight two essential components of ToM: a representation of the affair state and a representation of an individual&amp;rsquo;s motivational link to the state, i.e., belief and intention&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ToM entails attributing mental states (such as beliefs, intents, or desires) to oneself and others&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Perspective taking in an internal simulation process is one of the defining characteristics of ToM&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The infamous Sally-Anne test[90,91] , a classic first-order false belief task, is a well-known experiment on perspective taking.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414191524787.png&#34; alt=&#34;image-20230414191524787&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Later in the development trajectory[95] is the establishment of second-order ToM, which entails predicting what one person thinks or feels about what another person thinks or feels&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;intent&#34;&gt;Intent&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In fact, research has shown that humans do not encode the entirety of action details but rather observe and interpret actions in terms of their intentions and store these interpretations for later retrieval.&lt;/li&gt;
&lt;li&gt;The developmental psychology literature indicates that sixmonth-old infants view human actions as goal-directed behavior. By the age of 10 months, infants segment continuous behavior streams into discrete units that correspond to what adults would perceive as distinct goal-directed acts . After their first birthday, infants begin to comprehend that an agent may explore multiple plans to achieve a goal and choose one based on environmental conditions. 18-month-old children can deduce and reproduce an action’s intended purpose, even if the activity frequently fails to achieve the aim[102] . In addition, infants can replicate behaviors rationally and effectively based on an evaluation of the environmental restrictions, as opposed to just duplicating movements, indicating that they understand the relationships between the environment, action, and underlying intent.&lt;/li&gt;
&lt;li&gt;intentions are hierarchically arranged across extensive spatiotemporal ranges as a sequence of goals . Infants are already capable of perceiving intentions on multiple levels, including concrete action goals, higher order plans, and collaborative goals&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;categorization&#34;&gt;Categorization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cognitive ToM can be further divided into ToM for motivation (i.e., another organism’s valuation, intention, purpose, and goal) and ToM for knowledge (i.e., another organism&amp;rsquo;s belief states or taught schemas/scripts) .&lt;/li&gt;
&lt;li&gt;Individual differences in cognitive strategies are also present. The theory-theory method and simulation-theory approach are examples of these diverse ToM strategies. The theory-theory approach may be based on a set of intrinsic rules or on causal and probabilistic reasoning models, which may be analogous to cold cognition in which mental states are inferred through intellectual processes. The simulation-theory approach relies on the individual’s own motivations and deductive reasoning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;social-interaction&#34;&gt;Social interaction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;equip ASI with more sophisticated human-like communication and collaboration capabilities&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;social-cues&#34;&gt;Social cues&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;paralinguistic (voice prosody and non-language sounds),&lt;/li&gt;
&lt;li&gt;facial expression (motion and position of facial muscles),&lt;/li&gt;
&lt;li&gt;gaze (motion and position of the eyes and predicted sight-line),&lt;/li&gt;
&lt;li&gt;kinematics (motion, position, and posture of the body),&lt;/li&gt;
&lt;li&gt;proxemics (use of interpersonal space)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;gaze-communication&#34;&gt;Gaze communication&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Psychological evidence suggests that eyes are stimuli with distinct “hardwired” neural pathways in the brain for their interpretation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;joint-attention&#34;&gt;Joint attention&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230412215415220.png&#34; alt=&#34;image-20230412215415220&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;layers of human gaze communication dynamics: atomic-level and event-level.&lt;/li&gt;
&lt;li&gt;Event-level gaze communication refers to high-level, complex social communication events, such as non-communicative, mutual gaze, gaze aversion, gaze following, and joint attention.&lt;/li&gt;
&lt;li&gt;Atomic-level gaze communication describes the granular structures of human gaze interactions, including single, mutual, avoid, refer, follow, and share.&lt;/li&gt;
&lt;li&gt;Joint attention is the most advanced sort of gaze communication, as it requires two agents (1) to have the same intention to share attention on common stimuli and (2) to be aware that they are sharing a common ground.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pointing&#34;&gt;Pointing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;According to Tomasello, pointing is one of the earliest forms of communication exclusive to the human species (the other is pantomiming).&lt;/li&gt;
&lt;li&gt;Pointing is also an indicator of particular cognitive abilities, such as being an intentional actor and having ToM&lt;/li&gt;
&lt;li&gt;Bates et al and Brinck are credited with introducing the distinction between imperative pointing and declarative pointing.&lt;/li&gt;
&lt;li&gt;Declarative pointing is primarily intersubjective with a signaling function, whereas imperative pointing is based on behaviorally motivated regularities and is used to request the addressee to do something for the subject.&lt;/li&gt;
&lt;li&gt;Levinson developed the concept of interaction engine, which allows communication intentions to be conveyed and recognized in both linguistic and nonlinguistic encounters.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cooperation&#34;&gt;Cooperation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cooperation is a type of social interaction that is more complex than simple communication, as it requires a psychological infrastructure of shared intentionality.&lt;/li&gt;
&lt;li&gt;This infrastructure is comprised of two crucial factors: (1) socialcognitive skills for creating common conceptual ground with others, such as joint attention and joint intention, and (2) prosocial motivations and norms to help and share with others&lt;/li&gt;
&lt;li&gt;communication: the exchange of information between agents&lt;/li&gt;
&lt;li&gt;coordination: the alignment of multiple agents towards the achievement of specific common goals through the efforts of individual agents&lt;/li&gt;
&lt;li&gt;cooperation: each individual agent/robot exchanges relevant information and resources in support of each other’s goals, rather than a shared common goal,&lt;/li&gt;
&lt;li&gt;collaboration: requires agents to exchange information and knowledge in support of a shared task.&lt;/li&gt;
&lt;li&gt;Tomasello believes that  “shared cooperative actions” have two essential characteristics: (1) the participants have a joint goal in the sense that we (in mutual knowledge) do X together; and (2) the participants coordinate their interdependent roles—their plans and sub-plans of action, including helping one another as needed in their respective roles.&lt;/li&gt;
&lt;li&gt;Tomasello also proposed a dual-level attentional structure (the shared focus of attention at a higher level, differentiated into perspectives at a lower level) and a dual-level intentional structure (shared goal with individual roles), arguing that the former is directly parallel to the latter and may ultimately derive from it.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413183329097.png&#34; alt=&#34;image-20230413183329097&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;artificial-social-intelligence&#34;&gt;Artificial Social Intelligence&lt;/h1&gt;
&lt;h2 id=&#34;social-perception-in-simulated-scenarios&#34;&gt;Social perception in simulated scenarios&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Shu et al present a unified theory that describes the interrelationships between the perception of physical and social events&lt;/li&gt;
&lt;li&gt;Specifically, the model learns to identify latent forces by inferring a family of potential functions capturing physical laws and value functions of agent goals, thereby projecting the animations into a sociophysical space with two psychological dimensions: an intuitive sense of whether physical laws are violated and an impression of whether an agent possesses intentions to perform goal-directed actions.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://doi.org/10.1016/j.cogpsych.2021.101398&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1016/j.cogpsych.2021.101398&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413190203086.png&#34; alt=&#34;image-20230413190203086&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Tang et al investigate the problem of simultaneously perceiving physics and mind using a leash-chasing display, in which a disc (“sheep”) is being chased by another disc (“wolf”) that is physically constrained by a leash tied to a third disc (“master”). They discover that (1) an intuitive physical system, such as a leash, can significantly mitigate the detrimental effects of spatial deviation and the diminishing object-hood on perceived chasing, thereby enhancing its robustness, and (2) a mutual dependency exists between physics and mind, where disrupting one will inevitably impair the perception on the other, supporting a joint perception of physics and mind.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.stat.ucla.edu/~taogao/static/pdf/Jointly%20Perceiving%20Physics%20and%20Mind_TangGongGao_2021.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.stat.ucla.edu/~taogao/static/pdf/Jointly%20Perceiving%20Physics%20and%20Mind_TangGongGao_2021.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413191340154.png&#34; alt=&#34;image-20230413191340154&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Flatland is a new experimental paradigm introduced by Shu et al for exploring social inference in physical situations. Results demonstrate that human interpretations of interactive events in Flatland can be accounted for by a computational model that combines inverse hierarchical planning with a physical simulation engine to reason about objects and agents.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cocodev.fas.harvard.edu/publications/adventures-in-flatland-perceiving-social-interactions-under-physical-dynamics/2020-shu-adventures.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cocodev.fas.harvard.edu/publications/adventures-in-flatland-perceiving-social-interactions-under-physical-dynamics/2020-shu-adventures.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413191815934.png&#34; alt=&#34;image-20230413191815934&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413191646740.png&#34; alt=&#34;image-20230413191646740&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Shu et al examine the perception of social interaction using decontextualized motion trajectories, in which stimuli are extracted from drone-recorded aerial films of a real-world setting. To account for human judgments of interactiveness between two moving dots and the dynamic change of such judgments over time, they construct a hierarchical model that represents interactivity using latent variables and learns the distribution of critical movement features that signal potential interactivity.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://doi.org/10.1111/tops.12313&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1111/tops.12313&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413204239814.png&#34; alt=&#34;image-20230413204239814&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413192231676.png&#34; alt=&#34;image-20230413192231676&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;To investigate the cognitive architecture of perceived animacy, Gao et al devise Bayesian models that integrate domain-specific hypotheses of social agency with domain-general cognitive constraints on sensory, memory, and attentional processing. The proposed model posits that perceived animacy combines a bottomup, feature-based, parallel search for goal-directed movements with architecturally distinct processes that make perceived animacy fast, flexible, and cognitively efficient. By distinguishing target agents from distractor objects in the “wolf-chasing-sheep” setting, they demonstrate that a Bayesian ideal observer model may explain the efficacy of human perceived animacy with realistic cognitive constraints.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/cogs.12775&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Cognitive Architecture of Perceived Animacy: Intention, Attention, and Memory (wiley.com)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413205304659.png&#34; alt=&#34;image-20230413205304659&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;social-perception-in-real-world-scenarios&#34;&gt;Social perception in real-world scenarios&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fan et al investigate the topic of inferring shared attention in their collected third-person social scene video dataset VideoCoAtt by employing a spatiotemporal neural network utilizing human gaze directions and potential target boxes extracted from the context. In their subsequent study. the authors systematically investigate the subject of human gaze communication by constructing spatiotemporal graphs for realworld social scenarios in the collected VACATION video dataset. the authors systematically investigate the subject of human gaze communication by constructing spatiotemporal graphs for realworld social scenarios in the collected VACATION video dataset.&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/1909.02144&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;1909.02144] Understanding Human Gaze Communication by Spatio-Temporal Graph Reasoning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413205604467.png&#34; alt=&#34;image-20230413205604467&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;To jointly infer human attention, intention, and task from videos, Wei et al introduce a hierarchical model of humanattention-object (HAO) and a beam search algorithm. According to their definition, the intention consists of the human pose, attention, and objects, whereas the task is represented as a series of intentions.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/8578809&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Where and Why are They Looking? Jointly Inferring Human Attention and Intentions in Complex Tasks | IEEE Conference Publication | IEEE Xplore&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413205946790.png&#34; alt=&#34;image-20230413205946790&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Xie et al offer an unsupervised method for localizing functional objects and predicting human intents and trajectories from surveillance footage of public places. Agents are influenced by the attractive or repulsive “fields” of functioning objects, referred to as “dark matter”. In addition to estimating the agent’s intent, the model can also derive the agent’s trajectory via agent-based Lagrangian mechanics&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/7984896&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning and Inferring “Dark Matter” and Predicting Human Intents and Trajectories in Videos | IEEE Journals &amp;amp; Magazine | IEEE Xplore&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413210621863.png&#34; alt=&#34;image-20230413210621863&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Holtzen et al[137] present a method that enables robots to infer a person&amp;rsquo;s hierarchical intent from partially observed RGB-D videos. They represent intent as a novel hierarchical, compositional, and probabilistic And-Or-Graph structure that describes a relationship between actions and plans. Human intent is inferred by reverseengineering a person’s decision-making and action-planning processes under a Bayesian probabilistic programming framework.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/7759242&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Inferring human intent from video by sampling hierarchical plans | IEEE Conference Publication | IEEE Xplore&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413214642238.png&#34; alt=&#34;image-20230413214642238&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413214625377.png&#34; alt=&#34;image-20230413214625377&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tom-1&#34;&gt;ToM&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The computational modeling of ToM may concentrate on different components, such as belief, intent, and desire.&lt;/li&gt;
&lt;li&gt;Gonzalez and Chang divide computational models of ToM into several broad categories, including &lt;strong&gt;Game ToM , Observational (RL) , Inverse RL , and Bayesian ToM&lt;/strong&gt;. These models contain modules for representing the goals and desires of an agent, inferring the mental states of other agents (e.g., beliefs, goals, desires, intentions, and feelings), and integrating these goals and mentalizing computations to generate optimal policies.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;different-tom-components-and-modeling-methods&#34;&gt;different ToM components and modeling methods&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Yuan et al jointly infer object states, robot knowledge, and human beliefs using parse graphs, which accurately identify human (false-)beliefs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2004.12248&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/2004.12248&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To answer this Sally-Anne correctly, an agent should understand and disentangle the object state (observation from the current frame), the (accumulated) knowledge, the belief of other agents, the ground-truth/reality of the world, and importantly, the concept of false-belief.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fan et al incorporate different nonverbal communication cues (e.g., gaze, human poses, and gestures) to infer agents’ mental states based solely on visual inputs. By aggregating beliefs and physical-world states, their approach effectively forms five minds during the interactions between two agents. In particular, they construct a common mind to avoid the infinite recursion commonly used in prior works. In addition, they devise a hierarchical energy-based model&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2104.02841&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2104.02841] Learning Triadic Belief Dynamics in Nonverbal Communication from Videos (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413224240325.png&#34; alt=&#34;image-20230413224240325&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Arslan investigate how 5-year-olds choose and revise reasoning strategies in second-order false belief tasks by constructing two computational cognitive models of this process: an instance-based learning model and a RL model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fpsyg.2017.00275/pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.frontiersin.org/articles/10.3389/fpsyg.2017.00275/pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413224634548.png&#34; alt=&#34;image-20230413224634548&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414003302108.png&#34; alt=&#34;image-20230414003302108&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Oguntola develop an interpretable modular neural framework for modeling the intentions of other observed entities, demonstrating the model&amp;rsquo;s efficacy in a Minecraft search and rescue task. They also demonstrate that, under the right conditions, integrating interpretability can dramatically improve prediction performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2104.02938&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/2104.02938&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414003958427.png&#34; alt=&#34;image-20230414003958427&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Zeng et al suggest a brain-inspired model of belief ToM, leveraging high-level knowledge of brain regions’ functions relevant to ToM. Although tested on false belief tasks, such cognitive architecture may be difficult to motivate at the computational level&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.3389/fnbot.2020.00060&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.3389/fnbot.2020.00060&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414004300635.png&#34; alt=&#34;image-20230414004300635&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414004338055.png&#34; alt=&#34;image-20230414004338055&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414004359629.png&#34; alt=&#34;image-20230414004359629&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bayesian-methods&#34;&gt;Bayesian methods&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Baker et al investigate the rational quantitative attribution of beliefs, desires, and percepts in human mentalizing from agents&amp;rsquo; movement in a local spatial environment. They devise a Bayesian theory of mind (BToM) model in a partially observable Markov decision process (POMDP) setting for rational planning and state estimation, which extends classical expected-utility agent models to sequential actions in complex, partially observable domains.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://compdevlab.yale.edu/docs/2017/Bakeretal2017.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://compdevlab.yale.edu/docs/2017/Bakeretal2017.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://static-content.springer.com/esm/art%3A10.1038%2Fs41562-017-0064/MediaObjects/41562_2017_BFs415620170064_MOESM348_ESM.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://static-content.springer.com/esm/art%3A10.1038%2Fs41562-017-0064/MediaObjects/41562_2017_BFs415620170064_MOESM348_ESM.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/clbaker/BToM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;clbaker/BToM (github.com)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414085632574.png&#34; alt=&#34;image-20230414085632574&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rl-method&#34;&gt;RL method&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Wen et al and Moreno et al are examples of recursive reasoning models for higher-order ToM in a RL framework.&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/1901.09207&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;1901.09207] Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414095228222.png&#34; alt=&#34;image-20230414095228222&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/2102.02274&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2102.02274] Neural Recursive Belief States in Multi-Agent Reinforcement Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414105134026.png&#34; alt=&#34;image-20230414105134026&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Hakimzadeh contend that RL plays a crucial role in human intuition and cognition, and theories such as the language of thought hypothesis, script theory, and Piaget’s theory of cognitive development offer complementary approaches.&lt;/li&gt;
&lt;li&gt;ToM can indeed be formulated as an inverse reinforcement learning (IRL) problem, where expectations for how mental states produce behavior are represented by a RL model.&lt;/li&gt;
&lt;li&gt;RL models, such as IRL and multi-agent reinforcement learning (MARL), are highly scalable but computationally intensive and less interpretable&lt;/li&gt;
&lt;li&gt;Under a POMDP setting, Yuan et al argue that misalignment of values could impede group performance in cooperation; hence, communication plays a vital role during which a robot needs to serve as an effective listener and an expressive speaker. In the context of value alignment, they investigate how to foster effective bidirectional human-robot communications and propose an explainable artificial intelligence (XAI) system in which a collection of robots anticipates human values by using in-situ feedback while explaining their decisionmaking processes to users (see Fig. 11). Their XAI system integrates a cooperative communication model to infer human values associated with multiple desirable goals, mimic human mental dynamics, and predict optimal explanations using graphical models.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.science.org/stoken/author-tokens/ST-617/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;In situ bidirectional human-robot value alignment | Science Robotics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.science.org/doi/suppl/10.1126/scirobotics.abm4183/suppl_file/scirobotics.abm4183_sm.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.science.org/doi/suppl/10.1126/scirobotics.abm4183/suppl_file/scirobotics.abm4183_sm.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414112527794.png&#34; alt=&#34;image-20230414112527794&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414112620204.png&#34; alt=&#34;image-20230414112620204&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;game-tom--which-leverages-concepts-like-nash-equilibria&#34;&gt;game ToM , which leverages concepts like Nash equilibria&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;de Weerd et al employ a combination of computational agents and Bayesian model selection to determine the extent to which individuals use higher-order ToM reasoning in a particularly competitive game known as matching pennies. Their findings suggest that humans do not primarily employ their high-order ToM abilities.&lt;/li&gt;
&lt;li&gt;In a case study of the paperscissors-rock game, Kanwal et al develop a ToM-based agent, capable of using gestures for non-verbal communication&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lgurjcsit.lgu.edu.pk/index.php/lgurjcsit/article/view/96/90&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;View of A Step Towards the Development of Socio-cognitive Agent (lgu.edu.pk)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414113611571.png&#34; alt=&#34;image-20230414113611571&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Tejwani formalize a theory of social interactions, encompassing cooperation, conflict, coercion, competition, and trade, by extending a nested Markov decision process (MDP) where agents reason about arbitrary functions of each other&amp;rsquo;s hidden rewards.&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/2110.10298&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2110.10298] Incorporating Rich Social Interactions Into MDPs (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414114753785.png&#34; alt=&#34;image-20230414114753785&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414114919915.png&#34; alt=&#34;image-20230414114919915&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;In a follow-up study, Tejwani expand the reward function to incorporate both physical and social goals. Their method permits more complex behaviors, such as politely hindering or aggressively assisting another agent.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.mlr.press/v164/tejwani22a/tejwani22a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://proceedings.mlr.press/v164/tejwani22a/tejwani22a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Panella and Gmytrasiewicz devise a new computational framework, interactive partially observable Markov decision process (I-POMDP), wherein the agent does not explicitly model the beliefs and preferences of other agents but rather represents them as stochastic processes implemented by probabilistic deterministic finite-state controllers (PDFCs). Using Bayesian inference, the agent updates its belief over the PDFCs models of other agents.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://link.springer.com/article/10.1007/s10458-016-9359-z&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interactive POMDPs with finite-state models of other agents | SpringerLink&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deep-learning-dl&#34;&gt;Deep learning (DL)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Aru et al. point out that the problems of existing DL methods are taking shortcuts rather than learning ToM; the system may learn a much simpler decision rule . DL for ToM is explored predominantly with deep reinforcement learning (DRL), wherein the agent&amp;rsquo;s experiences and objectives are intertwined. Usually, the task&amp;rsquo;s reward structure determines what the agent accomplishes and learns. However, in the case of ToM, there may not exist a straightforward cost function or reward structure that would necessitate the emergence of ToM.&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/2203.16540&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2203.16540] Mind the gap: Challenges of deep learning approaches to Theory of Mind (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414120438385.png&#34; alt=&#34;image-20230414120438385&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Crucially, Zhao et al[160] demonstrate in a multi-agent setting that rewards may simply be a byproduct of ToM, not playing a causal role in establishing effective coordination.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;social-communication-and-cooperation&#34;&gt;Social communication and cooperation&lt;/h2&gt;
&lt;h3 id=&#34;nonverbal-communication&#34;&gt;Nonverbal communication&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Jiang et al model pointing as a communicative act between agents who have a mutual understanding that the pointed observation must be relevant and interpretable; the act of pointing is an invitation to jointly attend to an object, which elicits mutual inference between agents of each other&amp;rsquo;s minds. The proposed model measures relevance by defining a Smithian value of information (SVI) as the utility gain of a pointing signal. By integrating SVI into rational speech act (RSA), their pragmatic model of pointing permits contextually flexible interpretations.&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/2106.02003&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2106.02003] Individual vs. Joint Perception: a Pragmatic Model of Pointing as Communicative Smithian Helping (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pku.ai/publication/pointing2022cogsci/paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://pku.ai/publication/pointing2022cogsci/paper.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tang et al demonstrate that agents can successfully and robustly employ bootstrapping to converge to a joint intention from randomness under an Imagined We framework, leveraging a real-time cooperative hunting task subject to various setting manipulations.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cognitivesciencesociety.org/cogsci20/papers/0584/0584.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cognitivesciencesociety.org/cogsci20/papers/0584/0584.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Stacy et al propose a computational account of overloaded signaling from a shared agency perspective, which we refer to as the Imagined We for communication. Within this framework, communication is a means for cooperators to coordinate their perspectives, allowing them to act in concert to achieve shared objectives. In a series of simulations, the model performs effectively under growing ambiguity and increasing levels of reasoning, highlighting how shared knowledge and cooperative logic may perform the majority of the heavy lifting in language&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/2106.02164&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2106.02164] Modeling Communication to Coordinate Perspectives in Cooperation (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414153526746.png&#34; alt=&#34;image-20230414153526746&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cooperation-1&#34;&gt;Cooperation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Wang et al introduce ToM to build socially intelligent agents, who can communicate and cooperate effectively to accomplish challenging tasks. These agents determine when and with whom to reveal their intentions and sub-goals based on the inferred mental states of others.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2111.09189&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2111.09189] ToM2C: Target-oriented Multi-agent Communication and Cooperation with Theory of Mind (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414154521125.png&#34; alt=&#34;image-20230414154521125&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pöppel et al study how efficient, automatic coordination mechanisms at the level of mental states (intentions, objectives), also known as belief resonance, may lead to collaborative situated problem-solving. They describe a model of hierarchical active inference for collaborative agent (HAICA) that blends Bayesian ToM with a perception-action system based on predictive processing and active inference. Belief resonance is realized by allowing the inferred mental states of one agent influence another agent&amp;rsquo;s prediction beliefs regarding its own goals and intentions, hence influencing the agent&amp;rsquo;s task behavior without explicit collaborative reasoning.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.1007/s12559-021-09960-4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1007/s12559-021-09960-4&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414161510468.png&#34; alt=&#34;image-20230414161510468&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414161532796.png&#34; alt=&#34;image-20230414161532796&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;llm&#34;&gt;LLM&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Michal tested several language models using 40 classic false-belief tasks widely used to test ToM in humans. These findings suggest that ToM-like ability (thus far considered to be uniquely human) may have spontaneously emerged as a byproduct of language models’ improving language skills&lt;/li&gt;
&lt;li&gt;The important role of language in cognition&lt;/li&gt;
&lt;li&gt;Backwards cognition from language&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/2302.02083&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2302.02083] Theory of Mind May Have Spontaneously Emerged in Large Language Models (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;social-robot-and-cognitive-architectures&#34;&gt;Social robot and cognitive architectures&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The social robot is an interdisciplinary research field that requires comprehensive studies of social perception, ToM, and social interaction&lt;/li&gt;
&lt;li&gt;A social robot is expected to&lt;/li&gt;
&lt;li&gt;(1) develop adaptive behavioral models ,&lt;/li&gt;
&lt;li&gt;(2) be socially adept,&lt;/li&gt;
&lt;li&gt;(3) establish a natural, fluent, and effective human-like communication and interaction with humans&lt;/li&gt;
&lt;li&gt;(4) establish empathetic relationships with humans and be perceived as a teammate or a colleague rather than a tool,&lt;/li&gt;
&lt;li&gt;(5) offer proactive and parental help based on the observations and understanding of the human situation&lt;/li&gt;
&lt;li&gt;(6) build trust with humans.&lt;/li&gt;
&lt;li&gt;However, there are still many obstacles to overcome before constructing an ideal social robot.&lt;/li&gt;
&lt;li&gt;It is difficult to incorporate behavioral adaption techniques, cognitive architectures, persuasive communication strategies, and empathy into a single solution for understanding nonverbal phenomena in social interactions, as contexts are constantly changing.&lt;/li&gt;
&lt;li&gt;A common limitation of current research is that researchers have focused on a particular aspect of a social robot, such as&lt;/li&gt;
&lt;li&gt;(1) emphasizing a communication strategy,&lt;/li&gt;
&lt;li&gt;(2) studying a particular behavior as a response to human action, or&lt;/li&gt;
&lt;li&gt;(3) conducting experimental studies that include only partial factors.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cognitive-architecture&#34;&gt;Cognitive architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ASI in robotic agents relies heavily on the construction of cognitive architecture, which involves both abstract models of cognition and software instantiations of such models&lt;/li&gt;
&lt;li&gt;three classic cognitive architecture&lt;/li&gt;
&lt;li&gt;LIDA (Learning Intelligent Distribution Agent) is a cognitive architecture proposed by Stan Franklin et al. in 2005. Its core idea is to view cognitive processes as distributed processing of information, achieved through modules such as perception, attention, working memory, and long-term memory at different levels.&lt;/li&gt;
&lt;li&gt;Soar (State, Operator, and Result) is a cognitive architecture proposed by Allen Newell and Paul Rosenbloom et al. in 1983. Its core idea is to view cognitive processes as a series of state transitions for problem solving, achieved through mechanisms such as state representation, rule matching, and subgoals.&lt;/li&gt;
&lt;li&gt;ACT-R (Adaptive Control of Thought - Rational) is a cognitive architecture proposed by John Anderson et al. in 1983. Its core idea is to view cognitive processes as a series of rule-based symbol processing, achieved through the interaction between declarative and procedural knowledge.&lt;/li&gt;
&lt;li&gt;Therefore, LIDA was proposed in 2005, Soar was proposed in 1983, and ACT-R was also proposed in 1983.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;learning-intelligent-distribution-agent-lida&#34;&gt;Learning intelligent distribution agent (LIDA)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;cognitive architecture is an integrated artificial cognitive system that models a broad spectrum of biological cognition, from low-level perception and action to high-level reasoning. Two hypotheses underlie the LIDA architecture and its corresponding conceptual model:&lt;/li&gt;
&lt;li&gt;(1) Much of human cognition functions through cognitive cycles, which are interactions between conscious contents, memory systems, and action selection, occur frequently (10 Hz).&lt;/li&gt;
&lt;li&gt;(2) Cognitive cycles serve as the cognitive atoms of which higherlevel cognitive processes are composed.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-642-22887-2_14&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The LIDA Framework as a General Tool for AGI | SpringerLink&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414153338050.png&#34; alt=&#34;image-20230414153338050&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;soar&#34;&gt;Soar&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The Soar cognitive architecture is composed of interacting task-independent modules, including short-term and long-term memories, processing modules, learning mechanisms, and interfaces between them. Since Soar hypothesizes that sufficient regularities exist above the neural level to capture the functionality of the human mind, the majority of knowledge representations in Soar are symbol structures, with architecturally maintained numeric metadata biasing the retrieval and learning of those structures&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Soar_%28cognitive_architecture%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Soar (cognitive architecture) - Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;J. E. Laird, The Soar Cognitive Architecture, Cambridge, MA, USA: MIT Press, 2019.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;adaptive-control-of-thought-rationale-architecture-act-r&#34;&gt;Adaptive control of thought-rationale architecture (ACT-R)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;includes modules such as&lt;/li&gt;
&lt;li&gt;(1) a visual module for identifying objects in the visual field,&lt;/li&gt;
&lt;li&gt;(2) a manual module for controlling the hands,&lt;/li&gt;
&lt;li&gt;(3) a declarative module for retrieving information from memory,&lt;/li&gt;
&lt;li&gt;(4) a goal module for tracking current goals and intentions, and&lt;/li&gt;
&lt;li&gt;(5) a central production system to coordinate these modules.&lt;/li&gt;
&lt;li&gt;There are buffers within each module that transmit information back and forth to the central production system. The architecture assumes a mixture of serial and parallel processing&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/actr-a-higherlevel-account-of-processing-capacity/73969664D69FB461CC04097B6D49FC77&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ACT-R: A higher-level account of processing capacity | Behavioral and Brain Sciences | Cambridge Core&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://psycnet.apa.org/doiLanding?doi=10.1037%2F0033-295X.111.4.1036&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Integrated Theory of the Mind. (apa.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cognitive-architectures-in-social-robots&#34;&gt;Cognitive architectures in social robots&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Wiltshire et al discuss the problem of engineering human social-cognitive mechanisms to enable robot social intelligence and provide an integrative perspective of social cognition as a systematic theoretical underpinning for computational instantiations of these mechanisms. They also provide a series of recommendations to facilitate the development of the perceptual, motor, and cognitive architecture.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S1389041716300493?via%3Dihub&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Enabling robotic social intelligence by engineering human social-cognitive mechanisms - ScienceDirect&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Breazeal et al provide an integrated sociocognitive architecture (see Fig. 14) to endow an anthropomorphic robot with the ability to infer mental states such as beliefs, intents, and desires from the observable behavior of its human partner via simulation-theoretic techniques.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414173012629.png&#34; alt=&#34;image-20230414173012629&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kennedy et al describe an approach known as a like-me simulation, in which the agent uses its own knowledge and capabilities as a model of another agent to predict that agent’s actions. They present three examples of a likeme mental simulation in a social context implemented in the embodied version of the adaptive control of thought-rationale architecture (ACT-R) cognitive architecture, ACT-R Embodied (ACT-R/E), including perspective taking, teamwork, and dominant-submissive social behavior.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://link.springer.com/article/10.1007/s12369-009-0014-6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Like-Me” Simulation as an Effective and Cognitively Plausible Basis for Social Robotics | SpringerLink&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Moulin-Frier et al suggest the DAC-h3 architecture, which incorporates a reactive interaction engine, a number of state-of-the-art perceptual and motor learning algorithms, planning capabilities, and an autobiographical memory. The architecture as a whole drives the robot’s behavior to solve the symbol grounding problem, acquire language capabilities, perform goal-oriented behavior, and articulate a verbal narrative of its own experience in the world.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.1109/TCDS.2017.2754143&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1109/TCDS.2017.2754143&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Franchi et al[179] present a brain-inspired architecture, the intentional distributed robotic architecture (IDRA), which aims to permit the autonomous development of new goals in situated agents beginning with simple hard-coded instincts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/01691864.2016.1172732&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;From learning to new goal generation in a bioinspired robotic setup: Advanced Robotics: Vol 30, No 11-12 (tandfonline.com)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;discussions&#34;&gt;Discussions&lt;/h1&gt;
&lt;h2 id=&#34;recent-advances-in-datasets-and-environments&#34;&gt;Recent advances in datasets and environments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PHASE:&lt;/strong&gt; Netanyahu et al[180] resemble a collection of physically-grounded abstract social events (PHASE) that simulates a wide variety of real-world social interactions by incorporating social concepts, such as helping another agent. PHASE is comprised of 2D animations of agent pairs, moving in continuous space with multiple objects and landmarks, generated procedurally by a physics engine and a hierarchical planner.&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/2103.01933&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2103.01933] PHASE: PHysically-grounded Abstract Social Events for Machine Social Perception (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AGENT:&lt;/strong&gt; Inspired by intuitive psychology, Shu et al[181] present a benchmark consisting of a large dataset of procedurally generated 3D animations, Action, Goal, Efficiency, coNstraint, uTility (AGENT), structured around four scenarios (goal preferences, action efficiency, unobserved constraints, and cost-reward tradeoffs) that probe key concepts of core intuitive psychology&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/2102.12321&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2102.12321] AGENT: A Benchmark for Core Psychological Reasoning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;WAH:&lt;/strong&gt; Puig et al[182] introduced watch-and-help (WAH), a challenge for testing social intelligence in agents, wherein an AI agent is tasked to help a human-like agent perform a complex household task efficiently. They build VirtualHomeSocial, a multi-agent household environment, and provide a benchmark including both planning and learning-based baselines.&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/2010.09890&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2010.09890] Watch-And-Help: A Challenge for Social Perception and Human-AI Collaboration (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sap et al proposed a dataset to evaluate language-based commonsense reasoning about social interactions, including reasoning about motivation and about emotional reactions .&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/1904.09728&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;1904.09728] SocialIQA: Commonsense Reasoning about Social Interactions (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hanabi:&lt;/strong&gt; Bard et al propose the cooperative and imperfect information card game, Hanabi, as a challenging benchmark. It requires reasoning about the beliefs and the intentions of other players, focusing on the ad-hoc setting where an agent has to coordinate with a team they encounter for the first time.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0004370219300116?via%3Dihub&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Hanabi challenge: A new frontier for AI research - ScienceDirect&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414181418930.png&#34; alt=&#34;image-20230414181418930&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation-protocols&#34;&gt;Evaluation protocols&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;the formation of universally accepted criteria for the design and implementation of ASI benchmarks and the accompanying evaluation protocols is still in its infancy and represents a significant barrier to the field&amp;rsquo;s continued progress.&lt;/li&gt;
&lt;li&gt;Because human judgments can be ambiguous and difficult to express, many social intelligence tasks do not include requirements that can be easily captured using hand-crafted rules.&lt;/li&gt;
&lt;li&gt;The Turing test is a test of a machine&amp;rsquo;s ability to exhibit intelligent behavior equivalent to or indistinguishable from that of a human.&lt;/li&gt;
&lt;li&gt;However, current systems that perform well on these tests typically do so by employing techniques that are not generalizable to other problems.&lt;/li&gt;
&lt;li&gt;Other approaches for assessing social intelligence competency are often derived from various sources, such as peer-/superior-/self-ratings and observers’ behavioral assessments&lt;/li&gt;
&lt;li&gt;Notably, the Animal-AI Olympics[187] is initiated by testing artificial agents on tasks derived directly from animal cognition research in an effort to establish common ground&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/s42256-019-0050-3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Animal-AI Olympics | Nature Machine Intelligence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414184518872.png&#34; alt=&#34;image-20230414184518872&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;An important aspect of the ASI is to measure cognitive skills, adaptability, and meta-level learning and reasoning ability rather than specific problem-solving ability.&lt;/li&gt;
&lt;li&gt;Using more abstract cognitive processes, such as the ability to&lt;/li&gt;
&lt;li&gt;(1) transfer information from one domain to another,&lt;/li&gt;
&lt;li&gt;(2) retain information for extended periods, and&lt;/li&gt;
&lt;li&gt;(3) correct errors in performance, may be future effective strategies for assessing ASI.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;future-trends&#34;&gt;Future trends&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A holistic approach&lt;/strong&gt;: Cognitive and neuroscience research shows that while distinct brain regions are involved in specific tasks, a core network is involved in all ToM tasks, suggesting that humans take a more holistic approach to social intelligence than existing computational models, which often focus on a single aspect of the problem.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning methods&lt;/strong&gt;: Infants develop intelligence gradually. This suggests that learning, and in particular lifelong/continuous learning , is a crucial path for developing ASI. The objective of lifelong/continuous learning is to successively learn a model for a large number of activities without forgetting the knowledge acquired from the previous tasks. Other potentially effective learning strategies include multi-task learning, one-/few-shot learning, and meta-learning .&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Open-ended and interactive environment&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Human biases&lt;/strong&gt;:  We must also introduce better biases, even structural biases, as a form of built-in common sense, as there may be multiple biases and limits in the human brain that facilitate the acquisition of social intelligence. For instance, there may be innate biases of attention to the human face, speech, hands, eyes, gaze-direction, and biological motion, and these early biases ensure that the infant learns about the components of the world that provide information about the minds of other people.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;ASI is a crucial missing component for artificial general intelligence (AGI) on par with humans and symbolizes the future path of AI.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>PPOxFamliy MARL</title>
      <link>https://example.com/post/ppoxfamliy/</link>
      <pubDate>Wed, 12 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/ppoxfamliy/</guid>
      <description>&lt;h1 id=&#34;policy-gradient&#34;&gt;Policy Gradient&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;


$$G(\tau)=\sum_{t=0}^{T-1}\gamma^{t}r_{t}$$
&lt;/li&gt;
&lt;li&gt;


$$J(\theta)=\mathbb{E}_S\Big[V_\pi(S)\Big].$$
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Policy gradient theorem&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;


$$\nabla J_\theta=\frac{1}{N}\sum_{n=1}^N\sum_{t=0}^{T_n-1}G_t(\tau)\nabla logp_\theta(a_t|s_t)$$
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SGD&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;higher variance&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;no bias&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;proof:&lt;/strong&gt;&lt;/em&gt; &lt;a href=&#34;https://github.com/opendilab/PPOxFamily/blob/main/chapter1_overview/chapter1_supp_trpo.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PPOxFamily/chapter1_supp_trpo.pdf at main · opendilab/PPOxFamily (github.com)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;actor-critic&#34;&gt;Actor-Critic&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;


$$Q_\phi\left(s_r,a_t\right)=E_{t-\pi}\left[\sum_{l=0}^\infty\gamma^l r^{t+l}\right]$$
&lt;/li&gt;
&lt;li&gt;


$$\nabla J_\theta=\dfrac{1}{N}\sum_{n=1}^{N}\sum_{t=0}^{T_a-1}Q_\theta(s_t,a_t)\nabla logp_\theta(a_t|s_t)$$
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;not unbiased&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;lower variance&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;a2c&#34;&gt;A2C&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;


$$\nabla J_\theta=\frac{1}{N}\sum_{n=1}^N\sum_{t=0}^{T_n-1}(G_t(\tau)-V_\phi(s_t))\nabla logp_\theta(a_t|s_t)$$
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;no bias&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;lower variance&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;why cut the baseline function:&lt;/strong&gt;&lt;/em&gt; &lt;a href=&#34;https://github.com/opendilab/PPOxFamily/blob/main/chapter1_overview/chapter1_supp_a2c.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PPOxFamily/chapter1_supp_a2c.pdf at main · opendilab/PPOxFamily (github.com)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;trpo&#34;&gt;TRPO&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;


$$\eta(\pi)=E_{s_0,a_0,\dots}\left[\sum\limits_{t=0}^\infty\gamma^tr_t\right]\quad
  $$
&lt;/li&gt;
&lt;li&gt;


$$\rho_{\pi}(s)=P\left(s_{0}=s\right)+\gamma P\left(s_{1}=s\right)+{\gamma}^{2}P\left(s_{2}=s\right)+\ldots\quad$$
&lt;/li&gt;
&lt;li&gt;


$$\eta(\tilde{\pi})=\eta(\pi)+\sum_s\rho_{\tilde{\pi}}(s)\sum_a\tilde{\pi}(a\mid s)A_{\pi}(s,a)	$$
&lt;/li&gt;
&lt;li&gt;


$$\eta(\tilde{\pi})\approx L_{\pi}(\tilde{\pi})=\eta(\pi)+\sum_s\rho_{\pi}(s)\sum_a\tilde{\pi}(a\mid s)A_{\pi}(s,a)$$
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;further&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;


$$\eta(\tilde{\pi})\geq L_{\pi}(\tilde{\pi})-\dfrac{4\epsilon\gamma}{(1-\gamma)^2}\alpha$$


$$where\ \alpha=\operatorname*{max}_{s}D_{\mathrm{KL}}(\pi(\cdot\mid s)\Vert{\tilde{\pi}}(\cdot\mid s)),\epsilon=\operatorname*{{max}}_{s,a}\Big|A_{\pi}(s,a)\Big|$$
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Another form&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;


$$\underset{\theta}{\mathrm{maximize}}\quad\hat{\mathbb{E}}_t\bigg[\dfrac{\pi_\theta(a_t\mid s_t)}{\pi_{\theta_\mathrm{old}}(a_t\mid s_{t})}\hat{A}_t\bigg]\quad$$


$$where\ \hat{\mathbb{E}}_{t}[\mathrm{KL}[\pi_{\theta\text{old}}(\cdot\mid s_{t}),\pi_{\theta}(\cdot\mid s_t)]]\leq\delta.$$
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;proof:&lt;/strong&gt;&lt;/em&gt; &lt;a href=&#34;https://github.com/opendilab/PPOxFamily/blob/main/chapter1_overview/chapter1_supp_trpo.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PPOxFamily/chapter1_supp_trpo.pdf at main · opendilab/PPOxFamily (github.com)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;ppoproximal-policy-optimization&#34;&gt;PPO(Proximal Policy Optimization)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;


$$\mathbb{E}_t[\min(\dfrac{\pi_\theta(a_t|s_t)}{\pi_{\theta_k}(a_t|s_{t})}A^{\theta_k}(s_r,a_{t}),\text{clip}(\dfrac{\pi_\vartheta(a_t|s_i)}{\pi_{\vartheta_k}(a_r|s_i)},1-\epsilon,1+\epsilon)A^{\theta_4}(s_r,a_t))]$$
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230411205650895.png&#34; alt=&#34;image-20230411205650895&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;article:&lt;/strong&gt;&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/pdf/1707.06347&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/1707.06347&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;moving-towards-real-decision-problems-multi-agent-system&#34;&gt;Moving towards Real decision Problems (Multi-Agent System)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Biological swarm intelligence (population game, co-evolution)&lt;/li&gt;
&lt;li&gt;Machine swarm intelligence (division of labor and cooperation, each performs his own duties)&lt;/li&gt;
&lt;li&gt;GameAI (Cooperative mission objective)&lt;/li&gt;
&lt;li&gt;Loss Function&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230410215615355.png&#34; alt=&#34;image-20230410215615355&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;the-basic-theory-of-multi-agent-cooperation&#34;&gt;The basic theory of multi-agent cooperation&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230411210414509.png&#34; alt=&#34;image-20230411210414509&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;general-setting-for-multi-agent-decision-making&#34;&gt;General setting for multi-agent decision making&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Joint action $(a^1, a^2, \dots, a^N) \in A^1 \times A^2 \times \dots \times A^N$&lt;/li&gt;
&lt;li&gt;Agent-wise reward $R^i = R^i(s,\ a^{-i},\ a^i), \ a^{-i}=(a^1,\dots,a^N)\setminus a^i$&lt;/li&gt;
&lt;li&gt;Individual optimality: Maximizes the cumulative discount rewards of a single agent&lt;/li&gt;
&lt;li&gt;But team optimality is not a superposition of individual optimality, and collaboration, competition, and more complexity may occur.&lt;/li&gt;
&lt;li&gt;Formally, &lt;strong&gt;Dec-POMDP:&lt;/strong&gt; $M = &amp;lt;I, S, {A_i}, P, R, {\Omega_i}, O, h &amp;gt;$
&lt;ul&gt;
&lt;li&gt;$I$, the set of agents&lt;/li&gt;
&lt;li&gt;$I$ the set of states with initial state $s_0$&lt;/li&gt;
&lt;li&gt;$A_i$, the set of actions for agent $i$, with $A = \times_iA_i$ the set of joint actions&lt;/li&gt;
&lt;li&gt;$P$, the state transition probabilities: $P(s&amp;rsquo;| \ s,\ a)$, the probability of the environment transitioning to state $s&amp;rsquo;$ given it was in state $s$ and agents took actions $a$&lt;/li&gt;
&lt;li&gt;$R$, the global reward function: $R(s,\ a)$, the immediate reward the system receives for being in state $s$ and agents taking actions $a$&lt;/li&gt;
&lt;li&gt;$\Omega_i$, the set of observations for agent $i$, with $\Omega = \times_i\Omega_i$ the set of joint observations&lt;/li&gt;
&lt;li&gt;$O$, the observation probabilities: $O(o| \ s,\ a)$, the probability of agents seeing observations $o$, given the state is $s$ and agents take actions $a$&lt;/li&gt;
&lt;li&gt;$h$, the horizon, whether infinite or if finite, a positive integer&lt;/li&gt;
&lt;li&gt;when $h$ is infinite a discount factor, $0 ≤ \gamma &amp;lt; 1$ , is used&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;ctdecentealized-training-and-decentralized-execution&#34;&gt;CTDE(centealized training and decentralized execution)&lt;/h1&gt;
&lt;h2 id=&#34;value-decomposition&#34;&gt;Value decomposition&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;IGM: individual -global -max&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;


$$\operatorname{arg}\operatorname*{max}_{a\in\mathcal{A}}Q_{t o v}(s,a)=\left(\underset{a_{1}\in\mathcal{A}}{arg\operatorname*{max}}Q_{1}\left(s_{1},a_{1}\right),\ldots,\underset{a_{n}\in\mathbb{a}}{a_{n}}\left(s_{n},a_{n}\right)\right)$$
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TD-Learning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;VDN(value decomposition networks): The value function of each agent is integrated to obtain a joint action value function&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;QMIX:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If $\dfrac{\partial Q_{tot}}{\partial Q_a}\geq0,\forall a.$, IGM must be true.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230411215314073.png&#34; alt=&#34;image-20230411215314073&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;For each agent a, there is one agent network that represents its individual value function $Q_a(\tau_a , u_a )$. We represent agent networks as DRQNs that receive the current individual observation $o^a_t$ and the last action $u^a_{t−1}$ as input at each time step, as shown in Figure c.&lt;/li&gt;
&lt;li&gt;The mixing network is a feed-forward neural network that takes the agent network outputs as input and mixes them monotonically, producing the values of $Q_{tot}$, as shown in Figure a. To enforce the monotonicity constraint of $\dfrac{\partial Q_{tot}}{\partial Q_a}\geq0,\forall a.$, the weights (but not the biases) of the mixing network are restricted to be non-negative. This allows the mixing network to approximate any monotonic function arbitrarily closely.&lt;/li&gt;
&lt;li&gt;The weights of the mixing network are produced by separate hypernetworks. Each hypernetwork takes the state s as input and generates the weights of one layer of the mixing network. Each hypernetwork consists of a single linear layer, followed by an absolute activation function, to ensure that the mixing network weights are non-negative.&lt;/li&gt;
&lt;li&gt;Loss $\mathcal{L}(\theta)=\sum_{i=1}^b\left[\left(y_i^{tot}-Q_{tot}(\tau,\mathbf{u},s;\theta)\right)^2\right],$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Others: More and more complex designs guarantee that generated  $Q_{tot}$ meet the IGM conditions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The value decomposition method may fail&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;mapg&#34;&gt;MAPG&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;MADDPG (multi-agent deep deterministic policy gradient)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230411221822530.png&#34; alt=&#34;image-20230411221822530&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;MADDPG algorithm assumes that each agent has its own independent critic network and actor network, and each agent has its own independent return function. In this way, MADDPG algorithm can simultaneously solve the multi-agent problem in cooperative environment, competitive environment and mixed environment. But the MADDPG algorithm assumes that each agent has access to the local observations and actions of all other agents during training&lt;/li&gt;
&lt;li&gt;high variance
&lt;ul&gt;
&lt;li&gt;baseline trick&lt;/li&gt;
&lt;li&gt;MAPPO&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;multi-agent credit assignment&lt;/strong&gt;: Since all agents in Dec-POMDP problem share the same global return, each agent does not know how much influence its behavior has on the global return.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MAPPO&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230411223238938.png&#34; alt=&#34;image-20230411223238938&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230411223740103.png&#34; alt=&#34;image-20230411223740103&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Implement PG with PPO&lt;/li&gt;
&lt;li&gt;Critic Input design: Agent-Specific Global State
&lt;ul&gt;
&lt;li&gt;Different critic input information needs to be prepared for each agent to generate a value function exclusive to the agent.&lt;/li&gt;
&lt;li&gt;Different global state coding designs also have significant performance implications&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;COMA&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Agent $a$ &lt;strong&gt;counterfactual baseline&lt;/strong&gt;: $A^a(s, \boldsymbol{u})=Q(s,\boldsymbol{u})-\sum_{u&amp;rsquo;^a}\pi^a(u&amp;rsquo;^a|\tau^a)Q(s,(\boldsymbol{u}^{-a},u&amp;rsquo;^a))$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The formula for calculating the strategy gradient of COMA is as follows&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;


$$g_k=\mathbb{E}_\pi[\sum_a\nabla_{\theta_k}\log\pi^a(u^a|\tau^a)A^a(s,\textbf{u})]$$
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230412021143213.png&#34; alt=&#34;image-20230412021143213&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;COMA learns a stochastic policy for discrete actions, while MADDPG learns a deterministic policy for continuous actions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;COMA is mainly used for multi-agent cooperative tasks, with one critic evaluating the team&amp;rsquo;s overall reward, while MADDPG can be used for both cooperative and competitive tasks, with each agent having a separate reward function and critic.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the original paper, COMA uses historical observations and action sequences as inputs to the network, while MADDPG does not use historical sequences.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;All agents&amp;rsquo; actor networks in COMA share parameters, while MADDPG does not.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;COMA uses counterfactual baselines as the optimization target for actor networks, while MADDPG directly uses the Q function as the optimization target for each agent&amp;rsquo;s actor network.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;shared-parameter-method&#34;&gt;Shared parameter method&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The lack of strategy exploration can worsen exponentially with the increasing number of agents.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;trick&#34;&gt;Trick&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;MA Transformer with Advantage Decomposition&lt;/li&gt;
&lt;li&gt;Param Sharing
&lt;ul&gt;
&lt;li&gt;Sharing parameters among homogeneous agents can significantly improve training performance
Improper&lt;/li&gt;
&lt;li&gt;sharing of parameters between heterogeneous agents can be a hindrance&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Mask
&lt;ul&gt;
&lt;li&gt;death mask&lt;/li&gt;
&lt;li&gt;action mask&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Entropy Balance&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;multi-agent-advantage-decomposition&#34;&gt;Multi-Agent Advantage Decomposition&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;MATRPO
&lt;ul&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/2109.11251&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2109.11251] Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dr. Yaodong Yang.&lt;/li&gt;
&lt;li&gt;Without IGM condition and shared parameter, they apply TRPO iteration under MA Settings.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/qq_45832958/article/details/123644900?spm=1001.2014.3001.5502&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(258条消息) 强化学习 | Multi Agents | Trust Region | HATRPO | HAPPO_111辄的博客-CSDN博客&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MA-Transformer
&lt;ul&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/2205.14953&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2205.14953] Multi-Agent Reinforcement Learning is a Sequence Modeling Problem (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A unified framework for multi-agent cooperative Game: Multi-agent mirror Learning：
&lt;ul&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/2208.01682&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2208.01682] Heterogeneous-Agent Mirror Learning: A Continuum of Solutions to Cooperative MARL (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.techbeat.net/talk-info?id=715&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;一个合作博弈的通用求解框架 - TechBeat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;It needs to be read carefully, but I haven&amp;rsquo;t&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;aceaction-dependent-q-learning&#34;&gt;ACE(action-dependent Q-learning)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2211.16068&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2211.16068] ACE: Cooperative Multi-agent Q-learning with Bidirectional Action-Dependency (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dr. Yaodong Yang.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Question: The instability of reward and transition breaks the Markov hypothesis followed by the reinforcement learning algorithm, which leads to the unstable updating of the value function of the agent&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Core idea: Instead of requiring multiple agents to produce actions at the same time, the cooperative action problem of multiple agents is transformed into a stable and efficient sequentially expanded MDP (SE-MDP), which models the bidirectional action dependence between agents, abstracts the most simplified cooperative representation, and makes each agent produce decision behaviors one by one. Finally, the non-stationary multi-agent decision problem is transformed into a special stationary single agent decision problem&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230412101807551.png&#34; alt=&#34;image-20230412101807551&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Decision with Rollout:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;


$$a_{i+1}^t=\arg\max_{a_{i+1}}V\left(s_{a_{1i},a_{i+1}}^t\right)$$
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update with Rollout&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230412132216002.png&#34; alt=&#34;image-20230412132216002&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;transition 

$$(s^t,a,r(s^t, a), s^{t+1})$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Decomposed State Embedding&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;


$$\mathrm{state~embeddings~}e_{s}(s_{a_{1:i}}^{t}) ,\ \mathrm{embeddings~}e_{s}(s^{t}), \mathrm{action~embeddings~}e_{a}(a_i)); $$
&lt;/li&gt;
&lt;li&gt;


$$e_s(s_{a_{1i}}^t)=\left[e_u(u_{1,a_{1i}}),...,e_u(u_{m,a_{1i}})\right]$$
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230412130142428.png&#34; alt=&#34;image-20230412130142428&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;where $P\left(a_{1,i}\right)_{j}$ is the set of all passive action embeddings whose target unit is $u_j$ . When $i\geq j,$ which means uj is an agent-controlled unit and has made its decision $a_j$ , the active embedding $e_a^a (a_j )$ will also be added to$ e_u (u_j ).$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230412104112935.png&#34; alt=&#34;image-20230412104112935&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Unit-wise State Embedding(Unit Encoder)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230412104324369.png&#34; alt=&#34;image-20230412104324369&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Interaction-aware Action Embedding&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathrm{active~embeddings~}e_{a}^a(a_i)$: encode action $a_i$&amp;rsquo;s effect on the agent $u_i$&lt;/li&gt;
&lt;li&gt;$\mathrm{passive~embeddings~}e_{a}^p(a_i)$: encode action $a_i$&amp;rsquo;s effect on the agent $u_j$&lt;/li&gt;
&lt;li&gt;$e_{a}(a_i) = [e_{a}^a(a_i), e_{a}^p(a_i)]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>LLM/MMM for RL</title>
      <link>https://example.com/post/llm-for-rl/</link>
      <pubDate>Mon, 10 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/llm-for-rl/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Large Language Models&lt;/strong&gt;: GPT BERT&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-Modal Models&lt;/strong&gt;: CLIP&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;reasoning&#34;&gt;Reasoning&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;TidyBot&lt;/strong&gt;: &amp;ldquo;Personalized Robot Assistance with Large Language Models&amp;rdquo;, &lt;em&gt;arXiv, May 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2305.05658&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jimmyyhwu/tidybot/tree/main/robot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://tidybot.cs.princeton.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]
&lt;ul&gt;
&lt;li&gt;personalization of household cleanup with robots&lt;/li&gt;
&lt;li&gt;where objects should go is highly personal, and depends on cultural norms and individual preferences&lt;/li&gt;
&lt;li&gt;Our approach is to utilize the summarization capabilities of large language models (LLMs) to provide generalization from a small number of example preferences&lt;/li&gt;
&lt;li&gt;a publicly released benchmark dataset for evaluating generalization of receptacle selection preferences&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714222509970.png&#34; alt=&#34;image-20230714222509970&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PIGLeT&lt;/strong&gt;: &amp;ldquo;PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World&amp;rdquo;, &lt;em&gt;ACL, Jun 2021&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2106.00188&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;http://github.com/rowanz/piglet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://rowanzellers.com/piglet/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]
&lt;ul&gt;
&lt;li&gt;factorize PIGLeT into a physical dynamics model, and a separate language model.&lt;/li&gt;
&lt;li&gt;read a sentence, simulate neurally what might happen next, and then communicate that result through a literal symbolic representation, or natural language.&lt;/li&gt;
&lt;li&gt;Environment: THOR&lt;/li&gt;
&lt;li&gt;$ (o_1, o_2 &amp;hellip;) \times \boldsymbol{a} \rightarrow (o^{\prime}_1, o^{\prime}_2 &amp;hellip;)$ trained by data&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715003231256.png&#34; alt=&#34;image-20230715003231256&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Matcha&lt;/strong&gt;: &amp;ldquo;Chat with the Environment: Interactive Multimodal Perception using Large Language Models&amp;rdquo;, Accepted in IROS, 2023. [&lt;a href=&#34;https://arxiv.org/pdf/2303.08268.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/xf-zhao/Matcha&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://matcha-model.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]
&lt;ul&gt;
&lt;li&gt;able to interactively perceive (“chat” with) the environment through multimodal perception when the information from passive visual perception is insufficient for completing an instructed task.&lt;/li&gt;
&lt;li&gt;The epistemic actions are executed autoregressively until the agent is confident enough about the information sufficiency in that situation.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715002541087.png&#34; alt=&#34;image-20230715002541087&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LMPrior&lt;/strong&gt;: &amp;ldquo;LMPriors: Pre-Trained Language Models as Task-Specific Priors&amp;rdquo;, &lt;em&gt;arXiv, Oct 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2210.12530&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;LMPrior as a function transform&lt;/strong&gt;. We define LMPriors as a family of functions $\mathcal{P}$ which take some relevant metadata $D_{\text{meta}}$ which is not used by the traditional learning process $f$. The LMPrior then transforms $f$ to $\tilde{f}$ which exhibits a bias towards outputs which are consistent with the metadata $\mathcal{D}_{\text {meta}}$&lt;/li&gt;
&lt;li&gt;Use the name and brief description of the label to help complete the task Feature selection, Reinforcement learning, Causal discovery&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230710120308271.png&#34; alt=&#34;image-20230710120308271&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230710120424484.png&#34; alt=&#34;image-20230710120424484&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ReAct&lt;/strong&gt;: &amp;ldquo;ReAct: Synergizing Reasoning and Acting in Language Models&amp;rdquo;, &lt;em&gt;ICLR, 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2210.03629&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ysymyth/ReAct&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://react-lm.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]
&lt;ul&gt;
&lt;li&gt;their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics.&lt;/li&gt;
&lt;li&gt;combine reasoning and acting with language models for solving diverse language reasoning and decision making tasks.&lt;/li&gt;
&lt;li&gt;Knowledge-Intensive Reasoning Tasks&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230710172745445.png&#34; alt=&#34;image-20230710172745445&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generative Agents&lt;/strong&gt;: &amp;ldquo;Generative Agents: Interactive Simulacra of Human Behavior&amp;rdquo;, &lt;em&gt;arXiv, Apr 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2304.03442v1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]
&lt;ul&gt;
&lt;li&gt;generative agents—agents that draw on generative models to simulate believable human behavior&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711003847934.png&#34; alt=&#34;image-20230711003847934&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Large Language Models as Zero-Shot Human Models for Human-Robot Interaction&amp;rdquo;, &lt;em&gt;arXiv, Mar 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2303.03548v1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]
&lt;ul&gt;
&lt;li&gt;crafting good human models is challenging&lt;/li&gt;
&lt;li&gt;how LLM-based human models can be integrated into a social robot’s planning process and applied in HRI scenarios&lt;/li&gt;
&lt;li&gt;LLM do social inference tasks&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711013413057.png&#34; alt=&#34;image-20230711013413057&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Translating Natural Language to Planning Goals with Large-Language Models&amp;rdquo;, &lt;em&gt;arXiv, Feb 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2302.05128&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]
&lt;ul&gt;
&lt;li&gt;LLMs are unable to perform accurate reasoning nor solve planning problems, which may limit their usefulness for robotics-related tasks&lt;/li&gt;
&lt;li&gt;LLMs are able to translate goals specified in natural language to a structured planning language&lt;/li&gt;
&lt;li&gt;Planning Domain Definition Language (PDDL)&lt;/li&gt;
&lt;li&gt;Few-shots&lt;/li&gt;
&lt;li&gt;Sub benchmark: Domain Understanding, Goal Inference, PDDL Goal Specification&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&amp;ldquo;PDDL Planning with Pretrained Large Language Models&amp;rdquo;, &lt;em&gt;NeurlPS, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openreview.net/forum?id=1QMMUB4zfl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://tinyurl.com/llm4pddl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;]
&lt;ul&gt;
&lt;li&gt;In certain PDDL domains, LLMs are capable of solving some nontrivial problems.&lt;/li&gt;
&lt;li&gt;However, in many other cases, LLMs cannot yet solve problems on their own.&lt;/li&gt;
&lt;li&gt;LLMs are sensitive to the semantics of the English terms used in the PDDL problems.&lt;/li&gt;
&lt;li&gt;we prompt with the original PDDL syntax, rather than domain-specific natural language encodings of the PDDL.  it makes better result.&lt;/li&gt;
&lt;li&gt;use Codex — a version of GPT-3 fine-tuned on general code to get a better result&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LLM+P&lt;/strong&gt;:&amp;ldquo;LLM+P: Empowering Large Language Models with Optimal Planning Proficiency&amp;rdquo;, &lt;em&gt;arXiv, Apr 2023&lt;/em&gt;, [&lt;a href=&#34;https://arxiv.org/abs/2304.11477&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Cranial-XIX/llm-pddl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code&lt;/a&gt;]
&lt;ul&gt;
&lt;li&gt;the first framework that incorporates the strengths of classical planners into LLMs.&lt;/li&gt;
&lt;li&gt;LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL),&lt;/li&gt;
&lt;li&gt;then leveraging classical planners to quickly find a solution,&lt;/li&gt;
&lt;li&gt;and then translating the found solution back into natural language.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Foundation Models for Decision Making: Problems, Methods, and Opportunities&amp;rdquo;, &lt;em&gt;arXiv, Mar 2023&lt;/em&gt;, [&lt;a href=&#34;https://arxiv.org/abs/2303.04129&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]
&lt;ul&gt;
&lt;li&gt;foundation models can serve as generative models of behavior (e.g., skill discovery) and generative models of the environment&lt;/li&gt;
&lt;li&gt;foundation models can serve as representation learners of states, actions, rewards, and transition dynamics&lt;/li&gt;
&lt;li&gt;language foundation models can serve as interactive agents and environments, enabling new problems and applications to be considered under a sequential decision making framework&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;planning&#34;&gt;Planning&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PromptCraft&lt;/strong&gt;: &amp;ldquo;ChatGPT for Robotics: Design Principles and Model Abilities&amp;rdquo;, &lt;em&gt;Blog, Feb 2023&lt;/em&gt;, [&lt;a href=&#34;https://www.microsoft.com/en-us/research/uploads/prod/2023/02/ChatGPT___Robotics.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/chatgpt-for-robotics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]
&lt;ul&gt;
&lt;li&gt;evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks&lt;/li&gt;
&lt;li&gt;use free-form dialog,&lt;/li&gt;
&lt;li&gt;parse XML tags,&lt;/li&gt;
&lt;li&gt;and to synthesize code&lt;/li&gt;
&lt;li&gt;use of task-specific prompting functions and closed-loop reasoning through dialogues&lt;/li&gt;
&lt;li&gt;PromptCraft, which contains a platform where researchers can collaboratively upload and vote on examples of good prompting schemes for robotics applications&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FILM&lt;/strong&gt;: &amp;ldquo;FILM: Following Instructions in Language with Modular Methods&amp;rdquo;, &lt;em&gt;ICLR, 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2110.07342&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/soyeonm/FILM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://soyeonm.github.io/FILM_webpage/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]
&lt;ul&gt;
&lt;li&gt;embodied instruction&lt;/li&gt;
&lt;li&gt;(1) builds a semantic map of the scene&lt;/li&gt;
&lt;li&gt;(2) performs exploration with a semantic search policy&lt;/li&gt;
&lt;li&gt;ALFRED env&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715155943746.png&#34; alt=&#34;image-20230715155943746&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Don&amp;rsquo;t Copy the Teacher&lt;/strong&gt;: &amp;ldquo;Don’t Copy the Teacher: Data and Model Challenges in Embodied Dialogue&amp;rdquo;, &lt;em&gt;EMNLP, 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2210.04443&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.youtube.com/watch?v=qGPC65BDJw4&amp;amp;t=2s&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]
&lt;ul&gt;
&lt;li&gt;The article discusses TEACh, a new task system that focuses on Entire Dialogue History (EDH) and Trajectory from Dialogue (TfD), in which agents complete household tasks based on text and visual cues.&lt;/li&gt;
&lt;li&gt;In EDH, agents predict actions from partial dialogue history, while in TfD, they&amp;rsquo;re expected to complete a whole task from a full dialogue history.&lt;/li&gt;
&lt;li&gt;The authors present three models for these tasks: ET, a direct sequence imitation transformer, FILM, a four-module system originally designed for ALFRED tasks, and Symbiote, a competitive modular method for EDH.&lt;/li&gt;
&lt;li&gt;FILM was adapted for TEACh by refactoring the code to accept dialogue history as input, and retraining the model.&lt;/li&gt;
&lt;li&gt;The paper primarily investigates how to improve task execution by agents using dialogue history and how to evaluate their performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PaLM-E&lt;/strong&gt;: &amp;ldquo;PaLM-E: An Embodied Multimodal Language Model&amp;rdquo;, &lt;em&gt;arXiv, Mar 2023&lt;/em&gt;, [&lt;a href=&#34;https://arxiv.org/abs/2303.03378&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://palm-e.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Webpage&lt;/a&gt;]
&lt;ul&gt;
&lt;li&gt;grounding end-to-end model&lt;/li&gt;
&lt;li&gt;Input :multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings.&lt;/li&gt;
&lt;li&gt;in conjunction with a pretrained large language mode&lt;/li&gt;
&lt;li&gt;robotic manipulation planning, visual question answering, and captioning&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714234352286.png&#34; alt=&#34;image-20230714234352286&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RT-1&lt;/strong&gt;: &amp;ldquo;RT-1: Robotics Transformer for Real-World Control at Scale&amp;rdquo;, &lt;em&gt;arXiv, Dec 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2212.06817&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google-research/robotics_transformer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;] [&lt;a href=&#34;https://robotics-transformer.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]
&lt;ul&gt;
&lt;li&gt;open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data&lt;/li&gt;
&lt;li&gt;based on a large-scale data collection on real robots performing real-world tasks&lt;/li&gt;
&lt;li&gt;output is plan&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714235720100.png&#34; alt=&#34;image-20230714235720100&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;InnerMonlogue&lt;/strong&gt;: &amp;ldquo;Inner Monologue: Embodied Reasoning through Planning with Language Models&amp;rdquo;, &lt;em&gt;arXiv, July 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2207.05608&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://innermonologue.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;system Inner Monologue chains together these various components (perception models, robotic skills, and human feedback) in a shared language prompt, enabling it to successfully perform user instructions.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230410093435285.png&#34; alt=&#34;image-20230410093435285&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Inner Monologue(Left): InstructGPT&lt;/li&gt;
&lt;li&gt;pick-and-place primitive: CLIPort&lt;/li&gt;
&lt;li&gt;Manipulation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Housekeep&lt;/strong&gt;: &amp;ldquo;Housekeep: Tidying Virtual Households using Commonsense Reasoning&amp;rdquo;, &lt;em&gt;arXiv, May 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2205.10712&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yashkant/housekeep&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://yashkant.github.io/housekeep/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;New benchmark: In Housekeep, an embodied agent must tidy a house by rearranging misplaced objects without explicit instructions specifying which objects need to be rearranged&lt;/li&gt;
&lt;li&gt;A new dataset for an object (e.g. salt-shaker), a room (e.g. kitchen) in context, to classify into the following categories: misplaced, correct, implausible.&lt;/li&gt;
&lt;li&gt;Each Housekeep episode is created by instantiating 7-10 objects within a scene, out of which 3-5 objects are misplaced and the remaining are placed correct&lt;/li&gt;
&lt;li&gt;Object Room $[OR] &amp;ndash; P(room|object)$ : Generate compatibility scores for rooms for a given object.&lt;/li&gt;
&lt;li&gt;Object Room Receptacle $[ORR] &amp;ndash; P(receptacle|object, room)$: Generate compatibility scores for receptacles within a given room and for a given object&lt;/li&gt;
&lt;li&gt;Method1: Finetuning by Contrastive Matching (CM). We train a 3-layered MLP on top of language embeddings and compute pairwise cosine similarity between any two embeddings&lt;/li&gt;
&lt;li&gt;Method2: Zero-Shot Ranking via MLM (ZS-MLM).&lt;/li&gt;
&lt;li&gt;LLM: BERT for word embeddings&lt;/li&gt;
&lt;li&gt;MLM(Masked Language Model)&lt;/li&gt;
&lt;li&gt;Similarity calculation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LID&lt;/strong&gt;: &amp;ldquo;Pre-Trained Language Models for Interactive Decision-Making&amp;rdquo;, &lt;em&gt;arXiv, Feb 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2202.01771&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ShuangLI59/Language-Model-Pre-training-Improves-Generalization-in-Policy-Learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://shuangli-project.github.io/Pre-Trained-Language-Models-for-Interactive-Decision-Making/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409121632758.png&#34; alt=&#34;image-20230409121632758&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Fine-tuning&lt;/li&gt;
&lt;li&gt;Mid-level actions&lt;/li&gt;
&lt;li&gt;LLM&lt;/li&gt;
&lt;li&gt;As a part of policy network.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LLM-BRAIn&lt;/strong&gt;: &amp;ldquo;LLM-BRAIn: AI-driven Fast Generation of Robot Behaviour Tree based on Large Language Model&amp;rdquo;, &lt;em&gt;arXiv, May 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2305.19352&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLM-BRAIn is a transformer-based Large Language Model (LLM) finetuned from Stanford Alpaca 7B model to generate robot behavior tree (BT) from the text description.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714211624993.png&#34; alt=&#34;image-20230714211624993&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MOO&lt;/strong&gt;: &amp;ldquo;Open-World Object Manipulation using Pre-Trained Vision-Language Models&amp;rdquo;, &lt;em&gt;arXiv, Mar 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2303.00905&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://robot-moo.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To allow robots to generalize to new semantic concepts, we choose to leverage VLMs&lt;/li&gt;
&lt;li&gt;position information to encode&lt;/li&gt;
&lt;li&gt;imitation learning&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711180547447.png&#34; alt=&#34;image-20230711180547447&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CALM&lt;/strong&gt;: &amp;ldquo;Keep CALM and Explore: Language Models for Action Generation in Text-based Games&amp;rdquo;, &lt;em&gt;arXiv, Oct 2020&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2010.03903v1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/princeton-nlp/calm-textgame&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;text-based games.&lt;/li&gt;
&lt;li&gt;train a model for action choice&lt;/li&gt;
&lt;li&gt;Deep Reinforcement Relevance Network (DRRN) . This allows our model to combine generic linguistic priors for action generation with the ability to adaptively choose actions that are best suited for the game.&lt;/li&gt;
&lt;li&gt;dataset of 426 human gameplay transcripts for 590 different text-based games.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711102810528.png&#34; alt=&#34;image-20230711102810528&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ZSP&lt;/strong&gt;: &amp;ldquo;Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents&amp;rdquo;, &lt;em&gt;ICML, Jan 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2201.07207&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/huangwl18/language-planner&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://wenlong.page/language-planner/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230410095827203.png&#34; alt=&#34;image-20230410095827203&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;VirtualHome Executability Correctness&lt;/li&gt;
&lt;li&gt;Planning LM (Causal LLM): GPT-3 $\hat{a}$&lt;/li&gt;
&lt;li&gt;Translation LM(Masked LLM): BERT&lt;/li&gt;
&lt;li&gt;for each admissible environment action $a_e$ $max\ C(f(\hat{a}), f(a_e)) = \frac{f(\hat{a})\cdot f(a_e)}{||f(\hat{a})||\ ||f(a_e)||}$&lt;/li&gt;
&lt;li&gt;Mid-level actions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ChatGPT-Prompts&lt;/strong&gt;: &amp;ldquo;ChatGPT Empowered Long-Step Robot Control in Various Environments: A Case Application&amp;rdquo;, &lt;em&gt;arXiv, Jun 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2304.03893&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/microsoft/ChatGPT-Robot-Manipulation-Prompts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VirtualHome&lt;/li&gt;
&lt;li&gt;ChatGPT prompt engineering
&lt;ul&gt;
&lt;li&gt;Output a sequence of predefined robot actions with explanations in a readable JSON format.&lt;/li&gt;
&lt;li&gt;Represent the operating environment in a formalized style.&lt;/li&gt;
&lt;li&gt;Infer and output the updated state of the operating environment, which can be reused as the next input, allowing ChatGPT to operate based solely on the memory of the latest operations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Planning with Large Language Models via Corrective Re-prompting&amp;rdquo;, &lt;em&gt;arXiv, Nov 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2311.09935&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VirtualHome&lt;/li&gt;
&lt;li&gt;When an agent is unable to execute an action, our approach re-prompts the LLM with precondition error information to extract an executable corrective action to achieve the intended goal in the current context.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711162943164.png&#34; alt=&#34;image-20230711162943164&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions&amp;rdquo;, &lt;em&gt;arXiV, Oct 2020&lt;/em&gt;, [&lt;a href=&#34;https://arxiv.org/abs/2009.14259&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The ALFRED challenge task: a virtual robotic agent to complete everyday tasks  executing complex visually-grounded semantic plans&lt;/li&gt;
&lt;li&gt;successful plans from language directives alone without any visual input in 26% of unseen cases&lt;/li&gt;
&lt;li&gt;models using input from a single modality often performed nearly as good as or better than their multi-modal counterparts&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TIP&lt;/strong&gt;: &amp;ldquo;Multimodal Procedural Planning via Dual Text-Image Prompting&amp;rdquo;, &lt;em&gt;arXiV, May 2023&lt;/em&gt;, [&lt;a href=&#34;https://arxiv.org/abs/2305.01795&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the potential of providing instructions informed by texts and images to assist humans in completing tasks remains underexplored&lt;/li&gt;
&lt;li&gt;given a high-level goal and generate plans of paired text-image steps, providing more complementary and informative guidance than unimodal plans.&lt;/li&gt;
&lt;li&gt;Visual-Grounded Text Plan Text-Image Prompt&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714192447413.png&#34; alt=&#34;image-20230714192447413&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714192511754.png&#34; alt=&#34;image-20230714192511754&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VLaMP&lt;/strong&gt;: &amp;ldquo;Pretrained Language Models as Visual Planners for Human Assistance&amp;rdquo;, &lt;em&gt;arXiV, Apr 2023&lt;/em&gt;, [&lt;a href=&#34;https://arxiv.org/abs/2304.09179&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;‘Visual Planning for Assistance (VPA)’. Given a goal briefly described in natural language and a video of the user’s progress so far, the aim of VPA is to obtain a plan.&lt;/li&gt;
&lt;li&gt;Forecasting in Videos&lt;/li&gt;
&lt;li&gt;Fine-tune on LLM&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714193601697.png&#34; alt=&#34;image-20230714193601697&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Text2Motion&lt;/strong&gt;: &amp;ldquo;Text2Motion: From Natural Language Instructions to Feasible Plans&amp;rdquo;, &lt;em&gt;arXiV, Mar 2023&lt;/em&gt;, [&lt;a href=&#34;https://arxiv.org/abs/2303.12153&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/stanford.edu/text2motion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;interfaces an &lt;strong&gt;LLM&lt;/strong&gt; with a library of learned skills and a &lt;strong&gt;geometric feasibility planner&lt;/strong&gt; to solve complex sequential manipulation tasks&lt;/li&gt;
&lt;li&gt;infers goal states from a natural language instruction&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230713225625008.png&#34; alt=&#34;image-20230713225625008&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LLM-planner&lt;/strong&gt;: &amp;ldquo;LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models&amp;rdquo;, &lt;em&gt;arXiv, Mar 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2212.04088&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/OSU-NLP-Group/LLM-Planner/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://dki-lab.github.io/LLM-Planner/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;different from SayCan, we use LLMs to directly generate plans instead of ranking admissible skills&lt;/li&gt;
&lt;li&gt;its ability to dynamically re-plan based on what the agent observes in the current environment&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230713224102931.png&#34; alt=&#34;image-20230713224102931&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;kNN to get nearest plan&lt;/li&gt;
&lt;li&gt;ALFRED env&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GD&lt;/strong&gt;: &amp;ldquo;Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control&amp;rdquo;, &lt;em&gt;arXiv, Mar 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2303.00855&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://grounded-decoding.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;decodes the token probability of an LLM and token probabilities from token-conditioned, robotic functions, such as &lt;strong&gt;affordance, safety, preference&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230713233627295.png&#34; alt=&#34;image-20230713233627295&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Grounded Models: CLIPort, etc&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;COWP&lt;/strong&gt;: &amp;ldquo;Robot Task Planning and Situation Handling in Open Worlds&amp;rdquo;, &lt;em&gt;arXiv, Oct 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2210.01287&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yding25/GPT-Planner&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://cowplanning.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;acquire common sense from LLMs, and aim to improve the task completion and situation handling skills of service robots.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230713205214315.png&#34; alt=&#34;image-20230713205214315&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230713205228824-1.png&#34; alt=&#34;image-20230713205228824&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GLAM&lt;/strong&gt;: &amp;ldquo;Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning&amp;rdquo;, &lt;em&gt;arXiv, May 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2302.02662&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/flowersteam/Grounding_LLMs_with_online_RL&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230709192428587.png&#34; alt=&#34;image-20230709192428587&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&amp;ldquo;functional grounding&amp;rdquo; problem, is relative to a particular environment which may be the human physical environment but also more abstract interactive environments simulated in computers (where abstract physics can differ from human environments.&lt;/li&gt;
&lt;li&gt;textual worlds, Language-conditioned RL&lt;/li&gt;
&lt;li&gt;finetune LLM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Reward Design with Language Models&amp;rdquo;, &lt;em&gt;ICML, Feb 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2303.00001v1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/minaek/reward_design_with_llms&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We evaluate whether our approach can train agents aligned with user objectives.&lt;/li&gt;
&lt;li&gt;Given a few examples or a description demonstrating the user’s objective, an LLM should be able to provide an accurate instantiation of reward values on a new test example, allowing for easier generalization to new objectives.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230709205456525.png&#34; alt=&#34;image-20230709205456525&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;This article directly constructs a reward generation system related to the expected reward prompt. Similar to previous works, this study is also subject to limitations in specific domains (those related to common-sense knowledge) and specific environments (those that are easy to express in text and have short decision cycles), making it difficult to use LLMs for reward design in general domains. However, this paper, along with the ELLM, demonstrates the power of pre-training large models and shows that LLMs can be well integrated into RL to perform various roles, such as assisting in exploration and reward design, among others.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LLM-MCTS&lt;/strong&gt;: &amp;ldquo;Large Language Models as Commonsense Knowledge for Large-Scale Task Planning&amp;rdquo;, &lt;em&gt;arXiv, May 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2305.14078v1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;whether the knowledge of LLMs regarding states of the world is more complete than their knowledge of policies for accomplishing daily tasks.&lt;/li&gt;
&lt;li&gt;LLMs provide prior common sense beliefs of the world that can be used to sample likely states.&lt;/li&gt;
&lt;li&gt;MCTS summarizes the useful information in searching the likely states through the estimated Q value (the expected reward after taking action) so as to make a reasonable decision.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230709221701427.png&#34; alt=&#34;image-20230709221701427&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Collaborating with language models for embodied reasoning&amp;rdquo;, &lt;em&gt;NeurIPS, Feb 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2302.00763v1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One major issue is that LSLMs are not embodied or grounded. They do not have a way to directly take actions in embodied environments, or of knowing what is happening in an environment.&lt;/li&gt;
&lt;li&gt;The Planner is the LSLM—it reads the task description, does any required logical reasoning, and breaks the problem down into a sequence of simple instructions.&lt;/li&gt;
&lt;li&gt;These instructions are passed to the Actor, which is an RL agent programmed to complete a small set of simple instructions in the environment.&lt;/li&gt;
&lt;li&gt;Finally, to complete the feedback loop, we have the Reporter, which observes the environment and reports information back to the Planner so it can adjust the instructions it issues.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230710181542462.png&#34; alt=&#34;image-20230710181542462&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LLM-Brain&lt;/strong&gt;: &amp;ldquo;LLM as A Robotic Brain: Unifying Egocentric Memory and Control&amp;rdquo;, arXiv, Apr 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.09349v1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The LLM-Brain framework integrates multiple multimodal language models for robotic tasks















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711010819272.png&#34; alt=&#34;image-20230711010819272&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Eye (VLM): BLIP2 [13], a VQA model;&lt;/li&gt;
&lt;li&gt;Nerve (LLM): ChatGPT&lt;/li&gt;
&lt;li&gt;Brain (LLM): ChatGPT action space: move_forward, turn_left, turn_right, stop&lt;/li&gt;
&lt;li&gt;Active Exploration and Embodied Question Answering&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Socratic&lt;/strong&gt;: &amp;ldquo;Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language&amp;rdquo;, &lt;em&gt;arXiv, Apr 2021&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2204.00598&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://socraticmodels.github.io/#code&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://socraticmodels.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;different large pretrained models store different forms of commonsense knowledge&lt;/li&gt;
&lt;li&gt;multiple pretrained models may be composed zero-shot i.e., via multimodalinformed prompting,&lt;/li&gt;
&lt;li&gt;formulating video understanding as a reading comprehension problem&lt;/li&gt;
&lt;li&gt;activity $=f_{\mathrm{LM}}\left(f_{\mathrm{VLM}}\left(f_{\mathrm{LM}}\left(f_{\mathrm{ALM}}\left(f_{\mathrm{LM}}\left(f_{\mathrm{VLM}}(\right.\right.\right.\right.\right.$$video$$\left.\left.\left.\left.\left.)\right)\right)\right)\right)\right)$&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711012319689.png&#34; alt=&#34;image-20230711012319689&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Code-As-Policies&lt;/strong&gt;: &amp;ldquo;Code as Policies: Language Model Programs for Embodied Control&amp;rdquo;, &lt;em&gt;arXiv, Sept 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2209.07753&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google-research/google-research/tree/master/code_as_policies&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colab&lt;/a&gt;] [&lt;a href=&#34;https://code-as-policies.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This leads us to ask: how can LLMs be applied beyond just planning a sequence of skills?&lt;/li&gt;
&lt;li&gt;Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230710104335405.png&#34; alt=&#34;image-20230710104335405&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ProgPrompt&lt;/strong&gt;: &amp;ldquo;Generating Situated Robot Task Plans using Large Language Models&amp;rdquo;, &lt;em&gt;arXiv, Sept 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2209.11302&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/progprompt/progprompt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://progprompt.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;introduces situated-awareness in LLM-based robot task planning (program with assert)&lt;/li&gt;
&lt;li&gt;open-loop without any environment interaction&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715001819415.png&#34; alt=&#34;image-20230715001819415&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Instruct2Act&lt;/strong&gt;: &amp;ldquo;Mapping Multi-modality Instructions to Robotic Actions with Large Language Model&amp;rdquo;, &lt;em&gt;arXiv, May 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2305.11176.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/OpenGVLab/Instruct2Act&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CaP is restricted to what the perception APIs can provide and struggle to interpret longer and more complex commands due to the high precision requirements of the code.&lt;/li&gt;
&lt;li&gt;Perception with off-the-shelf Foundation Models: Segment Anything Model (SAM) accurately locates candidate objects, and CLIP classifies them.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714221335753.png&#34; alt=&#34;image-20230714221335753&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Say-Can&lt;/strong&gt;: &amp;ldquo;Do As I Can, Not As I Say: Grounding Language in Robotic Affordances&amp;rdquo;, &lt;em&gt;arXiv, Apr 2021&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2204.01691&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://say-can.github.io/#open-source&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colab&lt;/a&gt;] [&lt;a href=&#34;https://say-can.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230710102809920.png&#34; alt=&#34;image-20230710102809920&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;manipulation&#34;&gt;Manipulation&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ProgramPort&lt;/strong&gt;:&amp;ldquo;Programmatically Grounded, Compositionally Generalizable Robotic Manipulation&amp;rdquo;, &lt;em&gt;ICLR, Apr 2023&lt;/em&gt;, [&lt;a href=&#34;https://arxiv.org/abs/2304.13826&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://progport.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]
&lt;ul&gt;
&lt;li&gt;Given the natural language instruction, we first use a Combinatory Categorial Grammar (CCG)&lt;/li&gt;
&lt;li&gt;The program consists of functional modules that are either visual grounding modules (e.g., locate all objects of a given category) or action policies (e.g., produce a control parameter).&lt;/li&gt;
&lt;li&gt;This enables us to directly leverage a pretrained VL model to ground singular, independent categories or attribute descriptors to their corresponding pixels, and thus disentangles the learning of visual grounding and action policies&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715211647390.png&#34; alt=&#34;image-20230715211647390&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DIAL&lt;/strong&gt;:&amp;ldquo;Robotic Skill Acquistion via Instruction Augmentation with Vision-Language Models&amp;rdquo;, &lt;em&gt;arXiv, Nov 2022&lt;/em&gt;, [&lt;a href=&#34;https://arxiv.org/abs/2211.11736&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://instructionaugmentation.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409120027347.png&#34; alt=&#34;image-20230409120027347&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;CLIP Model&lt;/li&gt;
&lt;li&gt;Few Shot/Fine-tuning&lt;/li&gt;
&lt;li&gt;Manipulation trajectories&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CLIP-Fields&lt;/strong&gt;:&amp;ldquo;CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory&amp;rdquo;, &lt;em&gt;arXiv, Oct 2022&lt;/em&gt;, [&lt;a href=&#34;https://arxiv.org/abs/2210.05663&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/notmahi/clip-fields&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyTorch Code&lt;/a&gt;] [&lt;a href=&#34;https://mahis.life/clip-fields/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;weakly supervised semantic neural fields, called CLIP-Fields.&lt;/li&gt;
&lt;li&gt;dataset comes from LLM/MMM&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409211255049.png&#34; alt=&#34;image-20230409211255049&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;$g(x, y, z) : R_3 → R_d$ serves as a generic differentiable spatial database.  trained.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409211327053.png&#34; alt=&#34;image-20230409211327053&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;CLIP BERT&lt;/li&gt;
&lt;li&gt;EVALUATION:  Instance and semantic segmentation in scene images.  for visual encoder&lt;/li&gt;
&lt;li&gt;EVALUATION:   Semantic Navigation on Robot with CLIP-Fields as Semantic-Spatial Memory.  maximizing their similarity.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LaTTe&lt;/strong&gt;: &amp;ldquo;LaTTe: Language Trajectory TransformEr&amp;rdquo;, &lt;em&gt;arXiv, Aug 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2208.02918&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/arthurfenderbucker/NL_trajectory_reshaper&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TensorFlow Code&lt;/a&gt;] [&lt;a href=&#34;https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/robot-language/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409122254407.png&#34; alt=&#34;image-20230409122254407&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;BERT CLIP&lt;/li&gt;
&lt;li&gt;trained and evaluated the model described in Section III over a dataset containing 100k examples of procedurally generated trajectory modification.&lt;/li&gt;
&lt;li&gt;Trajectory reshaping obeying user’s constraints.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Robots Enact Malignant Stereotypes&lt;/strong&gt;: &amp;ldquo;Robots Enact Malignant Stereotypes&amp;rdquo;, &lt;em&gt;FAccT, Jun 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2207.11569&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ahundt/RobotsEnactMalignantStereotypes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/robots-enact-stereotypes/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;] [&lt;a href=&#34;https://www.washingtonpost.com/technology/2022/07/16/racist-robots-ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Washington Post&lt;/a&gt;] [&lt;a href=&#34;https://www.wired.com/story/how-to-stop-robots-becoming-racist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wired&lt;/a&gt;] (code access on request)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;experiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy.&lt;/li&gt;
&lt;li&gt;Ethical experiments on LLM/MMM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ATLA&lt;/strong&gt;: &amp;ldquo;Leveraging Language for Accelerated Learning of Tool Manipulation&amp;rdquo;, &lt;em&gt;CoRL, Jun 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2206.13074&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;combining linguistic information and meta-learning significantly accelerates tool learning&lt;/li&gt;
&lt;li&gt;tools $ \mathscr{T} = { \tau_i }&lt;em&gt;{i=1}^K$     corresponding language descriptions $L_i = {l&lt;/em&gt;{ij}}&lt;em&gt;{j=1}^{N_i}\ l&lt;/em&gt;{ij} \in  \mathscr{L}$ by LLM&lt;/li&gt;
&lt;li&gt;Goal $\pi_{\theta}:  \mathscr{O} \times  \mathscr{L} \rightarrow  \mathscr{A}$ that can be quickly fine-tuned at test time.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230410005237562.png&#34; alt=&#34;image-20230410005237562&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;GPT&lt;/li&gt;
&lt;li&gt;RL: PPO Meta-training&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ZeST&lt;/strong&gt;: &amp;ldquo;Can Foundation Models Perform Zero-Shot Task Specification For Robot Manipulation?&amp;rdquo;, &lt;em&gt;L4DC, Apr 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2204.11134&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409120237050.png&#34; alt=&#34;image-20230409120237050&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Zero Shot&lt;/li&gt;
&lt;li&gt;CLIP Model&lt;/li&gt;
&lt;li&gt;Similarity calculation&lt;/li&gt;
&lt;li&gt;As Reward Function in offline RL&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LSE-NGU&lt;/strong&gt;: &amp;ldquo;Semantic Exploration from Language Abstractions and Pretrained Representations&amp;rdquo;, &lt;em&gt;arXiv, Apr 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2204.05080&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Encourage to do exploration $max\ E[\sum_{t=0}^H\gamma^t(r_t^e+\beta r_t^i)]$&lt;/li&gt;
&lt;li&gt;exploration method calculates the intrinsic reward from $O_L$ or $O_V$ .&lt;/li&gt;
&lt;li&gt;CLIP Bert ALM&lt;/li&gt;
&lt;li&gt;Zero-Shot for calculating the intrinsic reward (NGP: L2 distances between the current state and the k-nearest neighbor representations stored in the memory buffer, RND:  mean squared error $|| f_V(O_V) - \hat{f_V}(O_V)|| $ , $f_V(O_V)$ is trainable, $\hat{f_V}(O_V)$ is pre-trained model)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Embodied-CLIP&lt;/strong&gt;: &amp;ldquo;Simple but Effective: CLIP Embeddings for Embodied AI &amp;ldquo;, &lt;em&gt;CVPR, Nov 2021&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2111.09888&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/allenai/embodied-clip&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409144829744.png&#34; alt=&#34;image-20230409144829744&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Embodied AI tasks: agents that learn to navigate and interact with their environments.&lt;/li&gt;
&lt;li&gt;CLIP for visual encoder&lt;/li&gt;
&lt;li&gt;RL: PPO&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CLIPort&lt;/strong&gt;: &amp;ldquo;CLIPort: What and Where Pathways for Robotic Manipulation&amp;rdquo;, &lt;em&gt;CoRL, Sept 2021&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2109.12098&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cliport/cliport&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://cliport.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230410010919998.png&#34; alt=&#34;image-20230410010919998&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Transporter for Pick-and-Place: The same architecture has three different networks $f_{pick}, \Psi_{query}, \Psi_{key}$&lt;/li&gt;
&lt;li&gt;CLIP&lt;/li&gt;
&lt;li&gt;Trajectory Imitative learning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;R3M&lt;/strong&gt;:&amp;ldquo;R3M: A Universal Visual Representation for Robot Manipulation&amp;rdquo;, * arXiv, Nov 2022*, [&lt;a href=&#34;https://arxiv.org/abs/2203.12601&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/facebookresearch/r3m&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://tinyurl.com/robotr3m&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;can visual representations pre-trained on diverse human videos enable efficient downstream learning of robotic manipulation skills?&lt;/li&gt;
&lt;li&gt;(1) time contrastive learning to learn a representation that captures temporal dynamics, (2) video-language alignment to capture semantically relevant features of the scene, and (3) L1 and L2 penalties to encourage sparsity.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230713210643467.png&#34; alt=&#34;image-20230713210643467&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VIP&lt;/strong&gt;:&amp;ldquo;Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training&amp;rdquo;, &lt;em&gt;ICLR, 2023&lt;/em&gt;, [&lt;a href=&#34;https://arxiv.org/abs/2210.00030&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/facebookresearch/vip&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/vip-rl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pre-trained on large-scale, in-the-wild human videos, frozen VIP network can provide visual reward and representation for downstream unseen robotics tasks and enable diverse visuomotor control strategies without any task-specific fine-tuning.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LIV&lt;/strong&gt;:&amp;ldquo;LIV: Language-Image Representations and Rewards for Robotic Control&amp;rdquo;, &lt;em&gt;arXiv, Jun 2023&lt;/em&gt;, [&lt;a href=&#34;https://arxiv.org/abs/2306.00958&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/penn-pal-lab/LIV&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://penn-pal-lab.github.io/LIV/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a multi-modal representation that implicitly encodes a universal value function for tasks specified as language or image goals.&lt;/li&gt;
&lt;li&gt;pretrain and can be fine-tuned&lt;/li&gt;
&lt;li&gt;LIV’s multi-modal value representations enable diverse visuomotor control applications&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LILAC&lt;/strong&gt;:&amp;ldquo;No, to the Right – Online Language Corrections for Robotic Manipulation via Shared Autonomy&amp;rdquo;, &lt;em&gt;arXiv, Jan 2023&lt;/em&gt;, [&lt;a href=&#34;https://arxiv.org/abs/2301.02555&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Stanford-ILIAD/lilac&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;human gives an instruction, then the robot executes autonomously &amp;ndash; lack of adaptivity&lt;/li&gt;
&lt;li&gt;natural language corrections&lt;/li&gt;
&lt;li&gt;Learning from Language &amp;amp; Demonstrations&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230713163941618.png&#34; alt=&#34;image-20230713163941618&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NLMap&lt;/strong&gt;:&amp;ldquo;Open-vocabulary Queryable Scene Representations for Real World Planning&amp;rdquo;, &lt;em&gt;arXiv, Oct 2023&lt;/em&gt;, [&lt;a href=&#34;https://arxiv.org/abs/2209.09874&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nlmap-saycan.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;establishes a natural language queryable scene representation with VLMs(CLIP VLiD)&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714005719623.png&#34; alt=&#34;image-20230714005719623&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;NLMap + SayCan&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714005745857.png&#34; alt=&#34;image-20230714005745857&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VIMA&lt;/strong&gt;:&amp;ldquo;VIMA: General Robot Manipulation with Multimodal Prompts&amp;rdquo;, &lt;em&gt;arXiv, Oct 2022&lt;/em&gt;, [&lt;a href=&#34;https://arxiv.org/abs/2210.03094&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/vimalabs/VIMA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://vimalabs.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a transformer-based generalist robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409213812956.png&#34; alt=&#34;image-20230409213812956&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Mask R-CNN ViT for T5 Encoder&lt;/li&gt;
&lt;li&gt;Build his own MMM&lt;/li&gt;
&lt;li&gt;Trajectory Imitative learning on discretization action space&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Perceiver-Actor&lt;/strong&gt;:&amp;ldquo;A Multi-Task Transformer for Robotic Manipulation&amp;rdquo;, &lt;em&gt;CoRL, Sep 2022&lt;/em&gt;. [&lt;a href=&#34;https://peract.github.io/paper/peract_corl2022.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/peract/peract&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://peract.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PERACT encodes a sequence of RGB-D voxel(3D info) patches and predicts discretized translations,&lt;/li&gt;
&lt;li&gt;PERACT is essentially a classifier trained with supervised learning to detect actions&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409220107716.png&#34; alt=&#34;image-20230409220107716&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Language Encoder: CLIP&lt;/li&gt;
&lt;li&gt;Voxel Encoder pre-trained 3D convolution network&lt;/li&gt;
&lt;li&gt;Build his own MMM&lt;/li&gt;
&lt;li&gt;Trajectory Imitative learning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LLM-GROP&lt;/strong&gt;:&amp;ldquo;Task and Motion Planning with Large Language Models for Object Rearrangement&amp;rdquo;, &lt;em&gt;arXiv, May 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2303.06247&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/llm-grop&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multi-object rearrangement&lt;/li&gt;
&lt;li&gt;How does a robot figure out a fork should be placed on the left of a plate and a knife on the right? Considerable commonsense knowledge is needed.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230713203801153.png&#34; alt=&#34;image-20230713203801153&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Towards a Unified Agent with Foundation Models&amp;rdquo;, &lt;em&gt;ICLR, 2023&lt;/em&gt;. [&lt;a href=&#34;https://www.semanticscholar.org/paper/TOWARDS-A-UNIFIED-AGENT-WITH-FOUNDATION-MODELS-Palo-Byravan/67188a50e1d8a601896f1217451b99f646af4ac8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLM + VLM + Grounding Instructions into Actions(trained from scratch within an RL loop)&lt;/li&gt;
&lt;li&gt;get inner reward: CLIP computing the similarity, as dot product, between observations and text descriptions.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711112333962.png&#34; alt=&#34;image-20230711112333962&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ELLM&lt;/strong&gt;:&amp;ldquo;Guiding Pretraining in Reinforcement Learning with Large Language Models&amp;rdquo;, &lt;em&gt;arXiv, Feb 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2302.06692.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Intrinsically motivated exploration methods address this limitation by rewarding agents for visiting novel states or transitions, but these methods offer limited benefits in large environments where most discovered novelty is irrelevant for downstream tasks.&lt;/li&gt;
&lt;li&gt;Reward design for Intrinsically motivated exploration.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230709201834168.png&#34; alt=&#34;image-20230709201834168&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;The reward is determined by the cosine similarity between the transition and each sub_goal. The closer it is to a certain goal, the greater the reward.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Language Instructed Reinforcement Learning for Human-AI Coordination&amp;rdquo;, &lt;em&gt;arXiv, Jun 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2304.07297&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;multi-agent reinforcement learning (RL) often converges to different equilibria from the ones that humans prefer.&lt;/li&gt;
&lt;li&gt;use pretrained large language models to generate a prior policy conditioned on the human instruction and use the prior to regularize the RL objective&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714210709287.png&#34; alt=&#34;image-20230714210709287&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents&amp;rdquo;, &lt;em&gt;arXiv, Jun 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2306.03314&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The objective of this paper is to explore and demonstrate the potential of having multiple agents within a black-box environment to enhance collaboration and problem-solving.&lt;/li&gt;
&lt;li&gt;The specific goals include: introducing a general framework that paves the way for the creation of more powerful AGI models;&lt;/li&gt;
&lt;li&gt;introducing an adaptive and dynamic system that can adjust itself to suit the tasks at hand;&lt;/li&gt;
&lt;li&gt;and exploring the possibility of using multiple LLMs in a collaborative manner.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Co-LLM-Agents&lt;/strong&gt;:&amp;ldquo;Building Cooperative Embodied Agents Modularly with Large Language Models&amp;rdquo;, &lt;em&gt;arXiv, Jul 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2307.02485&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://vis-www.cs.umass.edu/Co-LLM-Agents/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;investigate whether LLMs can help build cooperative embodied agents that can collaborate with other agents and humans to accomplish complex tasks through collaborative planning and communication&lt;/li&gt;
&lt;li&gt;human subjects to perform the experiments under four scenarios: cooperating with the HP Agent2 , LLM Agent, LLM Agent w/o communication, and doing the task alone.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230716170146348.png&#34; alt=&#34;image-20230716170146348&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Wireless Multi-Agent Generative AI: From Connected Intelligence to Collective Intelligence&amp;rdquo;, &lt;em&gt;arXiv, Jul 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2307.02757&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cloud-based LLMs vs On-device LLMs MAS&lt;/li&gt;
&lt;li&gt;power allocation on mobile users to minimize network power consumption&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230716171126769.png&#34; alt=&#34;image-20230716171126769&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems&amp;rdquo;, &lt;em&gt;arXiv, Jul 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2307.06187&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;anchor our methodology on the MAPE-K model, which is renowned for its robust support in monitoring, analyzing, planning, and executing system adaptations in response to dynamic environments&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230716172856825.png&#34; alt=&#34;image-20230716172856825&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VoxPoser&lt;/strong&gt;:&amp;ldquo;VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models&amp;rdquo;, &lt;em&gt;arXiv, Jul 2023&lt;/em&gt;. [&lt;a href=&#34;extension://idghocbbahafpfhjnfhpbfbmpegphmmp/assets/pdf/web/viewer.html?file=https%3A%2F%2Fvoxposer.github.io%2Fvoxposer.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://voxposer.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;extracts language-conditioned affordances and constraints from LLMs and grounds them to the perceptual space using VLMs, using a code interface and without additional training to either component.&lt;/li&gt;
&lt;li&gt;zero-shot&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711121324849.png&#34; alt=&#34;image-20230711121324849&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DEPS&lt;/strong&gt;:&amp;ldquo;Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents&amp;rdquo;, &lt;em&gt;arXiv, Feb 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2302.01560&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CraftJarvis/MC-Planner&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MC open-world&lt;/li&gt;
&lt;li&gt;LLM for plan, RL for manipulation (MineCLIP)&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711122150969.png&#34; alt=&#34;image-20230711122150969&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Plan4MC&lt;/strong&gt;:&amp;ldquo;Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks&amp;rdquo;, &lt;em&gt;arXiv, Mar 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2303.16563&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/PKU-RL/Plan4MC&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/plan4mc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Static Stochastic Choice&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711122243542.png&#34; alt=&#34;image-20230711122243542&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VOYAGER&lt;/strong&gt;:&amp;ldquo;VOYAGER: An Open-Ended Embodied Agent with Large Language Models&amp;rdquo;, &lt;em&gt;arXiv, May 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2305.16291&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MineDojo/Voyager&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://voyager.minedojo.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;an automatic curriculum that maximizes exploration&lt;/li&gt;
&lt;li&gt;an ever-growing skill library of executable code for storing and retrieving complex behaviors&lt;/li&gt;
&lt;li&gt;a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714194824768.png&#34; alt=&#34;image-20230714194824768&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;instructions-and-navigation&#34;&gt;Instructions and Navigation&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LM-Nav&lt;/strong&gt;: &amp;ldquo;Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action&amp;rdquo;, &lt;em&gt;arXiv, July 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2207.04429&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/blazejosinski/lm_nav&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/lmnav&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230410013423928.png&#34; alt=&#34;image-20230410013423928&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;LLM: GPT-3 parse textual instructions into a sequence of landmarks ($t$).&lt;/li&gt;
&lt;li&gt;VLM: CLIP compute likelihood that image $i_k$ corresponds to the string $t$&lt;/li&gt;
&lt;li&gt;VNM: ViNG  uses a goal-conditioned distance function to infer connectivity between the set of raw observations and constructs a topological graph&lt;/li&gt;
&lt;li&gt;Navigation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VLMaps&lt;/strong&gt;: &amp;ldquo;Visual Language Maps for Robot Navigation&amp;rdquo;, &lt;em&gt;arXiv, Mar 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2210.05714&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/vlmaps/vlmaps&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://vlmaps.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;fuses pretrained visual-language features with a 3D reconstruction of the physical world&lt;/li&gt;
&lt;li&gt;Our goal is to build a spatial visual-language map representation, in which landmarks (“the sofa”) or spatial references (“between the sofa and the TV”) can be directly localized using natural language.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230713214658725.png&#34; alt=&#34;image-20230713214658725&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CoW&lt;/strong&gt;: &amp;ldquo;CLIP on Wheels: Zero-Shot Object Navigation as Object Localization and Exploration&amp;rdquo;, &lt;em&gt;arXiv, Mar 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2203.10421&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A simple CoW, with CLIP-based object localization and classical exploration—and no additional training—matches the navigation efficiency of a state-of-the-art ZSON method&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715212626119.png&#34; alt=&#34;image-20230715212626119&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ADAPT&lt;/strong&gt;: &amp;ldquo;ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts&amp;rdquo;, &lt;em&gt;CVPR, May 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2205.15509&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The image sub-prompt is a single-view observation that highlights a significant visual object or location.&lt;/li&gt;
&lt;li&gt;The text sub-prompt is a phrase related to the object, describing an action&lt;/li&gt;
&lt;li&gt;Before utilizing these prompts, they need to be retrieved from a pre-built action prompt database, which correlates with the instructions.&lt;/li&gt;
&lt;li&gt;These prompts are then encoded via a prompt encoder, and the output feature is concatenated with the original instruction feature.&lt;/li&gt;
&lt;li&gt;The concatenated feature, along with the visual feature, is then fed into a multi-layer transformer to make the action decision.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715213931916.png&#34; alt=&#34;image-20230715213931916&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;The Unsurprising Effectiveness of Pre-Trained Vision Models for Control&amp;rdquo;, &lt;em&gt;ICML, Mar 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2203.03580&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sparisi/pvr_habitat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/pvr-control&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It was found that pre-trained visual representations (PVRs) trained on entirely out-of-domain datasets can compete with or surpass ground-truth state features for policy training.&lt;/li&gt;
&lt;li&gt;The study also revealed the superiority of self-supervised learning (SSL) over supervised learning in providing better features for control policies.&lt;/li&gt;
&lt;li&gt;Residual Network, Momentum Contrast, CLIP, Random Features, From Scratch, Ground-Truth Features&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715214826827.png&#34; alt=&#34;image-20230715214826827&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Recurrent VLN-BERT&lt;/strong&gt;: &amp;ldquo;A Recurrent Vision-and-Language BERT for Navigation&amp;rdquo;, &lt;em&gt;CVPR, Jun 2021&lt;/em&gt; [&lt;a href=&#34;https://arxiv.org/abs/2011.13922&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YicongHong/Recurrent-VLN-BERT&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reinforcement learning (RL) and imitation learning (IL)&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715215714278.png&#34; alt=&#34;image-20230715215714278&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VLN-BERT&lt;/strong&gt;: &amp;ldquo;Improving Vision-and-Language Navigation with Image-Text Pairs from the Web&amp;rdquo;, &lt;em&gt;ECCV, Apr 2020&lt;/em&gt; [&lt;a href=&#34;https://arxiv.org/abs/2004.14973&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/arjunmajum/vln-bert&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VLN-BERT, a visiolinguistic transformer-based model for scoring the compatibility between an instruction (‘&amp;hellip;stop at the brown sofa’) and a sequence of panoramic RGB images captured by the agent.&lt;/li&gt;
&lt;li&gt;We demonstrate that pretraining VLN-BERT on image-text pairs from the web before fine-tuning on embodied path-instruction data significantly improves performance on VLN&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715220528690.png&#34; alt=&#34;image-20230715220528690&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Interactive Language: Talking to Robots in Real Time&amp;rdquo;, &lt;em&gt;arXiv, Oct 2022&lt;/em&gt; [&lt;a href=&#34;https://arxiv.org/abs/2210.06407&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://interactive-language.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The dataset we release comprises nearly 600,000 language-labeled trajectories, an order of magnitude larger than prior available datasets (video, actions, language).&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715221039329.png&#34; alt=&#34;image-20230715221039329&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;simulation-frameworks&#34;&gt;Simulation Frameworks&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MineDojo&lt;/strong&gt;: &amp;ldquo;MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge&amp;rdquo;, &lt;em&gt;arXiv, Jun 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2206.08853&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MineDojo/MineDojo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://minedojo.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;] [&lt;a href=&#34;https://minedojo.org/knowledge_base.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Open Database&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Habitat 2.0&lt;/strong&gt;: &amp;ldquo;Habitat 2.0: Training Home Assistants to Rearrange their Habitat&amp;rdquo;, &lt;em&gt;NeurIPS, Dec 2021&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2106.14405&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/facebookresearch/habitat-sim&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://aihabitat.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BEHAVIOR&lt;/strong&gt;: &amp;ldquo;BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments&amp;rdquo;, &lt;em&gt;CoRL, Nov 2021&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2108.03332&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/StanfordVL/behavior&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://behavior.stanford.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;iGibson 1.0&lt;/strong&gt;: &amp;ldquo;iGibson 1.0: a Simulation Environment for Interactive Tasks in Large Realistic Scenes&amp;rdquo;, &lt;em&gt;IROS, Sep 2021&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2012.02924&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/StanfordVL/iGibson&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://svl.stanford.edu/igibson/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ALFRED&lt;/strong&gt;: &amp;ldquo;ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks&amp;rdquo;, &lt;em&gt;CVPR, Jun 2020&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/1912.01734&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/askforalfred/alfred&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://askforalfred.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BabyAI&lt;/strong&gt;: &amp;ldquo;BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning&amp;rdquo;, &lt;em&gt;ICLR, May 2019&lt;/em&gt;. [&lt;a href=&#34;https://openreview.net/pdf?id=rJeXCo0cYX&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mila-iqia/babyai/tree/iclr19&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Dark, Beyond Deep - A Paradigm Shift to Cognitive AI with Humanlike Common Sense</title>
      <link>https://example.com/post/dark-beyond-deep/</link>
      <pubDate>Sun, 02 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/dark-beyond-deep/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;“small data for big tasks” paradigm&lt;/li&gt;
&lt;li&gt;models of common sense&lt;/li&gt;
&lt;li&gt;We identify functionality, physics, intent, causality, and utility (FPICU) as the five core domains of cognitive AI with humanlike common sense.&lt;/li&gt;
&lt;li&gt;FPICU is concerned with the questions of “why” and “how,” beyond the dominant “what” and “where” framework for understanding vision. They are invisible in terms of pixels but nevertheless drive the creation, maintenance, and development of visual scenes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;a-call-for-a-paradigm-shift-in-vision-and-ai&#34;&gt;A Call for a Paradigm Shift in Vision and AI&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;The classic definition of computer vision proposed by the pioneer David Marr is to look at “what” is “where.” Here, “what” refers to object recognition (object vision), and “where” denotes three-dimensional (3D) reconstruction and object localization (spatial vision)&lt;/li&gt;
&lt;li&gt;require large sets of labeled training data designed for special tasks, and lack a general understanding of common facts—that is, facts that are obvious to the average human adult—that describe how our physical and social worlds work.&lt;/li&gt;
&lt;li&gt;missing dimensions and the potential benefits of joint representation and joint inference.&lt;/li&gt;
&lt;li&gt;The concept of “darkness” is perpendicular to and richer than the meanings of “latent” or “hidden” used in vision and probabilistic modeling;  darkness” is a measure of the relative difficulty of classifying an entity or inferring about a relationship based on how much invisible common sense needed beyond the visible appearance or geometry.&lt;/li&gt;
&lt;li&gt;Section 2: paper starts by revisiting a classic view of computer vision in terms of “what” and “where”, task-driven&lt;/li&gt;
&lt;li&gt;Section 3: In order to use “small data” to solve “big tasks,” we then identify and review five crucial axes of visual common sense: Functionality, Physics, perceived Intent, Causality, and Utility (FPICU). Causality&lt;/li&gt;
&lt;li&gt;Section 4: The application of causality (i.e., intuitive physics; )&lt;/li&gt;
&lt;li&gt;Section 5: Functionality&lt;/li&gt;
&lt;li&gt;Section 6: infer intent&lt;/li&gt;
&lt;li&gt;Section 7: utility-driven&lt;/li&gt;
&lt;li&gt;In a series of studies, we demonstrate that these five critical aspects of “dark entities” and “dark relationships” indeed support various visual tasks beyond just classification.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;vision-from-data-driven-to-task-driven&#34;&gt;Vision: From Data-driven to Task-driven&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;From a biological perspective, the majority of living creatures use a single (with multiple components) vision system to perform thousands of tasks.&lt;/li&gt;
&lt;li&gt;these results indicate that our biological vision system possesses a mechanism for perceiving object functionality (i.e., how an object can be manipulated as a tool) that is independent of the mechanism governing face recognition (and recognition of other objects)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-task-centered-visual-recognition&#34;&gt;“What”: Task-centered Visual Recognition&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;these approaches have left unclear how classification interacts with scene semantics and enables cognitive reasoning&lt;/li&gt;
&lt;li&gt;human vision organizes representations during the inference process even for “simple” categorical recognition tasks.&lt;/li&gt;
&lt;li&gt;scene categorization and the information-gathering process are constrained by these categorization tasks, suggesting a bidirectional interplay between the visual input and the viewer’s needs/tasks&lt;/li&gt;
&lt;li&gt;the representation of the same object can vary according to the planned task&lt;/li&gt;
&lt;li&gt;task-driven nature of scene categorization.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;where-constructing-3d-scenes-as-a-series-of-tasks&#34;&gt;“Where”: Constructing 3D Scenes as a Series of Tasks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;scene reconstruction from a single two-dimensional (2D) image is a well-known illposed problem; there may exist an infinite number of possible 3D configurations that match the projected 2D observed images&lt;/li&gt;
&lt;li&gt;enable agents to perform tasks by generating the best possible configuration in terms of functionality, physics, and object relationships.&lt;/li&gt;
&lt;li&gt;there is now abundant evidence that humans represent the 3D layout of a scene in a way that fundamentally differs from any current computer vision algorithms&lt;/li&gt;
&lt;li&gt;human vision is error-prone and distorted in terms of localization&lt;/li&gt;
&lt;li&gt;Grid cells encode a cognitive representation of Euclidean space, implying a different mechanism for perceiving and processing locations and directions.&lt;/li&gt;
&lt;li&gt;Xie et al.  proposed a representational model for grid cells, in which the 2D self-position of an agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector.&lt;/li&gt;
&lt;li&gt;how we navigate complex environments while remaining able at all times to return to an original location (i.e., homing) remains a mystery in biology and neuroscience.&lt;/li&gt;
&lt;li&gt;the task-dependent representation of space can shed some light.&lt;/li&gt;
&lt;li&gt;neither based on a stable 3D model of a scene nor a distorted one; instead, participants seemed to form a flat and task-dependent representation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;beyond-what-and-where-towards-scene-understanding-with-humanlike-common-sense&#34;&gt;Beyond “What” and “Where”: Towards Scene Understanding with Humanlike Common Sense&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;rich as videos and much sparser visual inputs&lt;/li&gt;
&lt;li&gt;To enable an artificial agent with similar capabilities, we call for joint reasoning algorithms on a joint representation that integrates (i) the “visible” traditional recognition and categorization of objects, scenes, actions, events, and so forth; and (ii) the “dark” higher level concepts of fluent, causality, physics, functionality, affordance, intentions/goals, utility, and so forth.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;fluent-and-perceived-causality&#34;&gt;Fluent and Perceived Causality&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A fluent refers to a transient state of an object that is time-variant.&lt;/li&gt;
&lt;li&gt;Fluents are linked to perceived causality in the psychology literature.&lt;/li&gt;
&lt;li&gt;Fluents and perceived causality are different from the visual attributes of objects. The latter are permanent over the course of observation;&lt;/li&gt;
&lt;li&gt;Human cognition has the innate capability (observed in infants) and strong inclination to perceive the causal effects between actions and changes of fluents;&lt;/li&gt;
&lt;li&gt;but most daily actions, such as opening a door, are defined by cause and effect (a door’s fluent changes from “closed” to “open,” regardless of how it is opened), rather than by the human’s position, movement, or spatial-temporal features&lt;/li&gt;
&lt;li&gt;Overall, the status of a scene can be viewed as a collection of fluents that record the history of actions. Nevertheless, fluents and causal reasoning have not yet been systematically studied in machine vision, despite their ubiquitous presence in images and videos.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;intuitive-physics&#34;&gt;Intuitive Physics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;the knowledge of Newtonian principles and probabilistic representations is generally applied in human physical reasoning, and that an intuitive physical model is an important aspect of human-level complex scene understanding.&lt;/li&gt;
&lt;li&gt;By human design, objects should be physically stable and safe with respect to gravity and various other potential disturbances&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;functionality&#34;&gt;Functionality&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;These functions and needs are invisible in images, but shape the scene’s layout.&lt;/li&gt;
&lt;li&gt;researchers identified mirror neurons in the pre-motor cortical area that seem to encode actions through poses and interactions with objects and scenes [102]. Concepts in the human mind are not only represented by prototypes—that is, exemplars as in current computer vision and machine learning approaches—but also by functionality&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;intentions-and-goals&#34;&gt;Intentions and Goals&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We argue that intent can be treated as the transient status of agents (humans and animals)&lt;/li&gt;
&lt;li&gt;(i) They are hierarchically organized in a sequence of goals and are the main factors driving actions and events in a scene. (ii) They are completely “dark,” that is, not represented by pixels. (iii) Unlike the instant change of fluents in response to actions, intentions are often formed across long spatiotemporal ranges.&lt;/li&gt;
&lt;li&gt;functional object&lt;/li&gt;
&lt;li&gt;emits a field of attraction over the scene, not much different from a gravity field or an electric field&lt;/li&gt;
&lt;li&gt;The trajectory of a person with a certain intention moving through these fields follows a least-action principle in Lagrange mechanics that derives all motion equations by minimizing the potential and kinematic energies integrated over time&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;utility-and-preference&#34;&gt;Utility and Preference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we can mostly assume that the observed agents make near-optimal choices to minimize the cost of certain tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Generalization.&lt;/li&gt;
&lt;li&gt;Small sample learning.&lt;/li&gt;
&lt;li&gt;Bidirectional inference.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;causal-perception-and-reasoning-the-basis-for-understanding&#34;&gt;Causal Perception and Reasoning: The Basis for Understanding&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;People have innate assumptions about causes, and causal reasoning can be activated almost automatically and irresistibly. In our opinion, causality is the foundation of the other four FPICU elements (functionality, physics, intent, and utility).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;human-causal-perception-and-reasoning&#34;&gt;Human Causal Perception and Reasoning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Early psychological work focused on an associative mechanism as the basis for human causal learning and reasoning&lt;/li&gt;
&lt;li&gt;Rescorla-Wagner model was used to explain how humans (and animals) build expectations using the cooccurrence of perceptual stimuli&lt;/li&gt;
&lt;li&gt;more recent studies have shown that human causal learning is a rational Bayesian process [126, 129, 130] involving the acquisition of abstract causal structure [131, 132] and strength values for cause-effect relationships&lt;/li&gt;
&lt;li&gt;Irresistibility. One cannot stop seeing salient causality, just as one cannot stop seeing color and depth.&lt;/li&gt;
&lt;li&gt;Tight control by spatial-temporal patterns of motion&lt;/li&gt;
&lt;li&gt;Richness&lt;/li&gt;
&lt;li&gt;“adaptation” is a phenomenon in which an observer adapts to stimuli after a period of sustained viewing, such that their perceptual response to those stimuli becomes weaker&lt;/li&gt;
&lt;li&gt;physical causality is extracted during early visual processing&lt;/li&gt;
&lt;li&gt;One unique function of causality is the support of counterfactual reasoning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;causal-transfer-challenges-for-machine-intelligence&#34;&gt;Causal Transfer: Challenges for Machine Intelligence&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Recent successes of systems such as deep reinforcement learning (RL) showcase a broad range of applications&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;causality-in-statistical-learning&#34;&gt;Causality in Statistical Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;“Estimating causal effects f treatments in randomized and nonrandomized studies;”&lt;/li&gt;
&lt;li&gt;Rubin causal model&lt;/li&gt;
&lt;li&gt;the Rubin causal model is potential outcomes.&lt;/li&gt;
&lt;li&gt;A common manifestation of this problem is the latent variables that influence both the treatment assignment and the potential outcomes&lt;/li&gt;
&lt;li&gt;A very prominent example is the propensity score [148], which is the conditional probability of assigning one treatment to a subject given the background variables of the subject&lt;/li&gt;
&lt;li&gt;Causality was further developed in Pearl’s probabilistic graphical model (i.e., causal Bayesian networks (CBNs))&lt;/li&gt;
&lt;li&gt;despite attempts to learn causal structure from observational data, most structure learning approaches cannot typically succeed beyond identifying a Markov equivalence class of possible structures&lt;/li&gt;
&lt;li&gt;These works suggest that human causal perception is less rigorous than formal science but still maintains effectiveness in learning and understanding of daily events.&lt;/li&gt;
&lt;li&gt;Fire and Zhu proposed a method to learn “dark” causal relationships from image and video inputs, as illustrated in Fig. 14; in this study, systems learn how the status of a door, light, and screen relate to human actions&lt;/li&gt;
&lt;li&gt;To answer this question, the method utilizes the information projection framework, maximizing the amount of information gain after adding a causal relation,  and then minimizing the divergence between the model and observed statistics.&lt;/li&gt;
&lt;li&gt;Xu et al. used a Causal And-Or Graph (C-AOG) model to tackle this kind of “visibility fluent reasoning” problem. They consider the visibility status of an object as a fluent variable, whose change is mostly attributed to its interaction with its surroundings,&lt;/li&gt;
&lt;li&gt;jointly reason about the visibility fluent change and track humans&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;causality-in-computer-vision&#34;&gt;Causality in Computer Vision&lt;/h2&gt;
&lt;h1 id=&#34;intuitive-physics-cues-of-the-physical-world&#34;&gt;Intuitive Physics: Cues of the Physical World&lt;/h1&gt;
&lt;h2 id=&#34;intuitive-physics-in-human-cognition&#34;&gt;Intuitive Physics in Human Cognition&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Early research in intuitive physics provides several examples of situations in which humans demonstrate common misconceptions about how objects in the environment behave.&lt;/li&gt;
&lt;li&gt;researchers have developed alternative experimental approaches to study the development of infants’ physical knowledge. The most widely used approach is the violation-of-expectation method, in which infants see two test events: an expected event, consistent with the expectation shown, and an unexpected event, violating the expectation.&lt;/li&gt;
&lt;li&gt;These findings suggest that these brain regions use a generalized mental engine for intuitive physical inference—that is, the brain’s “physics engine.”&lt;/li&gt;
&lt;li&gt;Human intuitive physics can be modeled as an approximated physical engine with a Bayesian probabilistic model, possessing the following distinguishing properties&lt;/li&gt;
&lt;li&gt;expands to the perception and simulation of the physical properties of liquids and sand&lt;/li&gt;
&lt;li&gt;instead, they rely on perceived physical variables to make quantitative judgments.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;physics-based-reasoning-in-computer-vision&#34;&gt;Physics-based Reasoning in Computer Vision&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Statistical modeling aims to capture the “patterns generated by the world in any modality, with all their naturally occurring complexity and ambiguity, with the goal of reconstructing the processes, objects and events that produced them.”&lt;/li&gt;
&lt;li&gt;Alternatively, perceptual organization and Gestalt laws aim to resolve the 3D reconstruction problem from a single RGB image without considering depth. Instead, they use priors—groupings and structural cues that are likely to be invariant over wide ranges of viewpoints—resulting in feature-based approaches .&lt;/li&gt;
&lt;li&gt;Stability and safety in scene understanding.&lt;/li&gt;
&lt;li&gt;Physical relationships in 3D scenes&lt;/li&gt;
&lt;li&gt;very limited (if any) physics-based simulation is applied.&lt;/li&gt;
&lt;li&gt;Specifically, a generative model named Galileo was proposed for physical scene understanding using real-world videos and images.&lt;/li&gt;
&lt;li&gt;The model can infer these latent properties using relatively brief runs of markov chain monte carlo (MCMC), which drive simulations in the physics engine to fit key features of visual observations.&lt;/li&gt;
&lt;li&gt;With a new dataset named Physics 101 containing 17 408 video clips and 101 objects of various materials and appearances (i.e., shapes, colors, and sizes), the proposed unsupervised representation learning model, which explicitly encodes basic physical laws into the structure, can learn the physical properties of objects from videos.&lt;/li&gt;
&lt;li&gt;built a system that calculated various physical concepts from just a single example of tool use (Fig. 19), enabling it to reason about the essential physical concepts of the task (e.g., the force required to crack nuts).&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;functionality-and-affordance-the-opportunity-for-task-and-action&#34;&gt;Functionality and Affordance: The Opportunity for Task and Action&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Perception of an environment inevitably leads to a course of action&lt;/li&gt;
&lt;li&gt;“an object is first identified as having important functional relations”&lt;/li&gt;
&lt;li&gt;“perceptual analysis is derived of the functional concept”&lt;/li&gt;
&lt;li&gt;Functional understanding of objects and scenes is rooted in identifying possible tasks that can be performed with an object . This is deeply related to the perception of causality, as covered in Section 3; to understand how an object can be used, an agent must understand what change of state will result if an object is interacted with in any way. While affordances depend directly on the actor, functionality is a permanent property of an object independent of the characteristics of the user; see an illustration of this distinction in Fig. 21. These two interweaving concepts are more invariant for object and scene understanding than their geometric and appearance aspects. Specifically, we argue that:&lt;/li&gt;
&lt;li&gt;Objects, especially human-made ones, are defined by their functions, or by the actions they are associated with;&lt;/li&gt;
&lt;li&gt;Scenes, especially human-made ones, are defined by the actions than can be performed within them.&lt;/li&gt;
&lt;li&gt;functionality and affordance&lt;/li&gt;
&lt;li&gt;both the object level and scene level&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;revelation-from-tool-use-in-animal-cognition&#34;&gt;Revelation from Tool Use in Animal Cognition&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Dr. Jane Goodall observed wild chimpanzees manufacturing and using tools with regularity&lt;/li&gt;
&lt;li&gt;some animals have the capability (and possibly the intrinsic motivation) to reason about the functional properties of tools.&lt;/li&gt;
&lt;li&gt;First, why can some species devise innovative solutions, while others facing the same situation cannot? Look at the example in Fig. 20 [232]: by observing only a single demonstration of a person achieving the complex task of cracking a nut, we humans can effortlessly reason about which of the potential candidates from a new set of random and very different objects is best capable of helping us complete the same task. Reasoning across such large intraclass variance is extremely difficult to capture and describe for modern computer vision and AI systems.&lt;/li&gt;
&lt;li&gt;Second, how can this functional reasoning capability emerge if one does not possess it innately? New Caledonian crows are well-known for their propensity and dexterity at making and using tools; meanwhile, although a crow’s distant cousin, the rook, is able to reason and use tools in a lab setting, even they do not use tools in the wild [259]. These findings suggest that the ability to represent tools may be more of a domain-general cognitive capacity based on functional reasoning than an adaptive specialization.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;perceiving-functionality-and-affordance&#34;&gt;Perceiving Functionality and Affordance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;available structures should be described in terms of functions provided and functions performed.&lt;/li&gt;
&lt;li&gt;They pointed out that it is possible to use a single functional description to represent all possible cups, despite there being an infinite number of individual physical descriptions of cups or many other objects.&lt;/li&gt;
&lt;li&gt;“Tool”  Zhu et al. [232] cast the tool understanding problem as a task-oriented object-recognition problem, the core of which is understanding an object’s underlying functions, physics, and causality.&lt;/li&gt;
&lt;li&gt;“Container”  they showed six-year-old children could still be confused by the complex phenomenon of pouring liquid into containers. one of the earliest spatial relationships to be learned, preceding other common ones e.g., occlusions [272] and support relationships. ontology, topology, first-order logic, and knowledge base.&lt;/li&gt;
&lt;li&gt;“Chair” is an exemplar class for affordance. In particular, Grabner et al. [108] designed an “affordance detector” for chairs by fitting typical human sitting poses onto 3D objects.&lt;/li&gt;
&lt;li&gt;“Human” context has proven to be a critical component in modeling the constraints on possible usage of objects in a scene. In approaching this kind of problem, all methods imagine different potential human positioning relative to objects to help parse and understand the visible elements of the scene.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;mirroring-causal-equivalent-functionality--affordance&#34;&gt;Mirroring: Causal-equivalent Functionality &amp;amp; Affordance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;It is difficult to evaluate a computer vision or AI system’s facility at reasoning with functionality and affordance;&lt;/li&gt;
&lt;li&gt;the same object or environment does not necessarily introduce the same functionality and affordance to both robots and humans&lt;/li&gt;
&lt;li&gt;In these cases, a system must reason about the underlying mechanisms of affordance, rather than simply mimicking the motions of a human demonstration. This common problem is known as the “correspondence problem”  in learning from demonstration (LfD);&lt;/li&gt;
&lt;li&gt;Currently, the majority of work in LfD uses a one-to-one mapping between human demonstration and robot execution, restricting the LfD to mimicking the human’s low-level motor controls and replicating a nearly identical procedure. Consequently, the “correspondence problem” is insufficiently addressed, and the acquired skills are difficult to adapt to new robots or new situations;&lt;/li&gt;
&lt;li&gt;robot must obtain deeper understanding in functional and causal understanding of the manipulation, which demands more explicit modeling of knowledge about physical objects and forces. The key to imitating manipulation is using functionality and affordance to create causal-equivalent manipulation;&lt;/li&gt;
&lt;li&gt;Rather than over-imitating the motion trajectories of the demonstration, the robot is encouraged to seek functionally equivalent but possibly visually different actions that can produce the same effect and achieve the same goal as those in the demonstration.&lt;/li&gt;
&lt;li&gt;force-based&lt;/li&gt;
&lt;li&gt;goal-oriented:&lt;/li&gt;
&lt;li&gt;mirroring without overimitation&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;perceiving-intent-the-sense-of-agency&#34;&gt;Perceiving Intent: The Sense of Agency&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Crucially, such a sense of agency further entails&lt;/li&gt;
&lt;li&gt;intentionality&lt;/li&gt;
&lt;li&gt;rationality of actions in relation to goals&lt;/li&gt;
&lt;li&gt;The perception and comprehension of intent enable humans to better understand and predict the behavior of other agents and engage with others in cooperative activities with shared goals.&lt;/li&gt;
&lt;li&gt;rationality principle as the mechanism with which both infants and adults perceive animate objects as intentional beings.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-sense-of-agency&#34;&gt;The Sense of Agency&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;theory of mind (ToM) refers to the ability to attribute mental states, including beliefs, desires, and intentions, to oneself and others. Perceiving and understanding an agent’s intent based on their belief and desire is the ultimate goal, since people largely act to fulfill intentions arising from their beliefs and desires.&lt;/li&gt;
&lt;li&gt;After their first birthday, infants begin to understand that an actor may consider various plans to pursue a goal, and choose one to intentionally enact based on environmental reality . Eighteen-month-old children are able to both infer and imitate the intended goal of an action even if the action repeatedly fails to achieve the goal&lt;/li&gt;
&lt;li&gt;concrete action goals, higher order plans, and collaborative goals&lt;/li&gt;
&lt;li&gt;Despite the complexity of the behavioral streams we actually witness, we readily process action in intentional terms from infancy onward [303]. It is underlying intent, rather than surface behavior, that matters when we observe motions&lt;/li&gt;
&lt;li&gt;Research has found that we do not encode the complete details of human motion in space; instead, we perceive motions in terms of intent&lt;/li&gt;
&lt;li&gt;Reading intentions has even led to species-unique forms of cultural learning and cognition&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;from-animacy-to-rationality&#34;&gt;From Animacy to Rationality&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;latent mental states about goals, beliefs, and intentions from nothing but visual stimuli. Surprisingly, such visual stimuli do not need to contain rich semantics or visual features. An iconic illustration of this is the seminal Heider-Simmel display created in the 1940s [313]; see Fig. 28 for more detail.&lt;/li&gt;
&lt;li&gt;dynamic motion and temporal contingency were the crucial factors for the successful perception of social relationships and mental states&lt;/li&gt;
&lt;li&gt;A question naturally arises: what is the underlying mechanism with which the human visual system perceives and interprets such a richly social world?&lt;/li&gt;
&lt;li&gt;“rationality principle.”&lt;/li&gt;
&lt;li&gt;This theory states that humans view themselves and others as causal agents: (i) they devote their limited time and resources only to those actions that change the world in accordance with their intentions and desires; and (ii) they achieve their intentions rationally by maximizing their utility while minimizing their costs, given their beliefs about the world&lt;/li&gt;
&lt;li&gt;psychophysics of chasing, one of the most salient and evolutionarily important types of intentional behavior.&lt;/li&gt;
&lt;li&gt;The results showed that humans can effectively detect and avoid wolves with small subtlety values, whereas wolves with modest subtlety values turned out to be the most “dangerous.&lt;/li&gt;
&lt;li&gt;This result is consistent with the “rationality principle,” where human perception assumes that an agent’s intentional action will be one that maximizes its efficiency in reaching its goal.&lt;/li&gt;
&lt;li&gt;Such an early-emerging sensitivity to the causal powers of agents engaged in costly and goal-directed actions may provide one important foundation for the rich causal and social learning that characterizes our species.&lt;/li&gt;
&lt;li&gt;The rationality principle has been formally modeled as inverse planning governed by Bayesian inference [104, 323, 114]. Planning is a process by which intent causes action. Inverse planning, by inverting the rational planning model via Bayesian inference that integrates the likelihood of observed actions with prior mental states, can infer the latent mental intent. Based on inverse planning, Baker et al. [104] proposed a framework for goal inference, in which the bottom-up information of behavior observations and the top-down prior knowledge of goal space are integrated to allow inference of underlying intent. In addition, Bayesian networks, with their flexibility in representing probabilistic dependencies and causal relationships, as well as the efficiency of inference methods, have proven to be one of the most powerful and successful approaches for intent recognition&lt;/li&gt;
&lt;li&gt;Moving from the symbolic input to real video input, Holtzen et al. [318] presented an inverse planning method to infer human hierarchical intentions from partially observed RGBD videos. Their algorithm is able to infer human intentions by reverse-engineering decision-making and action planning processes in human minds under a Bayesian probabilistic programming framework; see Fig. 30 [318] for more details. The intentions are represented as a novel hierarchical, compositional, and probabilistic graph structure that describes the relationships between actions and plans.&lt;/li&gt;
&lt;li&gt;By bridging from the abstract Heider-Simmel display to aerial videos, Shu et al. [112] proposed a method to infer humans’ intentions with respect to interaction by observing motion trajectories (Fig. 31).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;beyond-action-prediction&#34;&gt;Beyond Action Prediction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In modern computer vision and AI systems [327], intent is related to action prediction much more profoundly than through simply predicting action labels.&lt;/li&gt;
&lt;li&gt;(i) action-effect association, (ii) simulation procedures, and (iii) teleological reasoning. They concluded that action-effect association and simulation could only serve action monitoring and prediction; social learning, in contrast, requires the inferential productivity of teleological reasoning.&lt;/li&gt;
&lt;li&gt;Simulation theory claims that the mechanism underlying the attribution of intentions to actions might rely on simulating the observed action and mapping it onto our own experiences and intent representations&lt;/li&gt;
&lt;li&gt;In order to understand others’ intentions, humans subconsciously empathize with the person they are observing and estimate what their own actions and intentions might be in that situation. Here, action-effect association [329] plays an important role in quick online intent prediction, and the ability to encode and remember these two component associations contributes to infants’ imitation skills and intentional action understanding&lt;/li&gt;
&lt;li&gt;mirror neuron [331], which has been linked to intent understanding in many studies&lt;/li&gt;
&lt;li&gt;To address social learning, a teleological action interpretational system [335] takes a “functional stance” for the computational representation of goal-directed action [103], where such teleological representations are generated by the aforementioned inferential “rationality principle”&lt;/li&gt;
&lt;li&gt;Furthermore, action predictions can be made by breaking down a path toward a goal into a hierarchy of sub-goals, the most basic of which are comprised of elementary motor acts such as grasping.&lt;/li&gt;
&lt;li&gt;These three mechanisms do not compete; instead, they complement each other. The fast effect prediction provided by action-effect associations can serve as a starting hypothesis for teleological reasoning or simulation procedure; the solutions provided by teleological reasoning in social learning can also be stored as action-effect associations for subsequent rapid recall.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;building-blocks-for-intent-in-computer-vision&#34;&gt;Building Blocks for Intent in Computer Vision&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;visual surveillance, human-robot interaction, and autonomous driving. In order to better predict intent based on pixel inputs, it is necessary and indispensable to fully exploit comprehensive cues such as motion trajectory, gaze dynamics, body posture and movements, human-object relationships, and communicative gestures (e.g., pointing).&lt;/li&gt;
&lt;li&gt;Motion trajectory alone could be a strong signal for intent prediction,&lt;/li&gt;
&lt;li&gt;Shu et al. [113] studied possible underlying computational mechanisms and proposed a unified psychological space that reveals the partition between the perception of physical events involving inanimate objects and the perception of social events involving human interactions with other agents.&lt;/li&gt;
&lt;li&gt;Eye gaze, being closely related to underlying attention, intent, emotion, personality, and anything a human is thinking and doing, also plays an important role in allowing humans to “read” other peoples’ minds.  Social eye gaze functions also transcend cultural differences, forming a kind of universal language.  Wei et al. [334] proposed a hierarchical human-attention-object (HAO) model that represents tasks, intentions, and attention under a unified framework. Under this model, a task is represented as sequential intentions described by hand-eye coordination under a planner represented by a grammar;&lt;/li&gt;
&lt;li&gt;Communicative gazes and gestures (e.g., pointing) stand out for intent expression and perception in collaborative interactions.  They examined the inferring of shared eye gazes in third-person social scene videos, which is a phenomenon in which two or more individuals simultaneously look at a common target in social scenes. A follow-up work [340] studied various types of gaze communications in social activities from both the atomic level and event level&lt;/li&gt;
&lt;li&gt;Humans communicate intentions multimodally; thus, facial expression, head pose, body posture and orientation, arm motion, gesture, proxemics, and relationships with other agents and objects can all contribute to human intent analysis and comprehension. intent recognition that focuses on uncertainty reduction.  Shu et al. [344] presented a generative model for robot learning of social affordance from human activity videos.&lt;/li&gt;
&lt;li&gt;Such social affordance could also be represented by a hierarchical grammar model [345], enabling real-time motion inference for human-robot interaction;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;learning-utility-the-preference-of-choices&#34;&gt;Learning Utility: The Preference of Choices&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;an agent makes rational decisions choices based on their beliefs and desires to maximize its expected utility. This is known as the principle of maximum expected utility&lt;/li&gt;
&lt;li&gt;A utility function is a mathematical formulation that ranks the preferences of an individual such that
$U(a) &amp;gt; U(b)$, where choice $a$ is preferred over choice $b$.&lt;/li&gt;
&lt;li&gt;By observing a rational agent’s preferences, however, an observer can construct a utility function that represents what the agent is actually trying to achieve, even if the agent does not know it [346]. It is also worth noting that utility theory is a positive theory that seeks to explain the individuals’ observed behavior and choices, which is different from a normative theory that indicates how people should behave;&lt;/li&gt;
&lt;li&gt;Formally, the core idea behind utility theory is straightforward: every possible action or state within a given model can be described with a single, uniform value.&lt;/li&gt;
&lt;li&gt;utility measures how much we desire something in a more subjective and contextdependent perspective, whereas value is a measurable quantity (e.g., price), which tends to be more objective.&lt;/li&gt;
&lt;li&gt;Similarly, Shukla et al. [350] adopted the idea of learning human utility in order to teach a robotics task using human demonstrations.&lt;/li&gt;
&lt;li&gt;In addition, the rationality principle has been studied in the field of linguistics and philosophy, notably in influential work on the theory of implicature by Grice [351]. The core insight of Grice’s work is that language use is a form of rational action; thus, technical tools for reasoning about rational action should elucidate linguistic phenomena&lt;/li&gt;
&lt;li&gt;Based on how the expected utility influences the distribution, social goals (e.g., cooperation and competition) [364, 365] and faireness [366] can also be well explained. On a broader scale, utility can enable individuals to be self-identified in society during the social learning process;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;summary-and-discussions&#34;&gt;Summary and Discussions&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Today’s robots fundamentally lack physical and social common sense; this limitation inhibits their capacity to aid in our daily lives. In this article, we have reviewed five concepts that are the crucial building blocks of common sense: functionality, physics, intent, causality, and utility (FPICU).&lt;/li&gt;
&lt;li&gt;There are indeed many other topics that we believe are also essential AI ingredients; for example&lt;/li&gt;
&lt;li&gt;A physically realistic VR/MR platform: from big data to big tasks.  Here, we argue that the ultimate standard for validating the effectiveness of FPICU in AI is to examine whether an agent is capable of (i) accomplishing the very same task using different sets of objects with different instructions and/ or sequences of actions in different environments; and (ii) rapidly adapting such learned knowledge to entirely new tasks.&lt;/li&gt;
&lt;li&gt;Social system: the emergence of language, communication, and morality.  In most cases, algorithms designed for a single agent would be difficult to generalize to a multiple-agent systems (MAS) setting&lt;/li&gt;
&lt;li&gt;Measuring the limits of an intelligence system: IQ tests.  John C. Raven [373] proposed the raven’s prograssive matrices test (RPM) in the image domain. Empirical studies show that abstract-level reasoning, combined with effective feature-extraction models, could notably improve the performance of reasoning, analogy, and generalization. However, the performance gap between human and computational models calls for future research in this field;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;physically-realistic-vrmr-platform-from-big-data-to-big-tasks&#34;&gt;Physically-Realistic VR/MR Platform: From Big-Data to Big-Tasks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A hallmark of machine intelligence is the capability to rapidly adapt to new tasks and “achieve goals in a wide range of environments”&lt;/li&gt;
&lt;li&gt;Such synthetic data could be relatively easily scaled up compared with traditional data collection and labeling processes.&lt;/li&gt;
&lt;li&gt;synthetic data from the virtual world is becoming increasingly similar to data collected from the physical world.&lt;/li&gt;
&lt;li&gt;Using a holistic evaluation, whether a method or a system is intelligent or not is no longer measured by the successful performance of a single narrow task; rather, it is measured by the ability to perform well across various tasks:&lt;/li&gt;
&lt;li&gt;To build this kind of task-driven evaluation, physicsbased simulations for multi-material, multi-physics phenomena (Fig. 37) will play a central role.&lt;/li&gt;
&lt;li&gt;Here, we provide a brief review of the recent physics-based simulation methods, with a particular focus on the material point method (MPM).&lt;/li&gt;
&lt;li&gt;The accuracy of physics-based reasoning greatly relies on the fidelity of a physics-based simulation.&lt;/li&gt;
&lt;li&gt;The most challenging problems are those involving extreme deformation, topology change, and interactions among different materials and phases.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;social-system-emergence-of-language-communication-and-morality&#34;&gt;Social System: Emergence of Language, Communication, and Morality&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In classic AI, a multiagent communication strategy is modeled using a predefined rule-based system (e.g., adaptive learning of communication strategies in MAS&lt;/li&gt;
&lt;li&gt;By modeling communication as a particular type of action, recent research [370, 443, 444] has shown that agents can learn how to communicate with continuous signals that are only decipherable within a group.&lt;/li&gt;
&lt;li&gt;Morality is an abstract and complex concept composed of common principles such as fairness, obligation, and permissibility.&lt;/li&gt;
&lt;li&gt;One recent approach to moral learning combines utility calculus and Bayesian inference to distinguish and evaluate different principles&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;measuring-the-limits-of-intelligence-system-iq-tests&#34;&gt;Measuring the Limits of Intelligence System: IQ tests&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;rather, “analogous” emphasizes commonality on a more abstract level&lt;/li&gt;
&lt;li&gt;To make a successful analogy, the key is to understand causes and their effects&lt;/li&gt;
&lt;li&gt;One stream is the psychometric tradition of four-term or “proportional” analogies, the earliest discussions of which can be traced back to Aristotle [465]. An example in AI is the word2vec model [466, 467], which is capable of making a four-term word analogy; In the image domain, a similar test was invented by John C. Raven [373]—the raven’s prograssive matrices test (RPM).&lt;/li&gt;
&lt;li&gt;RPM lies directly at the center: it is diagnostic of abstract and structural reasoning ability [470], and captures the defining feature of high-level cognition—that is, fluid intelligence&lt;/li&gt;
&lt;li&gt;The RAVEN dataset [374] was created to push the limit of current vision systems’ reasoning and analogy-making ability, and to promote further research in this area. The dataset is designed to focus on reasoning and analogizing instead of only visual recognition. It is unique in the sense that it builds a semantic link between the visual reasoning and structural reasoning in RPM by grounding each problem into a sentence derived from an attributed stochastic image grammar attributed stochastic  image grammar (A-SIG): each instance is a sentence sampled from a predefined A-SIG, and a rendering engine transforms the sentence into its corresponding image.&lt;/li&gt;
&lt;li&gt;Smith and Gentner [485] summarized that comparing cases facilitates transfer learning and problem-solving, as well as the ability to learn relational categories. In his structure-mapping theory, Gentner [486] postulated that learners generate a structural alignment between two representations when they compare two cases.&lt;/li&gt;
&lt;li&gt;Parallel to work on RPM, work on number sense [490] bridges the induction of symbolic concepts and the competence of problem-solving;&lt;/li&gt;
&lt;li&gt;A recent work approaches the analogy problem from this perspective of strong mathematical reasoning&lt;/li&gt;
&lt;li&gt;relational reasoning, where the machine is given two figures of numbers following hidden arithmetic computations and is tasked to work out a missing entry in the final answer&lt;/li&gt;
&lt;li&gt;This work also sheds some light on how machine reasoning could be improved: the fusing of classic search-based algorithms with modern neural networks in order to discover essential number concepts in future research would be an encouraging development.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Building machines that learn and think like people</title>
      <link>https://example.com/post/build-humanlike-machine/</link>
      <pubDate>Mon, 20 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/build-humanlike-machine/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;h2 id=&#34;two-different-computational-approaches-to-intelligence&#34;&gt;two different computational approaches to intelligence&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;pattern recognition approach treats prediction as primary, usually in the context of a specific classification, regression, or control task.  discovering features that have high-value states in common across a large, diverse set of training data.&lt;/li&gt;
&lt;li&gt;model building. Cognition is about using these models to understand the world, to explain, to imagine what could have happened that didn’t, or what could be true that isn’t, and then planning.&lt;/li&gt;
&lt;li&gt;prediction and explanation, is central to our view of human intelligence.  pattern recognition can support model building, through “model-free” algorithms that learn through experience how to make essential inferences more computationally efficient&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-this-article-is-not&#34;&gt;What this article is not&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;we believe that reverse engineering human intelligence  can usefully inform AI and machine learning.&lt;/li&gt;
&lt;li&gt;avoiding cognitive or neural inspiration as well as claims of cognitive or neural plausibility  is a  approach to developing AI. But this article has little pertinence to this approach.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview-of-the-key-ideas&#34;&gt;Overview of the key ideas&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;propose a set of core ingredients for building more human-like learning and thinking machines&lt;/li&gt;
&lt;li&gt;mainly in Section 4 Core ingredients of human intelligence&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;cognitive-and-neural-inspiration-in-artificial-intelligence&#34;&gt;Cognitive and neural inspiration in artificial intelligence&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;behaviorist view&lt;/li&gt;
&lt;li&gt;Cognitive science&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;pdp-approach&#34;&gt;PDP approach&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;knowledge is thus distributed across the collection of units rather than localized as in most symbolic data structures.&lt;/li&gt;
&lt;li&gt;Neural network models and the PDP approach offer a view of the mind (and intelligence more broadly) that is sub-symbolic and often populated with minimal constraints and inductive biases to guide learning.&lt;/li&gt;
&lt;li&gt;Proponents of this approach maintain that many classic types of structured knowledge, such as graphs, grammars can be useful yet misleading metaphors for characterizing thought.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;question&#34;&gt;Question&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;neural networks have such broad application in machine vision, language, and control, and they can be trained to emulate the rule-like and structured behaviors that characterize cognition&lt;/li&gt;
&lt;li&gt;do we need more to develop truly human-like learning and thinking machines?&lt;/li&gt;
&lt;li&gt;How far can relatively generic neural networks bring us toward this goal?&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;challenges-for-building-more-human-like-machines&#34;&gt;Challenges for building more human-like machines&lt;/h1&gt;
&lt;h2 id=&#34;the-characters-challenge&#34;&gt;The Characters Challenge&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;learning simple visual concepts&lt;/li&gt;
&lt;li&gt;People learn a lot more from a lot less, and capturing these human-level learning abilities in machines is the Characters Challenge&lt;/li&gt;
&lt;li&gt;Although humans and neural networks may perform equally well on the MNIST digit recognition task and other large-scale image classification tasks, it does not mean that they learn and think in the same way.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;two-important-differences-between-cnn-and-human-in-learning-simple-visual-concepts&#34;&gt;two important differences between CNN and human in learning simple visual concepts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;people learn from fewer examples and they learn richer representations&lt;/li&gt;
&lt;li&gt;people learn more than how to do pattern recognition: they learn a concept, that is, a model of the class that allows their acquired knowledge to be flexibly applied in new ways.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-difficulty&#34;&gt;Some difficulty&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A single example of a new visual concept (red box) can be enough information to support the&lt;/li&gt;
&lt;li&gt;classification of new examples&lt;/li&gt;
&lt;li&gt;generation of new examples&lt;/li&gt;
&lt;li&gt;parsing an object into parts and relations&lt;/li&gt;
&lt;li&gt;generation of new concepts from related concepts.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-frostbite-challenge&#34;&gt;The Frostbite Challenge&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;learning to play the Atari game Frostbite&lt;/li&gt;
&lt;li&gt;Failed on accomplishing a sub-goal (such as reaching an ice floe) and then safely proceed to the next sub-goal&lt;/li&gt;
&lt;li&gt;the policy are highly specialized for the games it was trained on&lt;/li&gt;
&lt;li&gt;considering the amount of experience required for learning&lt;/li&gt;
&lt;li&gt;non-professional humans can grasp the basics of the game after just a few minutes of play.&lt;/li&gt;
&lt;li&gt;people do this by inferring a general schema to describe the goals of the game and the object types and their interactions, using the kinds of intuitive theories, model-building abilities and model-based planning mechanisms we describe below.&lt;/li&gt;
&lt;li&gt;the game of Frostbite provides incremental rewards for reaching each active ice floe, providing the DQN with the relevant sub-goals for completing the larger task of building an igloo.&lt;/li&gt;
&lt;li&gt;Without these sub-goals, the DQN would have to take random actions until it accidentally builds an igloo and is rewarded for completing the entire level.&lt;/li&gt;
&lt;li&gt;Human is possible to figure out the higher-level goal of building an igloo without incremental feedback;&lt;/li&gt;
&lt;li&gt;sparse feedback is a source of difficulty in other Atari 2600 games such as Montezuma’s Revenge&lt;/li&gt;
&lt;li&gt;inflexible to changes in its inputs and goals. Changing the color or appearance of objects or changing the goals of the network would have devastating consequences on performance if the network is not retrained&lt;/li&gt;
&lt;li&gt;In contrast, people require little or no retraining or reconfiguration, adding new tasks and goals to their repertoire with relative ease.&lt;/li&gt;
&lt;li&gt;Humans as a result often have important domain-specific knowledge for these tasks, even before they ‘begin.’ The DQN is starting completely from scratch.&lt;/li&gt;
&lt;li&gt;How do we bring to bear rich prior knowledge to learn new tasks and solve new problems so quickly?&lt;/li&gt;
&lt;li&gt;What form does that prior knowledge take, and how is it constructed, from some combination of inbuilt capacities and previous experience?&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;core-ingredients-of-human-intelligence&#34;&gt;Core ingredients of human intelligence&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Whether learned, built in, or enriched, the key claim is that these ingredients play an active and important role in producing human-like learning and thought, in ways contemporary machine learning has yet to capture.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;developmental-start-up-software&#34;&gt;Developmental start-up software&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Early in development, humans have a foundational understanding of several core domains including number (numerical and set operations), space (geometry and navigation), physics (inanimate objects and mechanics), and psychology (agents and groups).&lt;/li&gt;
&lt;li&gt;The underlying cognitive representations can be understood as “intuitive theories,” with a causal structure resembling a scientific theory&lt;/li&gt;
&lt;li&gt;focus on the early understanding of objects and agents.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;intuitive-physics&#34;&gt;Intuitive physics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;At the age of 2 months, and possibly earlier, human infants expect inanimate objects to follow principles of persistence, continuity, cohesion, and solidity.&lt;/li&gt;
&lt;li&gt;At around 6 months, infants have already developed different expectations for rigid bodies, soft bodies, and liquids&lt;/li&gt;
&lt;li&gt;By their first birthday, infants have gone through several transitions of comprehending basic physical concepts such as inertia, support, containment, and collisions&lt;/li&gt;
&lt;li&gt;There is no single agreed-upon computational account of these early physical principles and concepts, and previous suggestions have ranged from decision trees, to cues, to lists of rules&lt;/li&gt;
&lt;li&gt;A promising recent approach sees intuitive physical reasoning as similar to inference over a physics software engine&lt;/li&gt;
&lt;li&gt;This “intuitive physics engine” approach enables flexible adaptation to a wide range of everyday scenarios and judgments in a way that goes beyond perceptual cues.&lt;/li&gt;
&lt;li&gt;What are the prospects for embedding or acquiring this kind of intuitive physics in deep learning systems?&lt;/li&gt;
&lt;li&gt;Connectionist models in psychology have previously been applied to physical reasoning tasks such as balance-beam rules (McClelland 1988; Shultz 2003) or rules relating to distance, velocity, and time in motion&lt;/li&gt;
&lt;li&gt;PhysNet In contrast, people require far less experience to perform any particular task, and can generalize to many novel judgments and complex scenes with no new training required (although they receive large amounts of physics experience through interacting with the world more generally).&lt;/li&gt;
&lt;li&gt;Could deep learning systems such as PhysNet capture this flexibility, without explicitly simulating the causal interactions between objects in three dimensions?&lt;/li&gt;
&lt;li&gt;Whether such models can be learned with the kind (and quantity) of data available to human infants is not clear&lt;/li&gt;
&lt;li&gt;But incorporating a physics-engine–based representation could help DQNs learn to play games such as Frostbite in a faster and more general way, whether the physics knowledge is captured implicitly in a neural network or more explicitly in a simulator.&lt;/li&gt;
&lt;li&gt;When a new object type such as a bear is introduced, as in the later levels of Frostbite (Fig. 2D), a network endowed with intuitive physics would also have an easier time adding this object type to its knowledge&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;challenges&#34;&gt;Challenges&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;For networks trained on object classification,deeper layers often become sensitive to successively higher-level features, from edges to textures to shapeparts to full objects.&lt;/li&gt;
&lt;li&gt;For deep networks trained on physics-related data,it remains to be seen whether higher layers will encode objects, general physical properties, forces, and approximately Newtonian dynamics.&lt;/li&gt;
&lt;li&gt;would it generalize broadly beyond training contexts as people’s more explicit physical concepts do?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;intuitive-psychology&#34;&gt;Intuitive psychology&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Pre-verbal infants distinguish animate agents from inanimate objects. This distinction is partially based on innate or early-present detectors for low-level cues, such as the presence of eyes, motion initiated from rest, and biological motion&lt;/li&gt;
&lt;li&gt;Beyond these low-level cues, infants also expect agents to act contingently and reciprocally, to have goals, and to take efficient actions toward those goals subject to constraints&lt;/li&gt;
&lt;li&gt;It is generally agreed that infants expect agents to act in a goal-directed, efficient, and socially sensitive fashion&lt;/li&gt;
&lt;li&gt;One possibility is that intuitive psychology is simply cues “all the way down” This inference could be captured by a cue that states &amp;ldquo;If an agent’s expected trajectory is prevented from completion, the blocking agent is given some negative association.&lt;/li&gt;
&lt;li&gt;One alternative to a cue-based account is to use generative models of action choice, as in the Bayesian inverse planning, or Bayesian theory of mind (ToM), models of Baker et al. (2009) or the naive utility calculus models of Jara-Ettinger et al.&lt;/li&gt;
&lt;li&gt;These models formalize explicitly mentalistic concepts such as “goal,” “agent,”  “planning,” “cost,” “efficiency,” and “belief,” used to describe core psychological  reasoning in infancy.&lt;/li&gt;
&lt;li&gt;They assume adults and children treat agents as approximately rational planners who choose the most efficient means to their goals.&lt;/li&gt;
&lt;li&gt;By simulating these planning processes, people can predict what agents might do next, or use inverse reasoning from observing a series of actions to infer the utilities and beliefs of agents in a scene.&lt;/li&gt;
&lt;li&gt;Importantly, unlike in intuitive physics, simulation-based reasoning in intuitive psychology can be nested recursively to understand social interactions. We can think about agents thinking about other agents.&lt;/li&gt;
&lt;li&gt;Although deep networks have not yet been applied to scenarios involving theory of mind and intuitive psychology, they could probably learn visual cues, heuristics and summary statistics of a scene that happens to involve agents.&lt;/li&gt;
&lt;li&gt;However, it seems to us that any full formal account of intuitive psychological reasoning needs to include representations of agency, goals, efficiency, and reciprocal relations.&lt;/li&gt;
&lt;li&gt;Connectionists have argued that innate constraints in the form of hard-wired cortical circuits are unlikely , but a simple inductive bias, for example, the tendency to notice things that move other things, can bootstrap reasoning about more abstract concepts of agency&lt;/li&gt;
&lt;li&gt;Similarly, a great deal of goal-directed and socially directed actions can also be boiled down to a simple utility calculus, in a way that could be shared with other cognitive abilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;learning-as-rapid-model-building&#34;&gt;Learning as rapid model building&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;There are many learning algorithms for neural networks, including the perceptron algorithm (Rosenblatt 1958), Hebbian learning (Hebb 1949), the BCM rule (Bienenstock et al. 1982), backpropagation (Rumelhart et al. 1986a), the wake-sleep algorithm (Hinton et al. 1995), and contrastive divergence (Hinton 2002).&lt;/li&gt;
&lt;li&gt;Human “one-shot” learning&lt;/li&gt;
&lt;li&gt;Neural network sdata hungry&lt;/li&gt;
&lt;li&gt;the types of things that children learn as the meanings of words – people are still far better learners than machines.&lt;/li&gt;
&lt;li&gt;Even with just a few examples, people can learn remarkably rich conceptual models.&lt;/li&gt;
&lt;li&gt;Beyond classification, concepts support prediction, action, communication, imagination , explanation, and composition.&lt;/li&gt;
&lt;li&gt;In addition to evaluating several types of deep learning models, we developed an algorithm using Bayesian program learning (BPL) that represents concepts as simple stochastic programs: structured procedures that generate new examples of a concept when executed&lt;/li&gt;
&lt;li&gt;These programs allow the model to express causal knowledge about how the raw data are formed, and the probabilistic semantics allow the model to handle noise and perform creative tasks. Structure sharing across concepts is accomplished by the compositional re-use of stochastic primitives that can combine in new ways to create new concepts.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;compositionality&#34;&gt;Compositionality&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Compositionality is the classic idea that new representations can be constructed through the combination of primitive elements.&lt;/li&gt;
&lt;li&gt;Compositionality is also at the core of productivity: an infinite number of representations can be constructed from a finite set of primitives&lt;/li&gt;
&lt;li&gt;To capture the full extent of the mind’s compositionality, a model must include explicit representations of objects, identity, and relations, all while maintaining a notion of “coherence” when understanding novel configurations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;causality&#34;&gt;Causality&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;To capture the full extent of the mind’s compositionality, a model must include explicit representations of objects, identity, and relations, all while maintaining a notion of “coherence” when understanding novel configurations.&lt;/li&gt;
&lt;li&gt;Concept learning and vision models that use causality are usually generative but not every generative model is also causal.&lt;/li&gt;
&lt;li&gt;Causality has been influential in theories of perception. “Analysis-by-synthesis” theories of perception maintain that sensory data can be more richly represented by modeling the process that generated it&lt;/li&gt;
&lt;li&gt;Causal knowledge has also been shown to influence how people learn new concepts; providing a learner with different types of causal knowledge changes how he or she learns and generalizes.&lt;/li&gt;
&lt;li&gt;To explain the role of causality in learning, conceptual representations have been likened to intuitive theories or explanations, providing the glue that lets core features stick, whereas other equally applicable features wash away.&lt;/li&gt;
&lt;li&gt;Beyond concept learning, people also understand scenes by building causal models.&lt;/li&gt;
&lt;li&gt;There have been steps toward deep neural networks and related approaches that learn causal models.&lt;/li&gt;
&lt;li&gt;Incorporating causality may greatly improve these deep learning models; they were trained without access to causal data about how characters are actually produced, and without any incentive to learn the true causal process.&lt;/li&gt;
&lt;li&gt;A causal model of Frostbite would have to be more complex, gluing together object representations and explaining their interactions with intuitive physics and intu-
itive psychology, much like the game engine that generates the game dynamics and, ultimately, the frames of pixel images.&lt;/li&gt;
&lt;li&gt;Inference is the process of inverting this causal generative model, explaining the raw pixels as objects and their interactions, such as the agent stepping on an ice floe to deactivate it or a crab pushing the agent into the water.&lt;/li&gt;
&lt;li&gt;Deep neural networks could play a role in two ways: by serving as a bottom-up proposer to make probabilistic inference more tractable in a structured generative model or by serving as the causal generative model if imbued with the right set of ingredients.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;learning-to-learn&#34;&gt;Learning-to-learn&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;When humans or machines make inferences that go far beyond the data, strong prior knowledge (or inductive biases or constraints) must be making up the difference.&lt;/li&gt;
&lt;li&gt;One way people acquire this prior knowledge is through “learning-to-learn,” a term introduced by Harlow (1949) and closely related to the machine learning notions of “transfer learning,” “multitask learning,” and “representation learning.”&lt;/li&gt;
&lt;li&gt;The strong priors, constraints, or inductive bias needed to learn a particular task quickly are often shared to some extent with other related tasks.&lt;/li&gt;
&lt;li&gt;In hierarchical Bayesian modeling, a general prior on concepts is shared by multiple specific concepts, and the prior itself is learned over the course of learning the specific concepts&lt;/li&gt;
&lt;li&gt;In machine vision, for deep convolutional networks or other discriminative methods that form the core of recent recognition systems, learning-to-learn can occur through the sharing of features between the models learned for old objects or old tasks and the models learned for new objects or new tasks&lt;/li&gt;
&lt;li&gt;We cannot be sure how people get to the knowledge they have in this domain, but we do understand how this works in BPL, and we think people might be similar.&lt;/li&gt;
&lt;li&gt;BPL transfers readily to new concepts because it learns about object parts, sub-parts, and relations, capturing learning about what each concept is like and what concepts are like in general.  It is crucial that learning-to-learn occurs at multiple levels of the hierarchical generative process.&lt;/li&gt;
&lt;li&gt;Further transfer occurs by learning about the typical levels of variability within a typical generative model. This provides knowledge about how far and in what ways to generalize when we have seen only one example of a new character, which on its own could not possibly carry any information about variance.&lt;/li&gt;
&lt;li&gt;In the Frostbite Challenge, and in video games more generally, there is a similar interdependence between the form of the representation and the effectiveness of learning-to-learn.&lt;/li&gt;
&lt;li&gt;general world knowledge and previous video games may help inform exploration and generalization in new scenarios, helping people learn maximally from a single mistake or avoid mistakes altogether&lt;/li&gt;
&lt;li&gt;Deep reinforcement learning systems for playing Atari games have had some impressive successes in transfer learning, but they still have not come close to learning to play new games as quickly as humans can. For example, Parisotto et al. (2016) present the “actor-mimic” algorithm&lt;/li&gt;
&lt;li&gt;In sum, the interaction between representation and previous experience may be key to building machines that learn as fast as people.&lt;/li&gt;
&lt;li&gt;if such a system aims to learn compositionally structured causal models of each game – built on a foundation of intuitive physics and psychology – it could transfer knowledge more efficiently and thereby learn new games much more quickly.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;thinking-fast&#34;&gt;Thinking Fast&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In general, richer and more structured models require more complex and slower inference algorithms, similar to how complex models require more data, making the speed of perception and thought all the more remarkable.&lt;/li&gt;
&lt;li&gt;The combination of rich models with efficient inference suggests another way psychology and neuroscience may usefully inform AI.&lt;/li&gt;
&lt;li&gt;This section discusses possible paths toward resolving the conflict between fast inference and structured representations, including Helmholtz machine–style approximate inference in generative models and cooperation between model-free and model-based reinforcement learning systems&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;approximate-inference-in-structured-models&#34;&gt;Approximate inference in structured models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In contrast, whereas representing intuitive theories and structured causal models is less natural in deep neural networks, recent progress has demonstrated the remarkable effectiveness of gradient-based learning in high-dimensional parameter spaces.&lt;/li&gt;
&lt;li&gt;A complete account of learning and inference must explain how the brain does so much with limited computational resources&lt;/li&gt;
&lt;li&gt;Popular algorithms for approximate inference in probabilistic machine learning have been proposed as psychological models&lt;/li&gt;
&lt;li&gt;Most prominently, it has been proposed that humans can approximate Bayesian inference using Monte Carlo methods, which stochastically sample the space of possible hypotheses and evaluate these samples according to their consistency with the data and prior knowledge&lt;/li&gt;
&lt;li&gt;Although Monte Carlo methods are powerful and come with asymptotic guarantees, it is challenging to make them work on complex problems like program induction and theory learning.&lt;/li&gt;
&lt;li&gt;Discovering new theories can be slow and arduous, as testified by the long time scale of cognitive development, and learning in a saltatory fashion (rather than through gradual adaptation) is characteristic of aspects of human intelligence, including discovery and insight during development (Schulz 2012b), problem-solving (Sternberg &amp;amp; Davidson 1995), and epoch-making discoveries in scientific research (Langley et al. 1987).&lt;/li&gt;
&lt;li&gt;Dis-covering new theories can also occur much more quickly. A person learning the rules of Frostbite will probably undergo a loosely ordered sequence of “Aha!” moments:&lt;/li&gt;
&lt;li&gt;These little fragments of a “Frostbite theory” are assembled to form a causal understanding of the game relatively quickly, in what seems more like a guided process than arbitrary proposals in a Monte Carlo inference scheme&lt;/li&gt;
&lt;li&gt;For domains where program or theory learning occurs quickly, it is possible that people employ inductive biases not only to evaluate hypotheses, but also to guide hypothesis selection.&lt;/li&gt;
&lt;li&gt;How might efficient mappings from questions to a plausible subset of answers be learned?  spanning both deep learning and graphical models, has attempted to tackle this challenge by “amortizing” probabilistic inference computations into an efficient feed-forward mapping&lt;/li&gt;
&lt;li&gt;These feed-forward mappings can be learned in various ways, for example, using paired generative/recognition networks and variational optimization, or nearest-neighbor density estimation&lt;/li&gt;
&lt;li&gt;One implication of amortization is that solutions to different problems will become correlated because of the sharing of amortized computations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model-based-and-model-free-reinforcement-learning&#34;&gt;Model-based and model-free reinforcement learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;There is indeed substantial evidence that the brain uses similar model-free learning algorithms in simple associative learning or discrimination learning tasks&lt;/li&gt;
&lt;li&gt;Considerable evidence suggests that the brain also has a model-based learning system, responsible for building a “cognitive map” of the environment and using it to plan action sequences for more complex tasks Model-based planning is an essential ingredient of human intelligence, enabling flexible adaptation to new tasks and goals;&lt;/li&gt;
&lt;li&gt;One boundary condition on this flexibility is the fact that the skills become “habitized” with routine application,  possibly reflecting a shift from model-based to model-free control. This shift may arise from a rational arbitration between learning systems to balance the trade-off between flexibility and speed&lt;/li&gt;
&lt;li&gt;Similarly to how probabilistic computations can be amortized for efficiency (see previous section), plans can be amortized into cached values by allowing the model-based system to simulate training data for the model-free system&lt;/li&gt;
&lt;li&gt;Intrinsic motivation also plays an important role in human learning and behavior&lt;/li&gt;
&lt;li&gt;all externally provided rewards are reinterpreted according to the “internal value” of the agent,&lt;/li&gt;
&lt;li&gt;There may also be an intrinsic drive to reduce uncertainty and construct models of the 8environment&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;responses-to-common-questionswhat-we-believe&#34;&gt;Responses to common questions(What we believe)&lt;/h1&gt;
&lt;h2 id=&#34;comparing-the-learning-speeds-of-humans-and-neural-networks-on-specific-tasks-is-not-meaningful-because-humans-have-extensive-prior-experience&#34;&gt;Comparing the learning speeds of humans and neural networks on specific tasks is not meaningful, because humans have extensive prior experience&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;If deep learning researchers see themselves as trying to capture the equivalent of humans’ collective evolutionary experience, this would be equivalent to a truly immense “pre-training” phase.&lt;/li&gt;
&lt;li&gt;We are less committed to a particular story regarding the origins of the ingredients,&lt;/li&gt;
&lt;li&gt;successful learning-to-learn – or, at least, human-level transfer learning – is enabled by having models with the right representational structure, including the other building blocks discussed in this article.&lt;/li&gt;
&lt;li&gt;To build these representations from scratch might require exploring fundamental structural variations in the network’s architecture, which gradient-based learning in weight space is not prepared to do. Although deep learning researchers do explore many such architectural variations&lt;/li&gt;
&lt;li&gt;dynamics of structure search may look much more like the slow random hill climbing of evolution than the smooth, methodical progress of stochastic gradient descent.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;biological-plausibility-suggests-theories-of-intelligence-should-start-with-neural-networks&#34;&gt;Biological plausibility suggests theories of intelligence should start with neural networks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We have focused on how cognitive science can motivate and guide efforts to engineer human-like AI, in contrast to some advocates of deep neural networks who cite neuro-science for inspiration.&lt;/li&gt;
&lt;li&gt;Unfortunately, what we “know” about the brain is not all that clear-cut.&lt;/li&gt;
&lt;li&gt;Hebbian learning is another case in point. In the form of long-term potentiation (LTP) and spike-timing dependent plasticity (STDP), Hebbian learning mechanisms are often cited as biologically supported.&lt;/li&gt;
&lt;li&gt;Most relevantly for our focus, it would be especially challenging to try to implement the ingredients described in this article using purely Hebbian mechanisms.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;language-is-essential-for-human-intelligence-why-is-it-not-more-prominent-here&#34;&gt;Language is essential for human intelligence. Why is it not more prominent here?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We have said little in this article about people’s ability to communicate and think in natural language, a distinctively human cognitive capacity where machine capabilities strikingly lag.&lt;/li&gt;
&lt;li&gt;We believe that understanding language and its role in intelligence goes hand-in-hand with understanding the building blocks discussed in this article.&lt;/li&gt;
&lt;li&gt;These capacities are in place before children master language, and they provide the building blocks for linguistic meaning and language acquisition&lt;/li&gt;
&lt;li&gt;Is it recursion, or some new kind of recursive structure uilding ability? Is it the ability to re-use symbols by name ? Is it the ability to understand others intentionally and build shared intentionality ? Is it some new version of these things, or is it just more of the aspects of these capacities that are already present in infants?&lt;/li&gt;
&lt;li&gt;But with language, older children become able to reason about a much wider range of physical and psychological situations . Language also facilitates more powerful learning-to-learn and compositionality.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;looking-forward&#34;&gt;Looking forward&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Machine performance may rival or exceed human performance on particular tasks, and algorithms may take inspiration from neuroscience or aspects of psychology, but it does not follow that the algorithm learns or thinks like a person. This is a higher bar worth reaching for, potentially leading to more powerful algorithms, while also helping unlock the mysteries of the human mind.&lt;/li&gt;
&lt;li&gt;When comparing people with the current best algorithms in AI and machine learning, people learn from fewer data and generalize in richer and more flexible ways.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;promising-directions-in-deep-learning&#34;&gt;Promising directions in deep learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;There has been recent interest in integrating psychological ingredients with deep neural networks, especially selective   attention, augmented working memory, and experience replay.&lt;/li&gt;
&lt;li&gt;Paralleling the human perceptual apparatus, selective  attention forces deep learning models to process raw, perceptual data as a series of high-resolution “foveal glimpses” rather than all at once.&lt;/li&gt;
&lt;li&gt;Attention may help these models in several ways. It helps to coordinate complex, often sequential, outputs by attending to only specific aspects of the input, allowing the model to focus on smaller sub-tasks rather than solving an entire problem in one shot.&lt;/li&gt;
&lt;li&gt;Attention also allows larger models to be trained without requiring every model parameter to affect every output or action.&lt;/li&gt;
&lt;li&gt;Researchers are also developing neural networks with “working memories” that augment the shorter-term memory provided by unit activation and the longer-term memory provided by the connection weights&lt;/li&gt;
&lt;li&gt;Each model seems to learn genuine programs from examples, albeit in a representation more like assembly language than a high-level programming language.&lt;/li&gt;
&lt;li&gt;differentiable programming suggests the intriguing possibility of combining the best of program induction and deep learning.&lt;/li&gt;
&lt;li&gt;Another example of combining pattern recognition and model-based search comes from recent AI research into the game Go.&lt;/li&gt;
&lt;li&gt;One worthy goal would be to build an AI system that beats a world-class player with the amount and kind of training human champions receive, rather than overpowering them with Google-scale computational resources.&lt;/li&gt;
&lt;li&gt;Although techniques for handling variable-sized inputs in ConvNets may help in playing on different board sizes, the value functions and policies that AlphaGo learns seem unlikely to generalize as flexibly and automatically as people.&lt;/li&gt;
&lt;li&gt;the fact that it cannot even conceive of these variants, let alone adapt to them autonomously, is a sign that it does not understand the game as humans do.&lt;/li&gt;
&lt;li&gt;Humans represent their strategies as a response to these constraints, such that if the game changes, they can begin to adjust their strategies accordingly.&lt;/li&gt;
&lt;li&gt;We believe it would be richly rewarding for AI and cognitive science to pursue this challenge together and that such systems could be a compelling testbed for the principles this article suggests, as well as building on all of the progress to date that AlphaGo represents.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;future-applications-to-practical-ai-problems&#34;&gt;Future applications to practical AI problems&lt;/h2&gt;
&lt;h3 id=&#34;scene-understanding&#34;&gt;Scene understanding&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Deep learning is moving beyond object recognition and toward scene understanding, as evidenced by a flurry of recent work focused on generating natural language captions for images&lt;/li&gt;
&lt;li&gt;Yet current algorithms are still better at recognizing objects than understanding scenes, often getting the key objects right but their causal relationships wrong&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;autonomous-agents-and-intelligent-devices&#34;&gt;Autonomous agents and intelligent devices&lt;/h3&gt;
&lt;h3 id=&#34;autonomous-driving&#34;&gt;Autonomous driving&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Similarly, other drivers on the road have similarly complex mental states underlying their behavior&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;creative-design&#34;&gt;Creative design&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Although we are still far from developing AI systems that can tackle these types of tasks, we see compositionality and causality as central to this goal.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;toward-more-human-like-learning-and-thinking-machines&#34;&gt;Toward more human-like learning and thinking machines&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;we suggest that deep learning and other computational paradigms should aim to tackle these tasks using as few training data as people need, and also to evaluate models on a range of human-like generalizations beyond the one task on which the model was trained.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;note&#34;&gt;Note&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;The Atari games are deterministic, raising the possibility that a learner can succeed by memorizing long sequences of actions without learning to generalize&lt;/li&gt;
&lt;li&gt;Although it is unclear if the DQN also memorizes action sequences, an alternative “human starts” metric provides a stronger test of generalization&lt;/li&gt;
&lt;li&gt;Although connectionist networks have been used to model the general transition that children undergo between the ages of 3 and 4 regarding false belief,&lt;/li&gt;
&lt;li&gt;A new approach using convolutional “matching networks” achieves good one-shot classification performance when discriminating between characters from different alphabets&lt;/li&gt;
&lt;li&gt;In the interest of brevity, we do not discuss here another important vein of work linking neural circuits to variational approximations (Bastos et al. 2012), which have received less attention in the psychological literature.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;the-three-major-paradigms-of-ai-symbolicism-connectionism-behaviorism&#34;&gt;The three major paradigms of AI: symbolicism, connectionism, behaviorism&lt;/h1&gt;
&lt;h2 id=&#34;symbolicism&#34;&gt;Symbolicism&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;symbolicism=Logicism=Psychlogism=Computerism&lt;/li&gt;
&lt;li&gt;The main principles are the hypothesis of a physical symbol system (i.e., a system that manipulates symbols) and the principle of limited rationality.&lt;/li&gt;
&lt;li&gt;This category included most of the pioneers of artificial intelligence research.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;main-points&#34;&gt;Main points&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Symbolism is the foundation of human cognitive and mental activities&lt;/li&gt;
&lt;li&gt;Computers operate as physical systems that manipulate symbols&lt;/li&gt;
&lt;li&gt;Cognition involves performing computations on symbolic representations&lt;/li&gt;
&lt;li&gt;Computers can emulate or approximate human cognitive functions.&lt;/li&gt;
&lt;li&gt;In 1957, Newell, Simon and their colleagues created a program named &amp;ldquo;Logic Theorist&amp;rdquo; that could prove mathematical theorems. It verified 38 out of the first 52 propositions in Whitehead and Russell&amp;rsquo;s &amp;ldquo;Principia Mathematica&amp;rdquo;, and subsequently confirmed some more.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;connectionism&#34;&gt;Connectionism&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Connectionism is an approach in cognitive science that hopes to explain mental phenomena with artificial neural networks (ANNs).&lt;/li&gt;
&lt;li&gt;The central principle of connectionism is to use simple and often consistent units interconnected networks, to describe psychological phenomena. Different models of connections and unit forms may vary. For example, the units and connections of the network can represent neurons and synapses, as in the human brain.&lt;/li&gt;
&lt;li&gt;Lack of neuroscientific rationality.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;parallel-distributed-processing-pdp&#34;&gt;Parallel distributed processing, PDP&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;It is a method of artificial neural networks that emphasizes the parallelism of neural processing and the distributedness of neural representations, providing researchers with a general mathematical framework. It mainly includes eight aspects:&lt;/li&gt;
&lt;li&gt;A set of processing units, represented by a set of integers.&lt;/li&gt;
&lt;li&gt;The activation of the units, represented by a vector of time-dependent functions.&lt;/li&gt;
&lt;li&gt;The output function of the units, represented by a vector of activation functions.&lt;/li&gt;
&lt;li&gt;The connectivity pattern between units, represented by a real matrix indicating connection strengths.&lt;/li&gt;
&lt;li&gt;The propagation rule for propagating activation through connections, expressed as a function on unit outputs.&lt;/li&gt;
&lt;li&gt;The activation rule for combining inputs sent to units to determine new activations for units, represented by current activations and propagation functions.&lt;/li&gt;
&lt;li&gt;The learning rule for modifying connections based on experience, expressed as weight changes based on any number of variables.&lt;/li&gt;
&lt;li&gt;The environment that provides experience for the system, represented by a set of activation vectors for some subsets of units.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;experimentalism&#34;&gt;Experimentalism&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We can learn almost everything we know from the statistical patterns of sensory input.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;actionism&#34;&gt;Actionism&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Actionism (Behaviorism), also known as evolutionary or cybernetic school, is based on cybernetics and perception-action control systems.&lt;/li&gt;
&lt;li&gt;It argues that artificial intelligence originates from cybernetics.&lt;/li&gt;
&lt;li&gt;Cybernetic ideas became an important part of the zeitgeist in the 1940s and 1950s, influencing early artificial intelligence researchers. The cybernetics and self-organizing systems proposed by Wiener, McCulloch and others, as well as the engineering cybernetics and biological cybernetics proposed by Qian Xuesen and others, affected many fields. Cybernetics linked the working principles of neural systems with information theory, control theory, logic and computer science.&lt;/li&gt;
&lt;li&gt;And it assumes that all behaviors are produced by stimuli from the environment or shaped by individual life history; especially individual punishment, incentives, stimuli and behavioral outcomes caused by reinforcement in environment and life history. Therefore, although behaviorists generally accept that genetic factors are important determinants of behavior, they still pay more attention to environmental influences.&lt;/li&gt;
&lt;li&gt;The early research work focused on simulating human intelligent behavior and role in control processes, such as research on cybernetic systems such as self-optimization, self-adaptation, self-stabilization, self-organization and self-learning , And carried out research on &amp;ldquo;cybernetic animals&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;By the 1960s and 1970s , some progress had been made in these studies of cybernetic systems , sowing seeds for intelligent control and intelligent robots , which gave birth to intelligent control systems . Intelligent robot system .&lt;/li&gt;
&lt;li&gt;Behaviorism did not appear until the end of the 20th century as a new school of artificial intelligence , attracting many people&amp;rsquo;s interest . The representative author of this school was Brooks&amp;rsquo;s six-legged walking robot , which was regarded as a new generation of &amp;ldquo;cybernetic animals&amp;rdquo; . It is a control system based on perception-action mode to simulate insect behavior .&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Welcome to Wowchemy, the website builder for Hugo</title>
      <link>https://example.com/post/getting-started/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/getting-started/</guid>
      <description>&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;libr&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;hello&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site&lt;/li&gt;
&lt;li&gt;The template can be modified and customised to suit your needs. It&amp;rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a &lt;strong&gt;no-code solution (write in Markdown and customize with YAML parameters)&lt;/strong&gt; and having &lt;strong&gt;flexibility to later add even deeper personalization with HTML and CSS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/wowchemy/wowchemy-hugo-modules/main/starters/academic/preview.png&#34; alt=&#34;The template is mobile first with a responsive design to ensure that your site looks stunning on every device.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;get-started&#34;&gt;Get Started&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;👉 &lt;a href=&#34;https://wowchemy.com/templates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Create a new site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;📚 &lt;a href=&#34;https://wowchemy.com/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Personalize your site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;💬 &lt;a href=&#34;https://discord.gg/z8wNYzb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chat with the &lt;strong&gt;Wowchemy community&lt;/strong&gt;&lt;/a&gt; or &lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Hugo community&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;🐦 Twitter: &lt;a href=&#34;https://twitter.com/wowchemy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@wowchemy&lt;/a&gt; &lt;a href=&#34;https://twitter.com/GeorgeCushen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@GeorgeCushen&lt;/a&gt; &lt;a href=&#34;https://twitter.com/search?q=%23MadeWithWowchemy&amp;amp;src=typed_query&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#MadeWithWowchemy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;💡 &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-themes/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Request a &lt;strong&gt;feature&lt;/strong&gt; or report a &lt;strong&gt;bug&lt;/strong&gt; for &lt;em&gt;Wowchemy&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;⬆️ &lt;strong&gt;Updating Wowchemy?&lt;/strong&gt; View the &lt;a href=&#34;https://wowchemy.com/docs/hugo-tutorials/update/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Update Tutorial&lt;/a&gt; and &lt;a href=&#34;https://wowchemy.com/updates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Release Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;crowd-funded-open-source-software&#34;&gt;Crowd-funded open-source software&lt;/h2&gt;
&lt;p&gt;To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.&lt;/p&gt;
&lt;h3 id=&#34;-click-here-to-become-a-sponsor-and-help-support-wowchemys-future-httpswowchemycomsponsor&#34;&gt;&lt;a href=&#34;https://wowchemy.com/sponsor/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;❤️ Click here to become a sponsor and help support Wowchemy&amp;rsquo;s future ❤️&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;As a token of appreciation for sponsoring, you can &lt;strong&gt;unlock &lt;a href=&#34;https://wowchemy.com/sponsor/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;these&lt;/a&gt; awesome rewards and extra features 🦄✨&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;ecosystem&#34;&gt;Ecosystem&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/wowchemy/hugo-academic-cli&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Academic CLI&lt;/a&gt;:&lt;/strong&gt; Automatically import publications from BibTeX&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;inspiration&#34;&gt;Inspiration&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Check out the latest &lt;strong&gt;demo&lt;/strong&gt;&lt;/a&gt; of what you&amp;rsquo;ll get in less than 10 minutes, or &lt;a href=&#34;https://wowchemy.com/user-stories/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;view the &lt;strong&gt;showcase&lt;/strong&gt;&lt;/a&gt; of personal, project, and business sites.&lt;/p&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Page builder&lt;/strong&gt; - Create &lt;em&gt;anything&lt;/em&gt; with &lt;a href=&#34;https://wowchemy.com/docs/page-builder/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;widgets&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://wowchemy.com/docs/content/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;elements&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Edit any type of content&lt;/strong&gt; - Blog posts, publications, talks, slides, projects, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create content&lt;/strong&gt; in &lt;a href=&#34;https://wowchemy.com/docs/content/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Markdown&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://wowchemy.com/docs/import/jupyter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Jupyter&lt;/strong&gt;&lt;/a&gt;, or &lt;a href=&#34;https://wowchemy.com/docs/install-locally/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugin System&lt;/strong&gt; - Fully customizable &lt;a href=&#34;https://wowchemy.com/docs/customization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;color&lt;/strong&gt; and &lt;strong&gt;font themes&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Display Code and Math&lt;/strong&gt; - Code highlighting and &lt;a href=&#34;https://en.wikibooks.org/wiki/LaTeX/Mathematics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LaTeX math&lt;/a&gt; supported&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrations&lt;/strong&gt; - &lt;a href=&#34;https://analytics.google.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics&lt;/a&gt;, &lt;a href=&#34;https://disqus.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Disqus commenting&lt;/a&gt;, Maps, Contact Forms, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Beautiful Site&lt;/strong&gt; - Simple and refreshing one page design&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Industry-Leading SEO&lt;/strong&gt; - Help get your website found on search engines and social media&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media Galleries&lt;/strong&gt; - Display your images and videos with captions in a customizable gallery&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile Friendly&lt;/strong&gt; - Look amazing on every screen with a mobile friendly version of your site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-language&lt;/strong&gt; - 34+ language packs including English, 中文, and Português&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-user&lt;/strong&gt; - Each author gets their own profile page&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Privacy Pack&lt;/strong&gt; - Assists with GDPR&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stand Out&lt;/strong&gt; - Bring your site to life with animation, parallax backgrounds, and scroll effects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-Click Deployment&lt;/strong&gt; - No servers. No databases. Only files.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;p&gt;Wowchemy and its templates come with &lt;strong&gt;automatic day (light) and night (dark) mode&lt;/strong&gt; built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the &lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Demo&lt;/a&gt; to see it in action! Day/night mode can also be disabled by the site admin in &lt;code&gt;params.toml&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/customization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Choose a stunning &lt;strong&gt;theme&lt;/strong&gt; and &lt;strong&gt;font&lt;/strong&gt;&lt;/a&gt; for your site. Themes are fully customizable.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2016-present &lt;a href=&#34;https://georgecushen.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;George Cushen&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Released under the &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-themes/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Writing technical content in Markdown</title>
      <link>https://example.com/post/writing-technical-content/</link>
      <pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/writing-technical-content/</guid>
      <description>&lt;p&gt;Wowchemy is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Highlight your code snippets, take notes on math classes, and draw diagrams from textual representation.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;On this page, you&amp;rsquo;ll find some examples of the types of technical content that can be rendered with Wowchemy.&lt;/p&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;Wowchemy supports a Markdown extension for highlighting code syntax. You can customize the styles under the &lt;code&gt;syntax_highlighter&lt;/code&gt; option in your &lt;code&gt;config/_default/params.yaml&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```python
import pandas as pd
data = pd.read_csv(&amp;quot;data.csv&amp;quot;)
data.head()
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;data.csv&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;mindmaps&#34;&gt;Mindmaps&lt;/h3&gt;
&lt;p&gt;Wowchemy supports a Markdown extension for mindmaps.&lt;/p&gt;
&lt;p&gt;Simply insert a Markdown &lt;code&gt;markmap&lt;/code&gt; code block and optionally set the height of the mindmap as shown in the example below.&lt;/p&gt;
&lt;p&gt;A simple mindmap defined as a Markdown list:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
```markmap {height=&#34;200px&#34;}
- Hugo Modules
  - wowchemy
  - wowchemy-plugins-netlify
  - wowchemy-plugins-netlify-cms
  - wowchemy-plugins-reveal
```
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;markmap&#34; style=&#34;height: 200px;&#34;&gt;

&lt;pre&gt;- Hugo Modules
  - wowchemy
  - wowchemy-plugins-netlify
  - wowchemy-plugins-netlify-cms
  - wowchemy-plugins-reveal&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;A more advanced mindmap with formatting, code blocks, and math:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
```markmap
- Mindmaps
  - Links
    - [Wowchemy Docs](https://wowchemy.com/docs/)
    - [Discord Community](https://discord.gg/z8wNYzb)
    - [GitHub](https://github.com/wowchemy/wowchemy-hugo-themes)
  - Features
    - Markdown formatting
    - **inline** ~~text~~ *styles*
    - multiline
      text
    - `inline code`
    -
      ```js
      console.log(&#39;hello&#39;);
      console.log(&#39;code block&#39;);
      ```
    - Math: $x = {-b \pm \sqrt{b^2-4ac} \over 2a}$
```
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;markmap&#34; style=&#34;height: 500px;&#34;&gt;

&lt;pre&gt;- Mindmaps
  - Links
    - [Wowchemy Docs](https://wowchemy.com/docs/)
    - [Discord Community](https://discord.gg/z8wNYzb)
    - [GitHub](https://github.com/wowchemy/wowchemy-hugo-themes)
  - Features
    - Markdown formatting
    - **inline** ~~text~~ *styles*
    - multiline
      text
    - `inline code`
    -
      ```js
      console.log(&#39;hello&#39;);
      console.log(&#39;code block&#39;);
      ```
    - Math: $x = {-b \pm \sqrt{b^2-4ac} \over 2a}$&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&#34;charts&#34;&gt;Charts&lt;/h3&gt;
&lt;p&gt;Wowchemy supports the popular &lt;a href=&#34;https://plot.ly/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Plotly&lt;/a&gt; format for interactive charts.&lt;/p&gt;
&lt;p&gt;Save your Plotly JSON in your page folder, for example &lt;code&gt;line-chart.json&lt;/code&gt;, and then add the &lt;code&gt;{{&amp;lt; chart data=&amp;quot;line-chart&amp;quot; &amp;gt;}}&lt;/code&gt; shortcode where you would like the chart to appear.&lt;/p&gt;
&lt;p&gt;Demo:&lt;/p&gt;




&lt;div id=&#34;chart-687152349&#34; class=&#34;chart&#34;&gt;&lt;/div&gt;
&lt;script&gt;
  (function() {
    let a = setInterval( function() {
      if ( typeof window.Plotly === &#39;undefined&#39; ) {
        return;
      }
      clearInterval( a );

      Plotly.d3.json(&#34;./line-chart.json&#34;, function(chart) {
        Plotly.plot(&#39;chart-687152349&#39;, chart.data, chart.layout, {responsive: true});
      });
    }, 500 );
  })();
&lt;/script&gt;

&lt;p&gt;You might also find the &lt;a href=&#34;http://plotly-json-editor.getforge.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Plotly JSON Editor&lt;/a&gt; useful.&lt;/p&gt;
&lt;h3 id=&#34;math&#34;&gt;Math&lt;/h3&gt;
&lt;p&gt;Wowchemy supports a Markdown extension for $\LaTeX$ math. You can enable this feature by toggling the &lt;code&gt;math&lt;/code&gt; option in your &lt;code&gt;config/_default/params.yaml&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;To render &lt;em&gt;inline&lt;/em&gt; or &lt;em&gt;block&lt;/em&gt; math, wrap your LaTeX math with &lt;code&gt;{{&amp;lt; math &amp;gt;}}$...${{&amp;lt; /math &amp;gt;}}&lt;/code&gt; or &lt;code&gt;{{&amp;lt; math &amp;gt;}}$$...$${{&amp;lt; /math &amp;gt;}}&lt;/code&gt;, respectively. (We wrap the LaTeX math in the Wowchemy &lt;em&gt;math&lt;/em&gt; shortcode to prevent Hugo rendering our math as Markdown. The &lt;em&gt;math&lt;/em&gt; shortcode is new in v5.5-dev.)&lt;/p&gt;
&lt;p&gt;Example &lt;strong&gt;math block&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-latex&#34; data-lang=&#34;latex&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;{{&lt;/span&gt;&amp;lt; math &amp;gt;&lt;span class=&#34;nb&#34;&gt;}}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sb&#34;&gt;$$&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\gamma&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;_{n} &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\frac&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;{ &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\left&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; | &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\left&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\mathbf&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; x_{n} &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\mathbf&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; x_{n&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;} &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\right&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;^T &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\left&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\nabla&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; F &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\mathbf&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; x_{n}&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\nabla&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; F &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\mathbf&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; x_{n&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\right&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\right&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; |}{&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\left&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\|\nabla&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\mathbf&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;{x}_{n}&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\nabla&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\mathbf&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;{x}_{n&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\right&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\|&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;^&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;$$&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;{{&lt;/span&gt;&amp;lt; /math &amp;gt;&lt;span class=&#34;nb&#34;&gt;}}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;renders as&lt;/p&gt;



$$\gamma_{n} = \frac{ \left | \left (\mathbf x_{n} - \mathbf x_{n-1} \right )^T \left [\nabla F (\mathbf x_{n}) - \nabla F (\mathbf x_{n-1}) \right ] \right |}{\left \|\nabla F(\mathbf{x}_{n}) - \nabla F(\mathbf{x}_{n-1}) \right \|^2}$$

&lt;p&gt;Example &lt;strong&gt;inline math&lt;/strong&gt; &lt;code&gt;{{&amp;lt; math &amp;gt;}}$\nabla F(\mathbf{x}_{n})${{&amp;lt; /math &amp;gt;}}&lt;/code&gt; renders as 

$\nabla F(\mathbf{x}_{n})$.&lt;/p&gt;
&lt;p&gt;Example &lt;strong&gt;multi-line math&lt;/strong&gt; using the math linebreak (&lt;code&gt;\\&lt;/code&gt;):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-latex&#34; data-lang=&#34;latex&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;{{&lt;/span&gt;&amp;lt; math &amp;gt;&lt;span class=&#34;nb&#34;&gt;}}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sb&#34;&gt;$$&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;k;p_{&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;}^{&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\begin&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;{cases}p_{&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;}^{&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;} &amp;amp; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\text&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;{if }k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;, &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\\&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;p_{&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;}^{&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;} &amp;amp; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\text&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;{if }k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\end&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;{cases}&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;$$&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;{{&lt;/span&gt;&amp;lt; /math &amp;gt;&lt;span class=&#34;nb&#34;&gt;}}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;renders as&lt;/p&gt;




$$
f(k;p_{0}^{*}) = \begin{cases}p_{0}^{*} &amp; \text{if }k=1, \\
1-p_{0}^{*} &amp; \text{if }k=0.\end{cases}
$$


&lt;h3 id=&#34;diagrams&#34;&gt;Diagrams&lt;/h3&gt;
&lt;p&gt;Wowchemy supports a Markdown extension for diagrams. You can enable this feature by toggling the &lt;code&gt;diagram&lt;/code&gt; option in your &lt;code&gt;config/_default/params.toml&lt;/code&gt; file or by adding &lt;code&gt;diagram: true&lt;/code&gt; to your page front matter.&lt;/p&gt;
&lt;p&gt;An example &lt;strong&gt;flowchart&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
graph TD
A[Hard] --&amp;gt;|Text| B(Round)
B --&amp;gt; C{Decision}
C --&amp;gt;|One| D[Result 1]
C --&amp;gt;|Two| E[Result 2]
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;graph TD
A[Hard] --&gt;|Text| B(Round)
B --&gt; C{Decision}
C --&gt;|One| D[Result 1]
C --&gt;|Two| E[Result 2]
&lt;/div&gt;
&lt;p&gt;An example &lt;strong&gt;sequence diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
sequenceDiagram
Alice-&amp;gt;&amp;gt;John: Hello John, how are you?
loop Healthcheck
    John-&amp;gt;&amp;gt;John: Fight against hypochondria
end
Note right of John: Rational thoughts!
John--&amp;gt;&amp;gt;Alice: Great!
John-&amp;gt;&amp;gt;Bob: How about you?
Bob--&amp;gt;&amp;gt;John: Jolly good!
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;sequenceDiagram
Alice-&gt;&gt;John: Hello John, how are you?
loop Healthcheck
    John-&gt;&gt;John: Fight against hypochondria
end
Note right of John: Rational thoughts!
John--&gt;&gt;Alice: Great!
John-&gt;&gt;Bob: How about you?
Bob--&gt;&gt;John: Jolly good!
&lt;/div&gt;
&lt;p&gt;An example &lt;strong&gt;Gantt diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
gantt
section Section
Completed :done,    des1, 2014-01-06,2014-01-08
Active        :active,  des2, 2014-01-07, 3d
Parallel 1   :         des3, after des1, 1d
Parallel 2   :         des4, after des1, 1d
Parallel 3   :         des5, after des3, 1d
Parallel 4   :         des6, after des4, 1d
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;gantt
section Section
Completed :done,    des1, 2014-01-06,2014-01-08
Active        :active,  des2, 2014-01-07, 3d
Parallel 1   :         des3, after des1, 1d
Parallel 2   :         des4, after des1, 1d
Parallel 3   :         des5, after des3, 1d
Parallel 4   :         des6, after des4, 1d
&lt;/div&gt;
&lt;p&gt;An example &lt;strong&gt;class diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
classDiagram
Class01 &amp;lt;|-- AveryLongClass : Cool
Class03 *-- Class04
Class05 o-- Class06
Class07 .. Class08
Class09 --&amp;gt; C2 : Where am i?
Class09 --* C3
Class09 --|&amp;gt; Class07
Class07 : equals()
Class07 : Object[] elementData
Class01 : size()
Class01 : int chimp
Class01 : int gorilla
Class08 &amp;lt;--&amp;gt; C2: Cool label
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;classDiagram
Class01 &lt;|-- AveryLongClass : Cool
Class03 *-- Class04
Class05 o-- Class06
Class07 .. Class08
Class09 --&gt; C2 : Where am i?
Class09 --* C3
Class09 --|&gt; Class07
Class07 : equals()
Class07 : Object[] elementData
Class01 : size()
Class01 : int chimp
Class01 : int gorilla
Class08 &lt;--&gt; C2: Cool label
&lt;/div&gt;
&lt;p&gt;An example &lt;strong&gt;state diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
stateDiagram
[*] --&amp;gt; Still
Still --&amp;gt; [*]
Still --&amp;gt; Moving
Moving --&amp;gt; Still
Moving --&amp;gt; Crash
Crash --&amp;gt; [*]
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;stateDiagram
[*] --&gt; Still
Still --&gt; [*]
Still --&gt; Moving
Moving --&gt; Still
Moving --&gt; Crash
Crash --&gt; [*]
&lt;/div&gt;
&lt;h3 id=&#34;todo-lists&#34;&gt;Todo lists&lt;/h3&gt;
&lt;p&gt;You can even write your todo lists in Markdown too:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;- [x]&lt;/span&gt; Write math example
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;- [x]&lt;/span&gt; Write diagram example
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;- [ ]&lt;/span&gt; Do something else
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;renders as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Write math example
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Write diagram example&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Do something else&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tables&#34;&gt;Tables&lt;/h3&gt;
&lt;p&gt;Save your spreadsheet as a CSV file in your page&amp;rsquo;s folder and then render it by adding the &lt;em&gt;Table&lt;/em&gt; shortcode to your page:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{{&amp;lt;&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;table&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;results.csv&amp;#34;&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;header&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;caption&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;Table 1: My results&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;}}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;renders as&lt;/p&gt;













  



&lt;table class=&#34;table&#34;&gt;
  
    
    
    &lt;tr&gt;  &lt;th&gt;customer_id&lt;/th&gt;  &lt;th&gt;score&lt;/th&gt;  &lt;/tr&gt;
  
  
    &lt;tr&gt;
      
        
          &lt;td data-table-dtype=&#34;number&#34;&gt;1&lt;/td&gt;
        
      
        
          &lt;td data-table-dtype=&#34;number&#34;&gt;0&lt;/td&gt;
        
      
    &lt;/tr&gt;
  
    &lt;tr&gt;
      
        
          &lt;td data-table-dtype=&#34;number&#34;&gt;2&lt;/td&gt;
        
      
        
          &lt;td data-table-dtype=&#34;text&#34;&gt;0.5&lt;/td&gt;
        
      
    &lt;/tr&gt;
  
    &lt;tr&gt;
      
        
          &lt;td data-table-dtype=&#34;number&#34;&gt;3&lt;/td&gt;
        
      
        
          &lt;td data-table-dtype=&#34;number&#34;&gt;1&lt;/td&gt;
        
      
    &lt;/tr&gt;
  
  
    &lt;caption&gt;Table 1: My results&lt;/caption&gt;
  
&lt;/table&gt;

&lt;h3 id=&#34;callouts&#34;&gt;Callouts&lt;/h3&gt;
&lt;p&gt;Academic supports a &lt;a href=&#34;https://wowchemy.com/docs/content/writing-markdown-latex/#callouts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcode for callouts&lt;/a&gt;, also referred to as &lt;em&gt;asides&lt;/em&gt;, &lt;em&gt;hints&lt;/em&gt;, or &lt;em&gt;alerts&lt;/em&gt;. By wrapping a paragraph in &lt;code&gt;{{% callout note %}} ... {{% /callout %}}&lt;/code&gt;, it will render as an aside.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% callout note %}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;A Markdown aside is useful for displaying notices, hints, or definitions to your readers.
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% /callout %}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    A Markdown aside is useful for displaying notices, hints, or definitions to your readers.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;spoilers&#34;&gt;Spoilers&lt;/h3&gt;
&lt;p&gt;Add a spoiler to a page to reveal text, such as an answer to a question, after a button is clicked.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;spoiler&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;Click to view the spoiler&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;You found me!
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spoiler&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;renders as&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-6&#34;&gt;
  &lt;summary&gt;Click to view the spoiler&lt;/summary&gt;
  &lt;p&gt;You found me!&lt;/p&gt;
&lt;/details&gt;
&lt;h3 id=&#34;icons&#34;&gt;Icons&lt;/h3&gt;
&lt;p&gt;Academic enables you to use a wide range of &lt;a href=&#34;https://wowchemy.com/docs/getting-started/page-builder/#icons&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;icons from &lt;em&gt;Font Awesome&lt;/em&gt; and &lt;em&gt;Academicons&lt;/em&gt;&lt;/a&gt; in addition to &lt;a href=&#34;https://wowchemy.com/docs/content/writing-markdown-latex/#emojis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;emojis&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here are some examples using the &lt;code&gt;icon&lt;/code&gt; shortcode to render icons:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;icon&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;terminal&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;pack&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;fas&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}} Terminal  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;icon&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;python&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;pack&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;fab&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}} Python  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;icon&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;r-project&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;pack&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;fab&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}} R
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;renders as&lt;/p&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-terminal  pr-1 fa-fw&#34;&gt;&lt;/i&gt; Terminal&lt;br&gt;

  &lt;i class=&#34;fab fa-python  pr-1 fa-fw&#34;&gt;&lt;/i&gt; Python&lt;br&gt;

  &lt;i class=&#34;fab fa-r-project  pr-1 fa-fw&#34;&gt;&lt;/i&gt; R&lt;/p&gt;
&lt;h3 id=&#34;did-you-find-this-page-helpful-consider-sharing-it-&#34;&gt;Did you find this page helpful? Consider sharing it 🙌&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Display Jupyter Notebooks with Academic</title>
      <link>https://example.com/post/jupyter/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/jupyter/</guid>
      <description>&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;IPython.core.display&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Image&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./index_1_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Welcome to Academic!&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Welcome to Academic!
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;install-python-and-jupyterlab&#34;&gt;Install Python and JupyterLab&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Install Anaconda&lt;/a&gt; which includes Python 3 and JupyterLab.&lt;/p&gt;
&lt;p&gt;Alternatively, install JupyterLab with &lt;code&gt;pip3 install jupyterlab&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;create-or-upload-a-jupyter-notebook&#34;&gt;Create or upload a Jupyter notebook&lt;/h2&gt;
&lt;p&gt;Run the following commands in your Terminal, substituting &lt;code&gt;&amp;lt;MY-WEBSITE-FOLDER&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;SHORT-POST-TITLE&amp;gt;&lt;/code&gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;mkdir -p &amp;lt;MY-WEBSITE-FOLDER&amp;gt;/content/post/&amp;lt;SHORT-POST-TITLE&amp;gt;/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; &amp;lt;MY-WEBSITE-FOLDER&amp;gt;/content/post/&amp;lt;SHORT-POST-TITLE&amp;gt;/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;jupyter lab index.ipynb
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The &lt;code&gt;jupyter&lt;/code&gt; command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.&lt;/p&gt;
&lt;h2 id=&#34;edit-your-post-metadata&#34;&gt;Edit your post metadata&lt;/h2&gt;
&lt;p&gt;The first cell of your Jupter notebook will contain your post metadata (&lt;a href=&#34;https://sourcethemes.com/academic/docs/front-matter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;front matter&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In Jupter, choose &lt;em&gt;Markdown&lt;/em&gt; as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;title: My post&amp;#39;s title
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;date: 2019-09-01
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# Put any other Academic metadata here...
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Edit the metadata of your post, using the &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; as a guide to the available options.&lt;/p&gt;
&lt;p&gt;To set a &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#featured-image&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;featured image&lt;/a&gt;, place an image named &lt;code&gt;featured&lt;/code&gt; into your post&amp;rsquo;s folder.&lt;/p&gt;
&lt;p&gt;For other tips, such as using math, see the guide on &lt;a href=&#34;https://wowchemy.com/docs/content/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;writing content with Academic&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;convert-notebook-to-markdown&#34;&gt;Convert notebook to Markdown&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;This post was created with Jupyter. The orginal files can be found at &lt;a href=&#34;https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
