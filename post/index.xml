<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Haofei Hou | HUST</title>
    <link>https://example.com/post/</link>
      <atom:link href="https://example.com/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 10 Apr 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu88cdd4b7a12988e7d5bb56be50e4f21a_10097_512x512_fill_lanczos_center_3.png</url>
      <title>Posts</title>
      <link>https://example.com/post/</link>
    </image>
    
    <item>
      <title>LLM/MMM for RL</title>
      <link>https://example.com/post/llm-for-rl/</link>
      <pubDate>Mon, 10 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/llm-for-rl/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Large Language Models&lt;/strong&gt;: GPT BERT&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-Modal Models&lt;/strong&gt;: CLIP&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;reasoning&#34;&gt;Reasoning&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;RT-1&lt;/strong&gt;: &amp;ldquo;RT-1: Robotics Transformer for Real-World Control at Scale&amp;rdquo;, &lt;em&gt;arXiv, Dec 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2212.06817&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]  [&lt;a href=&#34;https://github.com/google-research/robotics_transformer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;] [&lt;a href=&#34;https://robotics-transformer.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Code-As-Policies&lt;/strong&gt;: &amp;ldquo;Code as Policies: Language Model Programs for Embodied Control&amp;rdquo;, &lt;em&gt;arXiv, Sept 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2209.07753&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]  [&lt;a href=&#34;https://github.com/google-research/google-research/tree/master/code_as_policies&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colab&lt;/a&gt;] [&lt;a href=&#34;https://code-as-policies.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Say-Can&lt;/strong&gt;: &amp;ldquo;Do As I Can, Not As I Say: Grounding Language in Robotic Affordances&amp;rdquo;, &lt;em&gt;arXiv, Apr 2021&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2204.01691&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]  [&lt;a href=&#34;https://say-can.github.io/#open-source&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colab&lt;/a&gt;] [&lt;a href=&#34;https://say-can.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Socratic&lt;/strong&gt;: &amp;ldquo;Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language&amp;rdquo;, &lt;em&gt;arXiv, Apr 2021&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2204.00598&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://socraticmodels.github.io/#code&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://socraticmodels.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PIGLeT&lt;/strong&gt;: &amp;ldquo;PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World&amp;rdquo;, &lt;em&gt;ACL, Jun 2021&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2201.07207&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;http://github.com/rowanz/piglet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://rowanzellers.com/piglet/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;planning&#34;&gt;Planning&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LM-Nav&lt;/strong&gt;: &amp;ldquo;Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action&amp;rdquo;, &lt;em&gt;arXiv, July 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2207.04429&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/blazejosinski/lm_nav&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/lmnav&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230410013423928.png&#34; alt=&#34;image-20230410013423928&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;LLM: GPT-3 parse textual instructions into a sequence of landmarks ($t$).&lt;/li&gt;
&lt;li&gt;VLM: CLIP compute likelihood that image $i_k$ corresponds to the string $t$&lt;/li&gt;
&lt;li&gt;VNM: ViNG  uses a goal-conditioned distance function to infer connectivity between the set of raw observations and constructs a topological graph&lt;/li&gt;
&lt;li&gt;Navigation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;InnerMonlogue&lt;/strong&gt;: &amp;ldquo;Inner Monologue: Embodied Reasoning through Planning with Language Models&amp;rdquo;, &lt;em&gt;arXiv, July 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2207.05608&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://innermonologue.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;system Inner Monologue chains together these various components (perception models, robotic skills, and human feedback) in a shared language prompt, enabling it to successfully perform user instructions.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230410093435285.png&#34; alt=&#34;image-20230410093435285&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Inner Monologue(Left): InstructGPT&lt;/li&gt;
&lt;li&gt;pick-and-place primitive: CLIPort&lt;/li&gt;
&lt;li&gt;Manipulation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Housekeep&lt;/strong&gt;: &amp;ldquo;Housekeep: Tidying Virtual Households using Commonsense Reasoning&amp;rdquo;, &lt;em&gt;arXiv, May 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2205.10712&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yashkant/housekeep&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://yashkant.github.io/housekeep/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;New benchmark: In Housekeep, an embodied agent must tidy a house by rearranging misplaced objects without explicit instructions specifying which objects need to be rearranged&lt;/li&gt;
&lt;li&gt;A new dataset for an object (e.g. salt-shaker), a room (e.g. kitchen) in context, to classify into the following categories: misplaced, correct, implausible.&lt;/li&gt;
&lt;li&gt;Each Housekeep episode is created by instantiating 7-10 objects within a scene, out of which 3-5 objects are misplaced and the remaining are placed correct&lt;/li&gt;
&lt;li&gt;Object Room $[OR] &amp;ndash; P(room|object)$ : Generate compatibility scores for rooms for a given object.&lt;/li&gt;
&lt;li&gt;Object Room Receptacle $[ORR] &amp;ndash; P(receptacle|object, room)$: Generate compatibility scores for receptacles within a given room and for a given object&lt;/li&gt;
&lt;li&gt;Method1: Finetuning by Contrastive Matching (CM). We train a 3-layered MLP on top of language embeddings and compute pairwise cosine similarity between any two embeddings&lt;/li&gt;
&lt;li&gt;Method2: Zero-Shot Ranking via MLM (ZS-MLM).&lt;/li&gt;
&lt;li&gt;LLM: BERT for word embeddings&lt;/li&gt;
&lt;li&gt;MLM(Masked Language Model)&lt;/li&gt;
&lt;li&gt;Similarity calculation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LID&lt;/strong&gt;: &amp;ldquo;Pre-Trained Language Models for Interactive Decision-Making&amp;rdquo;, &lt;em&gt;arXiv, Feb 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2202.01771&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ShuangLI59/Language-Model-Pre-training-Improves-Generalization-in-Policy-Learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://shuangli-project.github.io/Pre-Trained-Language-Models-for-Interactive-Decision-Making/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409121632758.png&#34; alt=&#34;image-20230409121632758&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Fine-tuning&lt;/li&gt;
&lt;li&gt;Mid-level actions&lt;/li&gt;
&lt;li&gt;LLM&lt;/li&gt;
&lt;li&gt;As a part of policy network.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ZSP&lt;/strong&gt;: &amp;ldquo;Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents&amp;rdquo;, &lt;em&gt;ICML, Jan 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2201.07207&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/huangwl18/language-planner&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://wenlong.page/language-planner/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230410095827203.png&#34; alt=&#34;image-20230410095827203&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;VirtualHome Executability Correctness&lt;/li&gt;
&lt;li&gt;Planning LM (Causal LLM): GPT-3 $\hat{a}$&lt;/li&gt;
&lt;li&gt;Translation LM(Masked LLM): BERT&lt;/li&gt;
&lt;li&gt;for each admissible environment action $a_e$ $max\ C(f(\hat{a}), f(a_e)) = \frac{f(\hat{a})\cdot f(a_e)}{||f(\hat{a})||\ ||f(a_e)||}$&lt;/li&gt;
&lt;li&gt;Mid-level actions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;manipulation&#34;&gt;Manipulation&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DIAL&lt;/strong&gt;:&amp;ldquo;Robotic Skill Acquistion via Instruction Augmentation with Vision-Language Models&amp;rdquo;, &amp;ldquo;arXiv, Nov 2022&amp;rdquo;, [&lt;a href=&#34;https://arxiv.org/abs/2211.11736&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://instructionaugmentation.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409120027347.png&#34; alt=&#34;image-20230409120027347&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;CLIP Model&lt;/li&gt;
&lt;li&gt;Few Shot/Fine-tuning&lt;/li&gt;
&lt;li&gt;Manipulation trajectories&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CLIP-Fields&lt;/strong&gt;:&amp;ldquo;CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory&amp;rdquo;, &amp;ldquo;arXiv, Oct 2022&amp;rdquo;, [&lt;a href=&#34;https://arxiv.org/abs/2210.05663&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/notmahi/clip-fields&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyTorch Code&lt;/a&gt;] [&lt;a href=&#34;https://mahis.life/clip-fields/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;weakly supervised semantic neural fields, called CLIP-Fields.&lt;/li&gt;
&lt;li&gt;dataset comes from LLM/MMM&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409211255049.png&#34; alt=&#34;image-20230409211255049&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;$g(x, y, z) : R_3 → R_d$ serves as a generic differentiable spatial database.  trained.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409211327053.png&#34; alt=&#34;image-20230409211327053&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;CLIP BERT&lt;/li&gt;
&lt;li&gt;EVALUATION:  Instance and semantic segmentation in scene images.  for visual encoder&lt;/li&gt;
&lt;li&gt;EVALUATION:   Semantic Navigation on Robot with CLIP-Fields as Semantic-Spatial Memory.  maximizing their similarity.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LaTTe&lt;/strong&gt;: &amp;ldquo;LaTTe: Language Trajectory TransformEr&amp;rdquo;, &lt;em&gt;arXiv, Aug 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2208.02918&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/arthurfenderbucker/NL_trajectory_reshaper&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TensorFlow Code&lt;/a&gt;] [&lt;a href=&#34;https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/robot-language/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409122254407.png&#34; alt=&#34;image-20230409122254407&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;BERT CLIP&lt;/li&gt;
&lt;li&gt;trained and evaluated the model described in Section III over a dataset containing 100k examples of procedurally generated trajectory modification.&lt;/li&gt;
&lt;li&gt;Trajectory reshaping obeying user’s constraints.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Robots Enact Malignant Stereotypes&lt;/strong&gt;: &amp;ldquo;Robots Enact Malignant Stereotypes&amp;rdquo;, &lt;em&gt;FAccT, Jun 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2207.11569&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ahundt/RobotsEnactMalignantStereotypes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/robots-enact-stereotypes/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;] [&lt;a href=&#34;https://www.washingtonpost.com/technology/2022/07/16/racist-robots-ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Washington Post&lt;/a&gt;] [&lt;a href=&#34;https://www.wired.com/story/how-to-stop-robots-becoming-racist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wired&lt;/a&gt;] (code access on request)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;experiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy.&lt;/li&gt;
&lt;li&gt;Ethical experiments on LLM/MMM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ATLA&lt;/strong&gt;: &amp;ldquo;Leveraging Language for Accelerated Learning of Tool Manipulation&amp;rdquo;, &lt;em&gt;CoRL, Jun 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2206.13074&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;combining linguistic information and meta-learning significantly accelerates tool learning&lt;/li&gt;
&lt;li&gt;tools $ \mathscr{T} = { \tau_i }&lt;em&gt;{i=1}^K$     corresponding language descriptions $L_i = {l&lt;/em&gt;{ij}}&lt;em&gt;{j=1}^{N_i}\ l&lt;/em&gt;{ij} \in  \mathscr{L}$ by LLM&lt;/li&gt;
&lt;li&gt;Goal $\pi_{\theta}:  \mathscr{O} \times  \mathscr{L} \rightarrow  \mathscr{A}$ that can be quickly fine-tuned at test time.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230410005237562.png&#34; alt=&#34;image-20230410005237562&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;GPT&lt;/li&gt;
&lt;li&gt;RL: PPO Meta-training&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ZeST&lt;/strong&gt;: &amp;ldquo;Can Foundation Models Perform Zero-Shot Task Specification For Robot Manipulation?&amp;rdquo;, &lt;em&gt;L4DC, Apr 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2204.11134&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409120237050.png&#34; alt=&#34;image-20230409120237050&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Zero Shot&lt;/li&gt;
&lt;li&gt;CLIP Model&lt;/li&gt;
&lt;li&gt;Similarity calculation&lt;/li&gt;
&lt;li&gt;As Reward Function in offline RL&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LSE-NGU&lt;/strong&gt;: &amp;ldquo;Semantic Exploration from Language Abstractions and Pretrained Representations&amp;rdquo;, &lt;em&gt;arXiv, Apr 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2204.05080&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Encourage to do exploration $max\ E[\sum_{t=0}^H\gamma^t(r_t^e+\beta r_t^i)]$&lt;/li&gt;
&lt;li&gt;exploration method calculates the intrinsic reward from $O_L$ or $O_V$ .&lt;/li&gt;
&lt;li&gt;CLIP Bert ALM&lt;/li&gt;
&lt;li&gt;Zero-Shot for calculating the intrinsic reward (NGP: L2 distances between the current state and the k-nearest neighbor representations stored in the memory buffer, RND:  mean squared error $|| f_V(O_V) - \hat{f_V}(O_V)|| $ , $f_V(O_V)$ is trainable, $\hat{f_V}(O_V)$ is pre-trained model)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Embodied-CLIP&lt;/strong&gt;: &amp;ldquo;Simple but Effective: CLIP Embeddings for Embodied AI &amp;ldquo;, &lt;em&gt;CVPR, Nov 2021&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2111.09888&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/allenai/embodied-clip&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409144829744.png&#34; alt=&#34;image-20230409144829744&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Embodied AI tasks: agents that learn to navigate and interact with their environments.&lt;/li&gt;
&lt;li&gt;CLIP for visual encoder&lt;/li&gt;
&lt;li&gt;RL: PPO&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CLIPort&lt;/strong&gt;: &amp;ldquo;CLIPort: What and Where Pathways for Robotic Manipulation&amp;rdquo;, &lt;em&gt;CoRL, Sept 2021&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2109.12098&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cliport/cliport&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://cliport.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230410010919998.png&#34; alt=&#34;image-20230410010919998&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Transporter for Pick-and-Place: The same architecture has three different networks $f_{pick}, \Psi_{query}, \Psi_{key}$&lt;/li&gt;
&lt;li&gt;CLIP&lt;/li&gt;
&lt;li&gt;Trajectory Imitative learning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VIMA&lt;/strong&gt;:&amp;ldquo;VIMA: General Robot Manipulation with Multimodal Prompts&amp;rdquo;, &amp;ldquo;arXiv, Oct 2022&amp;rdquo;, [&lt;a href=&#34;https://arxiv.org/abs/2210.03094&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/vimalabs/VIMA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://vimalabs.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a transformer-based generalist robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409213812956.png&#34; alt=&#34;image-20230409213812956&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Mask R-CNN ViT for T5 Encoder&lt;/li&gt;
&lt;li&gt;Build his own MMM&lt;/li&gt;
&lt;li&gt;Trajectory Imitative learning on discretization action space&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Perceiver-Actor&lt;/strong&gt;:&amp;ldquo;A Multi-Task Transformer for Robotic Manipulation&amp;rdquo;, &lt;em&gt;CoRL, Sep 2022&lt;/em&gt;. [&lt;a href=&#34;https://peract.github.io/paper/peract_corl2022.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/peract/peract&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://peract.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PERACT encodes a sequence of RGB-D voxel(3D info) patches and predicts discretized translations,&lt;/li&gt;
&lt;li&gt;PERACT is essentially a classifier trained with supervised learning to detect actions&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409220107716.png&#34; alt=&#34;image-20230409220107716&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Language Encoder: CLIP&lt;/li&gt;
&lt;li&gt;Voxel Encoder pre-trained 3D convolution network&lt;/li&gt;
&lt;li&gt;Build his own MMM&lt;/li&gt;
&lt;li&gt;Trajectory Imitative learning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;instructions-and-navigation&#34;&gt;Instructions and Navigation&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ADAPT&lt;/strong&gt;: &amp;ldquo;ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts&amp;rdquo;, &lt;em&gt;CVPR, May 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2205.15509&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;The Unsurprising Effectiveness of Pre-Trained Vision Models for Control&amp;rdquo;, &lt;em&gt;ICML, Mar 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2203.03580&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sparisi/pvr_habitat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/pvr-control&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CoW&lt;/strong&gt;: &amp;ldquo;CLIP on Wheels: Zero-Shot Object Navigation as Object Localization and Exploration&amp;rdquo;, &lt;em&gt;arXiv, Mar 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2203.10421&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Recurrent VLN-BERT&lt;/strong&gt;: &amp;ldquo;A Recurrent Vision-and-Language BERT for Navigation&amp;rdquo;, &lt;em&gt;CVPR, Jun 2021&lt;/em&gt; [&lt;a href=&#34;https://arxiv.org/abs/2011.13922&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YicongHong/Recurrent-VLN-BERT&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VLN-BERT&lt;/strong&gt;: &amp;ldquo;Improving Vision-and-Language Navigation with Image-Text Pairs from the Web&amp;rdquo;, &lt;em&gt;ECCV, Apr 2020&lt;/em&gt; [&lt;a href=&#34;https://arxiv.org/abs/2004.14973&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/arjunmajum/vln-bert&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch Code&lt;/a&gt;]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Interactive Language: Talking to Robots in Real Time&amp;rdquo;, &lt;em&gt;arXiv, Oct 2022&lt;/em&gt; [&lt;a href=&#34;https://arxiv.org/abs/2210.06407&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://interactive-language.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;simulation-frameworks&#34;&gt;Simulation Frameworks&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MineDojo&lt;/strong&gt;: &amp;ldquo;MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge&amp;rdquo;, &lt;em&gt;arXiv, Jun 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2206.08853&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MineDojo/MineDojo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://minedojo.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;] [&lt;a href=&#34;https://minedojo.org/knowledge_base.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Open Database&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Habitat 2.0&lt;/strong&gt;: &amp;ldquo;Habitat 2.0: Training Home Assistants to Rearrange their Habitat&amp;rdquo;, &lt;em&gt;NeurIPS, Dec 2021&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2106.14405&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/facebookresearch/habitat-sim&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://aihabitat.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BEHAVIOR&lt;/strong&gt;: &amp;ldquo;BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments&amp;rdquo;, &lt;em&gt;CoRL, Nov 2021&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2108.03332&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/StanfordVL/behavior&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://behavior.stanford.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;iGibson 1.0&lt;/strong&gt;: &amp;ldquo;iGibson 1.0: a Simulation Environment for Interactive Tasks in Large Realistic Scenes&amp;rdquo;, &lt;em&gt;IROS, Sep 2021&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2012.02924&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/StanfordVL/iGibson&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://svl.stanford.edu/igibson/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ALFRED&lt;/strong&gt;: &amp;ldquo;ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks&amp;rdquo;, &lt;em&gt;CVPR, Jun 2020&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/1912.01734&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/askforalfred/alfred&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://askforalfred.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BabyAI&lt;/strong&gt;: &amp;ldquo;BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning&amp;rdquo;, &lt;em&gt;ICLR, May 2019&lt;/em&gt;. [&lt;a href=&#34;https://openreview.net/pdf?id=rJeXCo0cYX&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mila-iqia/babyai/tree/iclr19&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Dark, Beyond Deep - A Paradigm Shift to Cognitive AI with Humanlike Common Sense</title>
      <link>https://example.com/post/dark-beyond-deep/</link>
      <pubDate>Sun, 02 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/dark-beyond-deep/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;“small data for big tasks” paradigm&lt;/li&gt;
&lt;li&gt;models of common sense&lt;/li&gt;
&lt;li&gt;We identify functionality, physics, intent, causality, and utility (FPICU) as the five core domains of cognitive AI with humanlike common sense.&lt;/li&gt;
&lt;li&gt;FPICU is concerned with the questions of “why” and “how,” beyond the dominant “what” and “where” framework for understanding vision. They are invisible in terms of pixels but nevertheless drive the creation, maintenance, and development of visual scenes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;a-call-for-a-paradigm-shift-in-vision-and-ai&#34;&gt;A Call for a Paradigm Shift in Vision and AI&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;The classic definition of computer vision proposed by the pioneer David Marr is to look at “what” is “where.” Here, “what” refers to object recognition (object vision), and “where” denotes three-dimensional (3D) reconstruction and object localization (spatial vision)&lt;/li&gt;
&lt;li&gt;require large sets of labeled training data designed for special tasks, and lack a general understanding of common facts—that is, facts that are obvious to the average human adult—that describe how our physical and social worlds work.&lt;/li&gt;
&lt;li&gt;missing dimensions and the potential benefits of joint representation and joint inference.&lt;/li&gt;
&lt;li&gt;The concept of “darkness” is perpendicular to and richer than the meanings of “latent” or “hidden” used in vision and probabilistic modeling;  darkness” is a measure of the relative difficulty of classifying an entity or inferring about a relationship based on how much invisible common sense needed beyond the visible appearance or geometry.&lt;/li&gt;
&lt;li&gt;Section 2: paper starts by revisiting a classic view of computer vision in terms of “what” and “where”, task-driven&lt;/li&gt;
&lt;li&gt;Section 3: In order to use “small data” to solve “big tasks,” we then identify and review five crucial axes of visual common sense: Functionality, Physics, perceived Intent, Causality, and Utility (FPICU). Causality&lt;/li&gt;
&lt;li&gt;Section 4: The application of causality (i.e., intuitive physics; )&lt;/li&gt;
&lt;li&gt;Section 5: Functionality&lt;/li&gt;
&lt;li&gt;Section 6: infer intent&lt;/li&gt;
&lt;li&gt;Section 7: utility-driven&lt;/li&gt;
&lt;li&gt;In a series of studies, we demonstrate that these five critical aspects of “dark entities” and “dark relationships” indeed support various visual tasks beyond just classification.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;vision-from-data-driven-to-task-driven&#34;&gt;Vision: From Data-driven to Task-driven&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;From a biological perspective, the majority of living creatures use a single (with multiple components) vision system to perform thousands of tasks.&lt;/li&gt;
&lt;li&gt;these results indicate that our biological vision system possesses a mechanism for perceiving object functionality (i.e., how an object can be manipulated as a tool) that is independent of the mechanism governing face recognition (and recognition of other objects)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-task-centered-visual-recognition&#34;&gt;“What”: Task-centered Visual Recognition&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;these approaches have left unclear how classification interacts with scene semantics and enables cognitive reasoning&lt;/li&gt;
&lt;li&gt;human vision organizes representations during the inference process even for “simple” categorical recognition tasks.&lt;/li&gt;
&lt;li&gt;scene categorization and the information-gathering process are constrained by these categorization tasks, suggesting a bidirectional interplay between the visual input and the viewer’s needs/tasks&lt;/li&gt;
&lt;li&gt;the representation of the same object can vary according to the planned task&lt;/li&gt;
&lt;li&gt;task-driven nature of scene categorization.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;where-constructing-3d-scenes-as-a-series-of-tasks&#34;&gt;“Where”: Constructing 3D Scenes as a Series of Tasks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;scene reconstruction from a single two-dimensional (2D) image is a well-known illposed problem; there may exist an infinite number of possible 3D configurations that match the projected 2D observed images&lt;/li&gt;
&lt;li&gt;enable agents to perform tasks by generating the best possible configuration in terms of functionality, physics, and object relationships.&lt;/li&gt;
&lt;li&gt;there is now abundant evidence that humans represent the 3D layout of a scene in a way that fundamentally differs from any current computer vision algorithms&lt;/li&gt;
&lt;li&gt;human vision is error-prone and distorted in terms of localization&lt;/li&gt;
&lt;li&gt;Grid cells encode a cognitive representation of Euclidean space, implying a different mechanism for perceiving and processing locations and directions.&lt;/li&gt;
&lt;li&gt;Xie et al.  proposed a representational model for grid cells, in which the 2D self-position of an agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector.&lt;/li&gt;
&lt;li&gt;how we navigate complex environments while remaining able at all times to return to an original location (i.e., homing) remains a mystery in biology and neuroscience.&lt;/li&gt;
&lt;li&gt;the task-dependent representation of space can shed some light.&lt;/li&gt;
&lt;li&gt;neither based on a stable 3D model of a scene nor a distorted one; instead, participants seemed to form a flat and task-dependent representation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;beyond-what-and-where-towards-scene-understanding-with-humanlike-common-sense&#34;&gt;Beyond “What” and “Where”: Towards Scene Understanding with Humanlike Common Sense&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;rich as videos and much sparser visual inputs&lt;/li&gt;
&lt;li&gt;To enable an artificial agent with similar capabilities, we call for joint reasoning algorithms on a joint representation that integrates (i) the “visible” traditional recognition and categorization of objects, scenes, actions, events, and so forth; and (ii) the “dark” higher level concepts of fluent, causality, physics, functionality, affordance, intentions/goals, utility, and so forth.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;fluent-and-perceived-causality&#34;&gt;Fluent and Perceived Causality&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A fluent refers to a transient state of an object that is time-variant.&lt;/li&gt;
&lt;li&gt;Fluents are linked to perceived causality in the psychology literature.&lt;/li&gt;
&lt;li&gt;Fluents and perceived causality are different from the visual attributes of objects. The latter are permanent over the course of observation;&lt;/li&gt;
&lt;li&gt;Human cognition has the innate capability (observed in infants) and strong inclination to perceive the causal effects between actions and changes of fluents;&lt;/li&gt;
&lt;li&gt;but most daily actions, such as opening a door, are defined by cause and effect (a door’s fluent changes from “closed” to “open,” regardless of how it is opened), rather than by the human’s position, movement, or spatial-temporal features&lt;/li&gt;
&lt;li&gt;Overall, the status of a scene can be viewed as a collection of fluents that record the history of actions. Nevertheless, fluents and causal reasoning have not yet been systematically studied in machine vision, despite their ubiquitous presence in images and videos.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;intuitive-physics&#34;&gt;Intuitive Physics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;the knowledge of Newtonian principles and probabilistic representations is generally applied in human physical reasoning, and that an intuitive physical model is an important aspect of human-level complex scene understanding.&lt;/li&gt;
&lt;li&gt;By human design, objects should be physically stable and safe with respect to gravity and various other potential disturbances&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;functionality&#34;&gt;Functionality&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;These functions and needs are invisible in images, but shape the scene’s layout.&lt;/li&gt;
&lt;li&gt;researchers identified mirror neurons in the pre-motor cortical area that seem to encode actions through poses and interactions with objects and scenes [102]. Concepts in the human mind are not only represented by prototypes—that is, exemplars as in current computer vision and machine learning approaches—but also by functionality&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;intentions-and-goals&#34;&gt;Intentions and Goals&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We argue that intent can be treated as the transient status of agents (humans and animals)&lt;/li&gt;
&lt;li&gt;(i) They are hierarchically organized in a sequence of goals and are the main factors driving actions and events in a scene. (ii) They are completely “dark,” that is, not represented by pixels. (iii) Unlike the instant change of fluents in response to actions, intentions are often formed across long spatiotemporal ranges.&lt;/li&gt;
&lt;li&gt;functional object&lt;/li&gt;
&lt;li&gt;emits a field of attraction over the scene, not much different from a gravity field or an electric field&lt;/li&gt;
&lt;li&gt;The trajectory of a person with a certain intention moving through these fields follows a least-action principle in Lagrange mechanics that derives all motion equations by minimizing the potential and kinematic energies integrated over time&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;utility-and-preference&#34;&gt;Utility and Preference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we can mostly assume that the observed agents make near-optimal choices to minimize the cost of certain tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Generalization.&lt;/li&gt;
&lt;li&gt;Small sample learning.&lt;/li&gt;
&lt;li&gt;Bidirectional inference.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;causal-perception-and-reasoning-the-basis-for-understanding&#34;&gt;Causal Perception and Reasoning: The Basis for Understanding&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;People have innate assumptions about causes, and causal reasoning can be activated almost automatically and irresistibly. In our opinion, causality is the foundation of the other four FPICU elements (functionality, physics, intent, and utility).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;human-causal-perception-and-reasoning&#34;&gt;Human Causal Perception and Reasoning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Early psychological work focused on an associative mechanism as the basis for human causal learning and reasoning&lt;/li&gt;
&lt;li&gt;Rescorla-Wagner model was used to explain how humans (and animals) build expectations using the cooccurrence of perceptual stimuli&lt;/li&gt;
&lt;li&gt;more recent studies have shown that human causal learning is a rational Bayesian process [126, 129, 130] involving the acquisition of abstract causal structure [131, 132] and strength values for cause-effect relationships&lt;/li&gt;
&lt;li&gt;Irresistibility. One cannot stop seeing salient causality, just as one cannot stop seeing color and depth.&lt;/li&gt;
&lt;li&gt;Tight control by spatial-temporal patterns of motion&lt;/li&gt;
&lt;li&gt;Richness&lt;/li&gt;
&lt;li&gt;“adaptation” is a phenomenon in which an observer adapts to stimuli after a period of sustained viewing, such that their perceptual response to those stimuli becomes weaker&lt;/li&gt;
&lt;li&gt;physical causality is extracted during early visual processing&lt;/li&gt;
&lt;li&gt;One unique function of causality is the support of counterfactual reasoning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;causal-transfer-challenges-for-machine-intelligence&#34;&gt;Causal Transfer: Challenges for Machine Intelligence&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Recent successes of systems such as deep reinforcement learning (RL) showcase a broad range of applications&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;causality-in-statistical-learning&#34;&gt;Causality in Statistical Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;“Estimating causal effects f treatments in randomized and nonrandomized studies;”&lt;/li&gt;
&lt;li&gt;Rubin causal model&lt;/li&gt;
&lt;li&gt;the Rubin causal model is potential outcomes.&lt;/li&gt;
&lt;li&gt;A common manifestation of this problem is the latent variables that influence both the treatment assignment and the potential outcomes&lt;/li&gt;
&lt;li&gt;A very prominent example is the propensity score [148], which is the conditional probability of assigning one treatment to a subject given the background variables of the subject&lt;/li&gt;
&lt;li&gt;Causality was further developed in Pearl’s probabilistic graphical model (i.e., causal Bayesian networks (CBNs))&lt;/li&gt;
&lt;li&gt;despite attempts to learn causal structure from observational data, most structure learning approaches cannot typically succeed beyond identifying a Markov equivalence class of possible structures&lt;/li&gt;
&lt;li&gt;These works suggest that human causal perception is less rigorous than formal science but still maintains effectiveness in learning and understanding of daily events.&lt;/li&gt;
&lt;li&gt;Fire and Zhu proposed a method to learn “dark” causal relationships from image and video inputs, as illustrated in Fig. 14; in this study, systems learn how the status of a door, light, and screen relate to human actions&lt;/li&gt;
&lt;li&gt;To answer this question, the method utilizes the information projection framework, maximizing the amount of information gain after adding a causal relation,  and then minimizing the divergence between the model and observed statistics.&lt;/li&gt;
&lt;li&gt;Xu et al. used a Causal And-Or Graph (C-AOG) model to tackle this kind of “visibility fluent reasoning” problem. They consider the visibility status of an object as a fluent variable, whose change is mostly attributed to its interaction with its surroundings,&lt;/li&gt;
&lt;li&gt;jointly reason about the visibility fluent change and track humans&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;causality-in-computer-vision&#34;&gt;Causality in Computer Vision&lt;/h2&gt;
&lt;h1 id=&#34;intuitive-physics-cues-of-the-physical-world&#34;&gt;Intuitive Physics: Cues of the Physical World&lt;/h1&gt;
&lt;h2 id=&#34;intuitive-physics-in-human-cognition&#34;&gt;Intuitive Physics in Human Cognition&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Early research in intuitive physics provides several examples of situations in which humans demonstrate common misconceptions about how objects in the environment behave.&lt;/li&gt;
&lt;li&gt;researchers have developed alternative experimental approaches to study the development of infants’ physical knowledge. The most widely used approach is the violation-of-expectation method, in which infants see two test events: an expected event, consistent with the expectation shown, and an unexpected event, violating the expectation.&lt;/li&gt;
&lt;li&gt;These findings suggest that these brain regions use a generalized mental engine for intuitive physical inference—that is, the brain’s “physics engine.”&lt;/li&gt;
&lt;li&gt;Human intuitive physics can be modeled as an approximated physical engine with a Bayesian probabilistic model, possessing the following distinguishing properties&lt;/li&gt;
&lt;li&gt;expands to the perception and simulation of the physical properties of liquids and sand&lt;/li&gt;
&lt;li&gt;instead, they rely on perceived physical variables to make quantitative judgments.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;physics-based-reasoning-in-computer-vision&#34;&gt;Physics-based Reasoning in Computer Vision&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Statistical modeling aims to capture the “patterns generated by the world in any modality, with all their naturally occurring complexity and ambiguity, with the goal of reconstructing the processes, objects and events that produced them.”&lt;/li&gt;
&lt;li&gt;Alternatively, perceptual organization and Gestalt laws aim to resolve the 3D reconstruction problem from a single RGB image without considering depth. Instead, they use priors—groupings and structural cues that are likely to be invariant over wide ranges of viewpoints—resulting in feature-based approaches .&lt;/li&gt;
&lt;li&gt;Stability and safety in scene understanding.&lt;/li&gt;
&lt;li&gt;Physical relationships in 3D scenes&lt;/li&gt;
&lt;li&gt;very limited (if any) physics-based simulation is applied.&lt;/li&gt;
&lt;li&gt;Specifically, a generative model named Galileo was proposed for physical scene understanding using real-world videos and images.&lt;/li&gt;
&lt;li&gt;The model can infer these latent properties using relatively brief runs of markov chain monte carlo (MCMC), which drive simulations in the physics engine to fit key features of visual observations.&lt;/li&gt;
&lt;li&gt;With a new dataset named Physics 101 containing 17 408 video clips and 101 objects of various materials and appearances (i.e., shapes, colors, and sizes), the proposed unsupervised representation learning model, which explicitly encodes basic physical laws into the structure, can learn the physical properties of objects from videos.&lt;/li&gt;
&lt;li&gt;built a system that calculated various physical concepts from just a single example of tool use (Fig. 19), enabling it to reason about the essential physical concepts of the task (e.g., the force required to crack nuts).&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;functionality-and-affordance-the-opportunity-for-task-and-action&#34;&gt;Functionality and Affordance: The Opportunity for Task and Action&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Perception of an environment inevitably leads to a course of action&lt;/li&gt;
&lt;li&gt;“an object is first identified as having important functional relations”&lt;/li&gt;
&lt;li&gt;“perceptual analysis is derived of the functional concept”&lt;/li&gt;
&lt;li&gt;Functional understanding of objects and scenes is rooted in identifying possible tasks that can be performed with an object . This is deeply related to the perception of causality, as covered in Section 3; to understand how an object can be used, an agent must understand what change of state will result if an object is interacted with in any way. While affordances depend directly on the actor, functionality is a permanent property of an object independent of the characteristics of the user; see an illustration of this distinction in Fig. 21. These two interweaving concepts are more invariant for object and scene understanding than their geometric and appearance aspects. Specifically, we argue that:&lt;/li&gt;
&lt;li&gt;Objects, especially human-made ones, are defined by their functions, or by the actions they are associated with;&lt;/li&gt;
&lt;li&gt;Scenes, especially human-made ones, are defined by the actions than can be performed within them.&lt;/li&gt;
&lt;li&gt;functionality and affordance&lt;/li&gt;
&lt;li&gt;both the object level and scene level&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;revelation-from-tool-use-in-animal-cognition&#34;&gt;Revelation from Tool Use in Animal Cognition&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Dr. Jane Goodall observed wild chimpanzees manufacturing and using tools with regularity&lt;/li&gt;
&lt;li&gt;some animals have the capability (and possibly the intrinsic motivation) to reason about the functional properties of tools.&lt;/li&gt;
&lt;li&gt;First, why can some species devise innovative solutions, while others facing the same situation cannot? Look at the example in Fig. 20 [232]: by observing only a single demonstration of a person achieving the complex task of cracking a nut, we humans can effortlessly reason about which of the potential candidates from a new set of random and very different objects is best capable of helping us complete the same task. Reasoning across such large intraclass variance is extremely difficult to capture and describe for modern computer vision and AI systems.&lt;/li&gt;
&lt;li&gt;Second, how can this functional reasoning capability emerge if one does not possess it innately? New Caledonian crows are well-known for their propensity and dexterity at making and using tools; meanwhile, although a crow’s distant cousin, the rook, is able to reason and use tools in a lab setting, even they do not use tools in the wild [259]. These findings suggest that the ability to represent tools may be more of a domain-general cognitive capacity based on functional reasoning than an adaptive specialization.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;perceiving-functionality-and-affordance&#34;&gt;Perceiving Functionality and Affordance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;available structures should be described in terms of functions provided and functions performed.&lt;/li&gt;
&lt;li&gt;They pointed out that it is possible to use a single functional description to represent all possible cups, despite there being an infinite number of individual physical descriptions of cups or many other objects.&lt;/li&gt;
&lt;li&gt;“Tool”  Zhu et al. [232] cast the tool understanding problem as a task-oriented object-recognition problem, the core of which is understanding an object’s underlying functions, physics, and causality.&lt;/li&gt;
&lt;li&gt;“Container”  they showed six-year-old children could still be confused by the complex phenomenon of pouring liquid into containers. one of the earliest spatial relationships to be learned, preceding other common ones e.g., occlusions [272] and support relationships. ontology, topology, first-order logic, and knowledge base.&lt;/li&gt;
&lt;li&gt;“Chair” is an exemplar class for affordance. In particular, Grabner et al. [108] designed an “affordance detector” for chairs by fitting typical human sitting poses onto 3D objects.&lt;/li&gt;
&lt;li&gt;“Human” context has proven to be a critical component in modeling the constraints on possible usage of objects in a scene. In approaching this kind of problem, all methods imagine different potential human positioning relative to objects to help parse and understand the visible elements of the scene.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;mirroring-causal-equivalent-functionality--affordance&#34;&gt;Mirroring: Causal-equivalent Functionality &amp;amp; Affordance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;It is difficult to evaluate a computer vision or AI system’s facility at reasoning with functionality and affordance;&lt;/li&gt;
&lt;li&gt;the same object or environment does not necessarily introduce the same functionality and affordance to both robots and humans&lt;/li&gt;
&lt;li&gt;In these cases, a system must reason about the underlying mechanisms of affordance, rather than simply mimicking the motions of a human demonstration. This common problem is known as the “correspondence problem”  in learning from demonstration (LfD);&lt;/li&gt;
&lt;li&gt;Currently, the majority of work in LfD uses a one-to-one mapping between human demonstration and robot execution, restricting the LfD to mimicking the human’s low-level motor controls and replicating a nearly identical procedure. Consequently, the “correspondence problem” is insufficiently addressed, and the acquired skills are difficult to adapt to new robots or new situations;&lt;/li&gt;
&lt;li&gt;robot must obtain deeper understanding in functional and causal understanding of the manipulation, which demands more explicit modeling of knowledge about physical objects and forces. The key to imitating manipulation is using functionality and affordance to create causal-equivalent manipulation;&lt;/li&gt;
&lt;li&gt;Rather than over-imitating the motion trajectories of the demonstration, the robot is encouraged to seek functionally equivalent but possibly visually different actions that can produce the same effect and achieve the same goal as those in the demonstration.&lt;/li&gt;
&lt;li&gt;force-based&lt;/li&gt;
&lt;li&gt;goal-oriented:&lt;/li&gt;
&lt;li&gt;mirroring without overimitation&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;perceiving-intent-the-sense-of-agency&#34;&gt;Perceiving Intent: The Sense of Agency&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Crucially, such a sense of agency further entails&lt;/li&gt;
&lt;li&gt;intentionality&lt;/li&gt;
&lt;li&gt;rationality of actions in relation to goals&lt;/li&gt;
&lt;li&gt;The perception and comprehension of intent enable humans to better understand and predict the behavior of other agents and engage with others in cooperative activities with shared goals.&lt;/li&gt;
&lt;li&gt;rationality principle as the mechanism with which both infants and adults perceive animate objects as intentional beings.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-sense-of-agency&#34;&gt;The Sense of Agency&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;theory of mind (ToM) refers to the ability to attribute mental states, including beliefs, desires, and intentions, to oneself and others. Perceiving and understanding an agent’s intent based on their belief and desire is the ultimate goal, since people largely act to fulfill intentions arising from their beliefs and desires.&lt;/li&gt;
&lt;li&gt;After their first birthday, infants begin to understand that an actor may consider various plans to pursue a goal, and choose one to intentionally enact based on environmental reality . Eighteen-month-old children are able to both infer and imitate the intended goal of an action even if the action repeatedly fails to achieve the goal&lt;/li&gt;
&lt;li&gt;concrete action goals, higher order plans, and collaborative goals&lt;/li&gt;
&lt;li&gt;Despite the complexity of the behavioral streams we actually witness, we readily process action in intentional terms from infancy onward [303]. It is underlying intent, rather than surface behavior, that matters when we observe motions&lt;/li&gt;
&lt;li&gt;Research has found that we do not encode the complete details of human motion in space; instead, we perceive motions in terms of intent&lt;/li&gt;
&lt;li&gt;Reading intentions has even led to species-unique forms of cultural learning and cognition&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;from-animacy-to-rationality&#34;&gt;From Animacy to Rationality&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;latent mental states about goals, beliefs, and intentions from nothing but visual stimuli. Surprisingly, such visual stimuli do not need to contain rich semantics or visual features. An iconic illustration of this is the seminal Heider-Simmel display created in the 1940s [313]; see Fig. 28 for more detail.&lt;/li&gt;
&lt;li&gt;dynamic motion and temporal contingency were the crucial factors for the successful perception of social relationships and mental states&lt;/li&gt;
&lt;li&gt;A question naturally arises: what is the underlying mechanism with which the human visual system perceives and interprets such a richly social world?&lt;/li&gt;
&lt;li&gt;“rationality principle.”&lt;/li&gt;
&lt;li&gt;This theory states that humans view themselves and others as causal agents: (i) they devote their limited time and resources only to those actions that change the world in accordance with their intentions and desires; and (ii) they achieve their intentions rationally by maximizing their utility while minimizing their costs, given their beliefs about the world&lt;/li&gt;
&lt;li&gt;psychophysics of chasing, one of the most salient and evolutionarily important types of intentional behavior.&lt;/li&gt;
&lt;li&gt;The results showed that humans can effectively detect and avoid wolves with small subtlety values, whereas wolves with modest subtlety values turned out to be the most “dangerous.&lt;/li&gt;
&lt;li&gt;This result is consistent with the “rationality principle,” where human perception assumes that an agent’s intentional action will be one that maximizes its efficiency in reaching its goal.&lt;/li&gt;
&lt;li&gt;Such an early-emerging sensitivity to the causal powers of agents engaged in costly and goal-directed actions may provide one important foundation for the rich causal and social learning that characterizes our species.&lt;/li&gt;
&lt;li&gt;The rationality principle has been formally modeled as inverse planning governed by Bayesian inference [104, 323, 114]. Planning is a process by which intent causes action. Inverse planning, by inverting the rational planning model via Bayesian inference that integrates the likelihood of observed actions with prior mental states, can infer the latent mental intent. Based on inverse planning, Baker et al. [104] proposed a framework for goal inference, in which the bottom-up information of behavior observations and the top-down prior knowledge of goal space are integrated to allow inference of underlying intent. In addition, Bayesian networks, with their flexibility in representing probabilistic dependencies and causal relationships, as well as the efficiency of inference methods, have proven to be one of the most powerful and successful approaches for intent recognition&lt;/li&gt;
&lt;li&gt;Moving from the symbolic input to real video input, Holtzen et al. [318] presented an inverse planning method to infer human hierarchical intentions from partially observed RGBD videos. Their algorithm is able to infer human intentions by reverse-engineering decision-making and action planning processes in human minds under a Bayesian probabilistic programming framework; see Fig. 30 [318] for more details. The intentions are represented as a novel hierarchical, compositional, and probabilistic graph structure that describes the relationships between actions and plans.&lt;/li&gt;
&lt;li&gt;By bridging from the abstract Heider-Simmel display to aerial videos, Shu et al. [112] proposed a method to infer humans’ intentions with respect to interaction by observing motion trajectories (Fig. 31).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;beyond-action-prediction&#34;&gt;Beyond Action Prediction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In modern computer vision and AI systems [327], intent is related to action prediction much more profoundly than through simply predicting action labels.&lt;/li&gt;
&lt;li&gt;(i) action-effect association, (ii) simulation procedures, and (iii) teleological reasoning. They concluded that action-effect association and simulation could only serve action monitoring and prediction; social learning, in contrast, requires the inferential productivity of teleological reasoning.&lt;/li&gt;
&lt;li&gt;Simulation theory claims that the mechanism underlying the attribution of intentions to actions might rely on simulating the observed action and mapping it onto our own experiences and intent representations&lt;/li&gt;
&lt;li&gt;In order to understand others’ intentions, humans subconsciously empathize with the person they are observing and estimate what their own actions and intentions might be in that situation. Here, action-effect association [329] plays an important role in quick online intent prediction, and the ability to encode and remember these two component associations contributes to infants’ imitation skills and intentional action understanding&lt;/li&gt;
&lt;li&gt;mirror neuron [331], which has been linked to intent understanding in many studies&lt;/li&gt;
&lt;li&gt;To address social learning, a teleological action interpretational system [335] takes a “functional stance” for the computational representation of goal-directed action [103], where such teleological representations are generated by the aforementioned inferential “rationality principle”&lt;/li&gt;
&lt;li&gt;Furthermore, action predictions can be made by breaking down a path toward a goal into a hierarchy of sub-goals, the most basic of which are comprised of elementary motor acts such as grasping.&lt;/li&gt;
&lt;li&gt;These three mechanisms do not compete; instead, they complement each other. The fast effect prediction provided by action-effect associations can serve as a starting hypothesis for teleological reasoning or simulation procedure; the solutions provided by teleological reasoning in social learning can also be stored as action-effect associations for subsequent rapid recall.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;building-blocks-for-intent-in-computer-vision&#34;&gt;Building Blocks for Intent in Computer Vision&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;visual surveillance, human-robot interaction, and autonomous driving. In order to better predict intent based on pixel inputs, it is necessary and indispensable to fully exploit comprehensive cues such as motion trajectory, gaze dynamics, body posture and movements, human-object relationships, and communicative gestures (e.g., pointing).&lt;/li&gt;
&lt;li&gt;Motion trajectory alone could be a strong signal for intent prediction,&lt;/li&gt;
&lt;li&gt;Shu et al. [113] studied possible underlying computational mechanisms and proposed a unified psychological space that reveals the partition between the perception of physical events involving inanimate objects and the perception of social events involving human interactions with other agents.&lt;/li&gt;
&lt;li&gt;Eye gaze, being closely related to underlying attention, intent, emotion, personality, and anything a human is thinking and doing, also plays an important role in allowing humans to “read” other peoples’ minds.  Social eye gaze functions also transcend cultural differences, forming a kind of universal language.  Wei et al. [334] proposed a hierarchical human-attention-object (HAO) model that represents tasks, intentions, and attention under a unified framework. Under this model, a task is represented as sequential intentions described by hand-eye coordination under a planner represented by a grammar;&lt;/li&gt;
&lt;li&gt;Communicative gazes and gestures (e.g., pointing) stand out for intent expression and perception in collaborative interactions.  They examined the inferring of shared eye gazes in third-person social scene videos, which is a phenomenon in which two or more individuals simultaneously look at a common target in social scenes. A follow-up work [340] studied various types of gaze communications in social activities from both the atomic level and event level&lt;/li&gt;
&lt;li&gt;Humans communicate intentions multimodally; thus, facial expression, head pose, body posture and orientation, arm motion, gesture, proxemics, and relationships with other agents and objects can all contribute to human intent analysis and comprehension. intent recognition that focuses on uncertainty reduction.  Shu et al. [344] presented a generative model for robot learning of social affordance from human activity videos.&lt;/li&gt;
&lt;li&gt;Such social affordance could also be represented by a hierarchical grammar model [345], enabling real-time motion inference for human-robot interaction;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;learning-utility-the-preference-of-choices&#34;&gt;Learning Utility: The Preference of Choices&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;an agent makes rational decisions choices based on their beliefs and desires to maximize its expected utility. This is known as the principle of maximum expected utility&lt;/li&gt;
&lt;li&gt;A utility function is a mathematical formulation that ranks the preferences of an individual such that
$U(a) &amp;gt; U(b)$, where choice $a$ is preferred over choice $b$.&lt;/li&gt;
&lt;li&gt;By observing a rational agent’s preferences, however, an observer can construct a utility function that represents what the agent is actually trying to achieve, even if the agent does not know it [346]. It is also worth noting that utility theory is a positive theory that seeks to explain the individuals’ observed behavior and choices, which is different from a normative theory that indicates how people should behave;&lt;/li&gt;
&lt;li&gt;Formally, the core idea behind utility theory is straightforward: every possible action or state within a given model can be described with a single, uniform value.&lt;/li&gt;
&lt;li&gt;utility measures how much we desire something in a more subjective and contextdependent perspective, whereas value is a measurable quantity (e.g., price), which tends to be more objective.&lt;/li&gt;
&lt;li&gt;Similarly, Shukla et al. [350] adopted the idea of learning human utility in order to teach a robotics task using human demonstrations.&lt;/li&gt;
&lt;li&gt;In addition, the rationality principle has been studied in the field of linguistics and philosophy, notably in influential work on the theory of implicature by Grice [351]. The core insight of Grice’s work is that language use is a form of rational action; thus, technical tools for reasoning about rational action should elucidate linguistic phenomena&lt;/li&gt;
&lt;li&gt;Based on how the expected utility influences the distribution, social goals (e.g., cooperation and competition) [364, 365] and faireness [366] can also be well explained. On a broader scale, utility can enable individuals to be self-identified in society during the social learning process;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;summary-and-discussions&#34;&gt;Summary and Discussions&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Today’s robots fundamentally lack physical and social common sense; this limitation inhibits their capacity to aid in our daily lives. In this article, we have reviewed five concepts that are the crucial building blocks of common sense: functionality, physics, intent, causality, and utility (FPICU).&lt;/li&gt;
&lt;li&gt;There are indeed many other topics that we believe are also essential AI ingredients; for example&lt;/li&gt;
&lt;li&gt;A physically realistic VR/MR platform: from big data to big tasks.  Here, we argue that the ultimate standard for validating the effectiveness of FPICU in AI is to examine whether an agent is capable of (i) accomplishing the very same task using different sets of objects with different instructions and/ or sequences of actions in different environments; and (ii) rapidly adapting such learned knowledge to entirely new tasks.&lt;/li&gt;
&lt;li&gt;Social system: the emergence of language, communication, and morality.  In most cases, algorithms designed for a single agent would be difficult to generalize to a multiple-agent systems (MAS) setting&lt;/li&gt;
&lt;li&gt;Measuring the limits of an intelligence system: IQ tests.  John C. Raven [373] proposed the raven’s prograssive matrices test (RPM) in the image domain. Empirical studies show that abstract-level reasoning, combined with effective feature-extraction models, could notably improve the performance of reasoning, analogy, and generalization. However, the performance gap between human and computational models calls for future research in this field;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;physically-realistic-vrmr-platform-from-big-data-to-big-tasks&#34;&gt;Physically-Realistic VR/MR Platform: From Big-Data to Big-Tasks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A hallmark of machine intelligence is the capability to rapidly adapt to new tasks and “achieve goals in a wide range of environments”&lt;/li&gt;
&lt;li&gt;Such synthetic data could be relatively easily scaled up compared with traditional data collection and labeling processes.&lt;/li&gt;
&lt;li&gt;synthetic data from the virtual world is becoming increasingly similar to data collected from the physical world.&lt;/li&gt;
&lt;li&gt;Using a holistic evaluation, whether a method or a system is intelligent or not is no longer measured by the successful performance of a single narrow task; rather, it is measured by the ability to perform well across various tasks:&lt;/li&gt;
&lt;li&gt;To build this kind of task-driven evaluation, physicsbased simulations for multi-material, multi-physics phenomena (Fig. 37) will play a central role.&lt;/li&gt;
&lt;li&gt;Here, we provide a brief review of the recent physics-based simulation methods, with a particular focus on the material point method (MPM).&lt;/li&gt;
&lt;li&gt;The accuracy of physics-based reasoning greatly relies on the fidelity of a physics-based simulation.&lt;/li&gt;
&lt;li&gt;The most challenging problems are those involving extreme deformation, topology change, and interactions among different materials and phases.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;social-system-emergence-of-language-communication-and-morality&#34;&gt;Social System: Emergence of Language, Communication, and Morality&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In classic AI, a multiagent communication strategy is modeled using a predefined rule-based system (e.g., adaptive learning of communication strategies in MAS&lt;/li&gt;
&lt;li&gt;By modeling communication as a particular type of action, recent research [370, 443, 444] has shown that agents can learn how to communicate with continuous signals that are only decipherable within a group.&lt;/li&gt;
&lt;li&gt;Morality is an abstract and complex concept composed of common principles such as fairness, obligation, and permissibility.&lt;/li&gt;
&lt;li&gt;One recent approach to moral learning combines utility calculus and Bayesian inference to distinguish and evaluate different principles&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;measuring-the-limits-of-intelligence-system-iq-tests&#34;&gt;Measuring the Limits of Intelligence System: IQ tests&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;rather, “analogous” emphasizes commonality on a more abstract level&lt;/li&gt;
&lt;li&gt;To make a successful analogy, the key is to understand causes and their effects&lt;/li&gt;
&lt;li&gt;One stream is the psychometric tradition of four-term or “proportional” analogies, the earliest discussions of which can be traced back to Aristotle [465]. An example in AI is the word2vec model [466, 467], which is capable of making a four-term word analogy; In the image domain, a similar test was invented by John C. Raven [373]—the raven’s prograssive matrices test (RPM).&lt;/li&gt;
&lt;li&gt;RPM lies directly at the center: it is diagnostic of abstract and structural reasoning ability [470], and captures the defining feature of high-level cognition—that is, fluid intelligence&lt;/li&gt;
&lt;li&gt;The RAVEN dataset [374] was created to push the limit of current vision systems’ reasoning and analogy-making ability, and to promote further research in this area. The dataset is designed to focus on reasoning and analogizing instead of only visual recognition. It is unique in the sense that it builds a semantic link between the visual reasoning and structural reasoning in RPM by grounding each problem into a sentence derived from an attributed stochastic image grammar attributed stochastic  image grammar (A-SIG): each instance is a sentence sampled from a predefined A-SIG, and a rendering engine transforms the sentence into its corresponding image.&lt;/li&gt;
&lt;li&gt;Smith and Gentner [485] summarized that comparing cases facilitates transfer learning and problem-solving, as well as the ability to learn relational categories. In his structure-mapping theory, Gentner [486] postulated that learners generate a structural alignment between two representations when they compare two cases.&lt;/li&gt;
&lt;li&gt;Parallel to work on RPM, work on number sense [490] bridges the induction of symbolic concepts and the competence of problem-solving;&lt;/li&gt;
&lt;li&gt;A recent work approaches the analogy problem from this perspective of strong mathematical reasoning&lt;/li&gt;
&lt;li&gt;relational reasoning, where the machine is given two figures of numbers following hidden arithmetic computations and is tasked to work out a missing entry in the final answer&lt;/li&gt;
&lt;li&gt;This work also sheds some light on how machine reasoning could be improved: the fusing of classic search-based algorithms with modern neural networks in order to discover essential number concepts in future research would be an encouraging development.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Building machines that learn and think like people</title>
      <link>https://example.com/post/build-humanlike-machine/</link>
      <pubDate>Mon, 20 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/build-humanlike-machine/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;h2 id=&#34;two-different-computational-approaches-to-intelligence&#34;&gt;two different computational approaches to intelligence&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;pattern recognition approach treats prediction as primary, usually in the context of a specific classification, regression, or control task.  discovering features that have high-value states in common across a large, diverse set of training data.&lt;/li&gt;
&lt;li&gt;model building. Cognition is about using these models to understand the world, to explain, to imagine what could have happened that didn’t, or what could be true that isn’t, and then planning.&lt;/li&gt;
&lt;li&gt;prediction and explanation, is central to our view of human intelligence.  pattern recognition can support model building, through “model-free” algorithms that learn through experience how to make essential inferences more computationally efficient&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-this-article-is-not&#34;&gt;What this article is not&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;we believe that reverse engineering human intelligence  can usefully inform AI and machine learning.&lt;/li&gt;
&lt;li&gt;avoiding cognitive or neural inspiration as well as claims of cognitive or neural plausibility  is a  approach to developing AI. But this article has little pertinence to this approach.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview-of-the-key-ideas&#34;&gt;Overview of the key ideas&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;propose a set of core ingredients for building more human-like learning and thinking machines&lt;/li&gt;
&lt;li&gt;mainly in Section 4 Core ingredients of human intelligence&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;cognitive-and-neural-inspiration-in-artificial-intelligence&#34;&gt;Cognitive and neural inspiration in artificial intelligence&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;behaviorist view&lt;/li&gt;
&lt;li&gt;Cognitive science&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;pdp-approach&#34;&gt;PDP approach&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;knowledge is thus distributed across the collection of units rather than localized as in most symbolic data structures.&lt;/li&gt;
&lt;li&gt;Neural network models and the PDP approach offer a view of the mind (and intelligence more broadly) that is sub-symbolic and often populated with minimal constraints and inductive biases to guide learning.&lt;/li&gt;
&lt;li&gt;Proponents of this approach maintain that many classic types of structured knowledge, such as graphs, grammars can be useful yet misleading metaphors for characterizing thought.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;question&#34;&gt;Question&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;neural networks have such broad application in machine vision, language, and control, and they can be trained to emulate the rule-like and structured behaviors that characterize cognition&lt;/li&gt;
&lt;li&gt;do we need more to develop truly human-like learning and thinking machines?&lt;/li&gt;
&lt;li&gt;How far can relatively generic neural networks bring us toward this goal?&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;challenges-for-building-more-human-like-machines&#34;&gt;Challenges for building more human-like machines&lt;/h1&gt;
&lt;h2 id=&#34;the-characters-challenge&#34;&gt;The Characters Challenge&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;learning simple visual concepts&lt;/li&gt;
&lt;li&gt;People learn a lot more from a lot less, and capturing these human-level learning abilities in machines is the Characters Challenge&lt;/li&gt;
&lt;li&gt;Although humans and neural networks may perform equally well on the MNIST digit recognition task and other large-scale image classification tasks, it does not mean that they learn and think in the same way.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;two-important-differences-between-cnn-and-human-in-learning-simple-visual-concepts&#34;&gt;two important differences between CNN and human in learning simple visual concepts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;people learn from fewer examples and they learn richer representations&lt;/li&gt;
&lt;li&gt;people learn more than how to do pattern recognition: they learn a concept, that is, a model of the class that allows their acquired knowledge to be flexibly applied in new ways.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-difficulty&#34;&gt;Some difficulty&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A single example of a new visual concept (red box) can be enough information to support the&lt;/li&gt;
&lt;li&gt;classification of new examples&lt;/li&gt;
&lt;li&gt;generation of new examples&lt;/li&gt;
&lt;li&gt;parsing an object into parts and relations&lt;/li&gt;
&lt;li&gt;generation of new concepts from related concepts.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-frostbite-challenge&#34;&gt;The Frostbite Challenge&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;learning to play the Atari game Frostbite&lt;/li&gt;
&lt;li&gt;Failed on accomplishing a sub-goal (such as reaching an ice floe) and then safely proceed to the next sub-goal&lt;/li&gt;
&lt;li&gt;the policy are highly specialized for the games it was trained on&lt;/li&gt;
&lt;li&gt;considering the amount of experience required for learning&lt;/li&gt;
&lt;li&gt;non-professional humans can grasp the basics of the game after just a few minutes of play.&lt;/li&gt;
&lt;li&gt;people do this by inferring a general schema to describe the goals of the game and the object types and their interactions, using the kinds of intuitive theories, model-building abilities and model-based planning mechanisms we describe below.&lt;/li&gt;
&lt;li&gt;the game of Frostbite provides incremental rewards for reaching each active ice floe, providing the DQN with the relevant sub-goals for completing the larger task of building an igloo.&lt;/li&gt;
&lt;li&gt;Without these sub-goals, the DQN would have to take random actions until it accidentally builds an igloo and is rewarded for completing the entire level.&lt;/li&gt;
&lt;li&gt;Human is possible to figure out the higher-level goal of building an igloo without incremental feedback;&lt;/li&gt;
&lt;li&gt;sparse feedback is a source of difficulty in other Atari 2600 games such as Montezuma’s Revenge&lt;/li&gt;
&lt;li&gt;inflexible to changes in its inputs and goals. Changing the color or appearance of objects or changing the goals of the network would have devastating consequences on performance if the network is not retrained&lt;/li&gt;
&lt;li&gt;In contrast, people require little or no retraining or reconfiguration, adding new tasks and goals to their repertoire with relative ease.&lt;/li&gt;
&lt;li&gt;Humans as a result often have important domain-specific knowledge for these tasks, even before they ‘begin.’ The DQN is starting completely from scratch.&lt;/li&gt;
&lt;li&gt;How do we bring to bear rich prior knowledge to learn new tasks and solve new problems so quickly?&lt;/li&gt;
&lt;li&gt;What form does that prior knowledge take, and how is it constructed, from some combination of inbuilt capacities and previous experience?&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;core-ingredients-of-human-intelligence&#34;&gt;Core ingredients of human intelligence&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Whether learned, built in, or enriched, the key claim is that these ingredients play an active and important role in producing human-like learning and thought, in ways contemporary machine learning has yet to capture.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;developmental-start-up-software&#34;&gt;Developmental start-up software&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Early in development, humans have a foundational understanding of several core domains including number (numerical and set operations), space (geometry and navigation), physics (inanimate objects and mechanics), and psychology (agents and groups).&lt;/li&gt;
&lt;li&gt;The underlying cognitive representations can be understood as “intuitive theories,” with a causal structure resembling a scientific theory&lt;/li&gt;
&lt;li&gt;focus on the early understanding of objects and agents.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;intuitive-physics&#34;&gt;Intuitive physics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;At the age of 2 months, and possibly earlier, human infants expect inanimate objects to follow principles of persistence, continuity, cohesion, and solidity.&lt;/li&gt;
&lt;li&gt;At around 6 months, infants have already developed different expectations for rigid bodies, soft bodies, and liquids&lt;/li&gt;
&lt;li&gt;By their first birthday, infants have gone through several transitions of comprehending basic physical concepts such as inertia, support, containment, and collisions&lt;/li&gt;
&lt;li&gt;There is no single agreed-upon computational account of these early physical principles and concepts, and previous suggestions have ranged from decision trees, to cues, to lists of rules&lt;/li&gt;
&lt;li&gt;A promising recent approach sees intuitive physical reasoning as similar to inference over a physics software engine&lt;/li&gt;
&lt;li&gt;This “intuitive physics engine” approach enables flexible adaptation to a wide range of everyday scenarios and judgments in a way that goes beyond perceptual cues.&lt;/li&gt;
&lt;li&gt;What are the prospects for embedding or acquiring this kind of intuitive physics in deep learning systems?&lt;/li&gt;
&lt;li&gt;Connectionist models in psychology have previously been applied to physical reasoning tasks such as balance-beam rules (McClelland 1988; Shultz 2003) or rules relating to distance, velocity, and time in motion&lt;/li&gt;
&lt;li&gt;PhysNet In contrast, people require far less experience to perform any particular task, and can generalize to many novel judgments and complex scenes with no new training required (although they receive large amounts of physics experience through interacting with the world more generally).&lt;/li&gt;
&lt;li&gt;Could deep learning systems such as PhysNet capture this flexibility, without explicitly simulating the causal interactions between objects in three dimensions?&lt;/li&gt;
&lt;li&gt;Whether such models can be learned with the kind (and quantity) of data available to human infants is not clear&lt;/li&gt;
&lt;li&gt;But incorporating a physics-engine–based representation could help DQNs learn to play games such as Frostbite in a faster and more general way, whether the physics knowledge is captured implicitly in a neural network or more explicitly in a simulator.&lt;/li&gt;
&lt;li&gt;When a new object type such as a bear is introduced, as in the later levels of Frostbite (Fig. 2D), a network endowed with intuitive physics would also have an easier time adding this object type to its knowledge&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;challenges&#34;&gt;Challenges&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;For networks trained on object classification,deeper layers often become sensitive to successively higher-level features, from edges to textures to shapeparts to full objects.&lt;/li&gt;
&lt;li&gt;For deep networks trained on physics-related data,it remains to be seen whether higher layers will encode objects, general physical properties, forces, and approximately Newtonian dynamics.&lt;/li&gt;
&lt;li&gt;would it generalize broadly beyond training contexts as people’s more explicit physical concepts do?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;intuitive-psychology&#34;&gt;Intuitive psychology&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Pre-verbal infants distinguish animate agents from inanimate objects. This distinction is partially based on innate or early-present detectors for low-level cues, such as the presence of eyes, motion initiated from rest, and biological motion&lt;/li&gt;
&lt;li&gt;Beyond these low-level cues, infants also expect agents to act contingently and reciprocally, to have goals, and to take efficient actions toward those goals subject to constraints&lt;/li&gt;
&lt;li&gt;It is generally agreed that infants expect agents to act in a goal-directed, efficient, and socially sensitive fashion&lt;/li&gt;
&lt;li&gt;One possibility is that intuitive psychology is simply cues “all the way down” This inference could be captured by a cue that states &amp;ldquo;If an agent’s expected trajectory is prevented from completion, the blocking agent is given some negative association.&lt;/li&gt;
&lt;li&gt;One alternative to a cue-based account is to use generative models of action choice, as in the Bayesian inverse planning, or Bayesian theory of mind (ToM), models of Baker et al. (2009) or the naive utility calculus models of Jara-Ettinger et al.&lt;/li&gt;
&lt;li&gt;These models formalize explicitly mentalistic concepts such as “goal,” “agent,”  “planning,” “cost,” “efficiency,” and “belief,” used to describe core psychological  reasoning in infancy.&lt;/li&gt;
&lt;li&gt;They assume adults and children treat agents as approximately rational planners who choose the most efficient means to their goals.&lt;/li&gt;
&lt;li&gt;By simulating these planning processes, people can predict what agents might do next, or use inverse reasoning from observing a series of actions to infer the utilities and beliefs of agents in a scene.&lt;/li&gt;
&lt;li&gt;Importantly, unlike in intuitive physics, simulation-based reasoning in intuitive psychology can be nested recursively to understand social interactions. We can think about agents thinking about other agents.&lt;/li&gt;
&lt;li&gt;Although deep networks have not yet been applied to scenarios involving theory of mind and intuitive psychology, they could probably learn visual cues, heuristics and summary statistics of a scene that happens to involve agents.&lt;/li&gt;
&lt;li&gt;However, it seems to us that any full formal account of intuitive psychological reasoning needs to include representations of agency, goals, efficiency, and reciprocal relations.&lt;/li&gt;
&lt;li&gt;Connectionists have argued that innate constraints in the form of hard-wired cortical circuits are unlikely , but a simple inductive bias, for example, the tendency to notice things that move other things, can bootstrap reasoning about more abstract concepts of agency&lt;/li&gt;
&lt;li&gt;Similarly, a great deal of goal-directed and socially directed actions can also be boiled down to a simple utility calculus, in a way that could be shared with other cognitive abilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;learning-as-rapid-model-building&#34;&gt;Learning as rapid model building&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;There are many learning algorithms for neural networks, including the perceptron algorithm (Rosenblatt 1958), Hebbian learning (Hebb 1949), the BCM rule (Bienenstock et al. 1982), backpropagation (Rumelhart et al. 1986a), the wake-sleep algorithm (Hinton et al. 1995), and contrastive divergence (Hinton 2002).&lt;/li&gt;
&lt;li&gt;Human “one-shot” learning&lt;/li&gt;
&lt;li&gt;Neural network sdata hungry&lt;/li&gt;
&lt;li&gt;the types of things that children learn as the meanings of words – people are still far better learners than machines.&lt;/li&gt;
&lt;li&gt;Even with just a few examples, people can learn remarkably rich conceptual models.&lt;/li&gt;
&lt;li&gt;Beyond classification, concepts support prediction, action, communication, imagination , explanation, and composition.&lt;/li&gt;
&lt;li&gt;In addition to evaluating several types of deep learning models, we developed an algorithm using Bayesian program learning (BPL) that represents concepts as simple stochastic programs: structured procedures that generate new examples of a concept when executed&lt;/li&gt;
&lt;li&gt;These programs allow the model to express causal knowledge about how the raw data are formed, and the probabilistic semantics allow the model to handle noise and perform creative tasks. Structure sharing across concepts is accomplished by the compositional re-use of stochastic primitives that can combine in new ways to create new concepts.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;compositionality&#34;&gt;Compositionality&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Compositionality is the classic idea that new representations can be constructed through the combination of primitive elements.&lt;/li&gt;
&lt;li&gt;Compositionality is also at the core of productivity: an infinite number of representations can be constructed from a finite set of primitives&lt;/li&gt;
&lt;li&gt;To capture the full extent of the mind’s compositionality, a model must include explicit representations of objects, identity, and relations, all while maintaining a notion of “coherence” when understanding novel configurations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;causality&#34;&gt;Causality&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;To capture the full extent of the mind’s compositionality, a model must include explicit representations of objects, identity, and relations, all while maintaining a notion of “coherence” when understanding novel configurations.&lt;/li&gt;
&lt;li&gt;Concept learning and vision models that use causality are usually generative but not every generative model is also causal.&lt;/li&gt;
&lt;li&gt;Causality has been influential in theories of perception. “Analysis-by-synthesis” theories of perception maintain that sensory data can be more richly represented by modeling the process that generated it&lt;/li&gt;
&lt;li&gt;Causal knowledge has also been shown to influence how people learn new concepts; providing a learner with different types of causal knowledge changes how he or she learns and generalizes.&lt;/li&gt;
&lt;li&gt;To explain the role of causality in learning, conceptual representations have been likened to intuitive theories or explanations, providing the glue that lets core features stick, whereas other equally applicable features wash away.&lt;/li&gt;
&lt;li&gt;Beyond concept learning, people also understand scenes by building causal models.&lt;/li&gt;
&lt;li&gt;There have been steps toward deep neural networks and related approaches that learn causal models.&lt;/li&gt;
&lt;li&gt;Incorporating causality may greatly improve these deep learning models; they were trained without access to causal data about how characters are actually produced, and without any incentive to learn the true causal process.&lt;/li&gt;
&lt;li&gt;A causal model of Frostbite would have to be more complex, gluing together object representations and explaining their interactions with intuitive physics and intu-
itive psychology, much like the game engine that generates the game dynamics and, ultimately, the frames of pixel images.&lt;/li&gt;
&lt;li&gt;Inference is the process of inverting this causal generative model, explaining the raw pixels as objects and their interactions, such as the agent stepping on an ice floe to deactivate it or a crab pushing the agent into the water.&lt;/li&gt;
&lt;li&gt;Deep neural networks could play a role in two ways: by serving as a bottom-up proposer to make probabilistic inference more tractable in a structured generative model or by serving as the causal generative model if imbued with the right set of ingredients.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;learning-to-learn&#34;&gt;Learning-to-learn&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;When humans or machines make inferences that go far beyond the data, strong prior knowledge (or inductive biases or constraints) must be making up the difference.&lt;/li&gt;
&lt;li&gt;One way people acquire this prior knowledge is through “learning-to-learn,” a term introduced by Harlow (1949) and closely related to the machine learning notions of “transfer learning,” “multitask learning,” and “representation learning.”&lt;/li&gt;
&lt;li&gt;The strong priors, constraints, or inductive bias needed to learn a particular task quickly are often shared to some extent with other related tasks.&lt;/li&gt;
&lt;li&gt;In hierarchical Bayesian modeling, a general prior on concepts is shared by multiple specific concepts, and the prior itself is learned over the course of learning the specific concepts&lt;/li&gt;
&lt;li&gt;In machine vision, for deep convolutional networks or other discriminative methods that form the core of recent recognition systems, learning-to-learn can occur through the sharing of features between the models learned for old objects or old tasks and the models learned for new objects or new tasks&lt;/li&gt;
&lt;li&gt;We cannot be sure how people get to the knowledge they have in this domain, but we do understand how this works in BPL, and we think people might be similar.&lt;/li&gt;
&lt;li&gt;BPL transfers readily to new concepts because it learns about object parts, sub-parts, and relations, capturing learning about what each concept is like and what concepts are like in general.  It is crucial that learning-to-learn occurs at multiple levels of the hierarchical generative process.&lt;/li&gt;
&lt;li&gt;Further transfer occurs by learning about the typical levels of variability within a typical generative model. This provides knowledge about how far and in what ways to generalize when we have seen only one example of a new character, which on its own could not possibly carry any information about variance.&lt;/li&gt;
&lt;li&gt;In the Frostbite Challenge, and in video games more generally, there is a similar interdependence between the form of the representation and the effectiveness of learning-to-learn.&lt;/li&gt;
&lt;li&gt;general world knowledge and previous video games may help inform exploration and generalization in new scenarios, helping people learn maximally from a single mistake or avoid mistakes altogether&lt;/li&gt;
&lt;li&gt;Deep reinforcement learning systems for playing Atari games have had some impressive successes in transfer learning, but they still have not come close to learning to play new games as quickly as humans can. For example, Parisotto et al. (2016) present the “actor-mimic” algorithm&lt;/li&gt;
&lt;li&gt;In sum, the interaction between representation and previous experience may be key to building machines that learn as fast as people.&lt;/li&gt;
&lt;li&gt;if such a system aims to learn compositionally structured causal models of each game – built on a foundation of intuitive physics and psychology – it could transfer knowledge more efficiently and thereby learn new games much more quickly.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;thinking-fast&#34;&gt;Thinking Fast&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In general, richer and more structured models require more complex and slower inference algorithms, similar to how complex models require more data, making the speed of perception and thought all the more remarkable.&lt;/li&gt;
&lt;li&gt;The combination of rich models with efficient inference suggests another way psychology and neuroscience may usefully inform AI.&lt;/li&gt;
&lt;li&gt;This section discusses possible paths toward resolving the conflict between fast inference and structured representations, including Helmholtz machine–style approximate inference in generative models and cooperation between model-free and model-based reinforcement learning systems&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;approximate-inference-in-structured-models&#34;&gt;Approximate inference in structured models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In contrast, whereas representing intuitive theories and structured causal models is less natural in deep neural networks, recent progress has demonstrated the remarkable effectiveness of gradient-based learning in high-dimensional parameter spaces.&lt;/li&gt;
&lt;li&gt;A complete account of learning and inference must explain how the brain does so much with limited computational resources&lt;/li&gt;
&lt;li&gt;Popular algorithms for approximate inference in probabilistic machine learning have been proposed as psychological models&lt;/li&gt;
&lt;li&gt;Most prominently, it has been proposed that humans can approximate Bayesian inference using Monte Carlo methods, which stochastically sample the space of possible hypotheses and evaluate these samples according to their consistency with the data and prior knowledge&lt;/li&gt;
&lt;li&gt;Although Monte Carlo methods are powerful and come with asymptotic guarantees, it is challenging to make them work on complex problems like program induction and theory learning.&lt;/li&gt;
&lt;li&gt;Discovering new theories can be slow and arduous, as testified by the long time scale of cognitive development, and learning in a saltatory fashion (rather than through gradual adaptation) is characteristic of aspects of human intelligence, including discovery and insight during development (Schulz 2012b), problem-solving (Sternberg &amp;amp; Davidson 1995), and epoch-making discoveries in scientific research (Langley et al. 1987).&lt;/li&gt;
&lt;li&gt;Dis-covering new theories can also occur much more quickly. A person learning the rules of Frostbite will probably undergo a loosely ordered sequence of “Aha!” moments:&lt;/li&gt;
&lt;li&gt;These little fragments of a “Frostbite theory” are assembled to form a causal understanding of the game relatively quickly, in what seems more like a guided process than arbitrary proposals in a Monte Carlo inference scheme&lt;/li&gt;
&lt;li&gt;For domains where program or theory learning occurs quickly, it is possible that people employ inductive biases not only to evaluate hypotheses, but also to guide hypothesis selection.&lt;/li&gt;
&lt;li&gt;How might efficient mappings from questions to a plausible subset of answers be learned?  spanning both deep learning and graphical models, has attempted to tackle this challenge by “amortizing” probabilistic inference computations into an efficient feed-forward mapping&lt;/li&gt;
&lt;li&gt;These feed-forward mappings can be learned in various ways, for example, using paired generative/recognition networks and variational optimization, or nearest-neighbor density estimation&lt;/li&gt;
&lt;li&gt;One implication of amortization is that solutions to different problems will become correlated because of the sharing of amortized computations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model-based-and-model-free-reinforcement-learning&#34;&gt;Model-based and model-free reinforcement learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;There is indeed substantial evidence that the brain uses similar model-free learning algorithms in simple associative learning or discrimination learning tasks&lt;/li&gt;
&lt;li&gt;Considerable evidence suggests that the brain also has a model-based learning system, responsible for building a “cognitive map” of the environment and using it to plan action sequences for more complex tasks Model-based planning is an essential ingredient of human intelligence, enabling flexible adaptation to new tasks and goals;&lt;/li&gt;
&lt;li&gt;One boundary condition on this flexibility is the fact that the skills become “habitized” with routine application,  possibly reflecting a shift from model-based to model-free control. This shift may arise from a rational arbitration between learning systems to balance the trade-off between flexibility and speed&lt;/li&gt;
&lt;li&gt;Similarly to how probabilistic computations can be amortized for efficiency (see previous section), plans can be amortized into cached values by allowing the model-based system to simulate training data for the model-free system&lt;/li&gt;
&lt;li&gt;Intrinsic motivation also plays an important role in human learning and behavior&lt;/li&gt;
&lt;li&gt;all externally provided rewards are reinterpreted according to the “internal value” of the agent,&lt;/li&gt;
&lt;li&gt;There may also be an intrinsic drive to reduce uncertainty and construct models of the 8environment&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;responses-to-common-questionswhat-we-believe&#34;&gt;Responses to common questions(What we believe)&lt;/h1&gt;
&lt;h2 id=&#34;comparing-the-learning-speeds-of-humans-and-neural-networks-on-specific-tasks-is-not-meaningful-because-humans-have-extensive-prior-experience&#34;&gt;Comparing the learning speeds of humans and neural networks on specific tasks is not meaningful, because humans have extensive prior experience&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;If deep learning researchers see themselves as trying to capture the equivalent of humans’ collective evolutionary experience, this would be equivalent to a truly immense “pre-training” phase.&lt;/li&gt;
&lt;li&gt;We are less committed to a particular story regarding the origins of the ingredients,&lt;/li&gt;
&lt;li&gt;successful learning-to-learn – or, at least, human-level transfer learning – is enabled by having models with the right representational structure, including the other building blocks discussed in this article.&lt;/li&gt;
&lt;li&gt;To build these representations from scratch might require exploring fundamental structural variations in the network’s architecture, which gradient-based learning in weight space is not prepared to do. Although deep learning researchers do explore many such architectural variations&lt;/li&gt;
&lt;li&gt;dynamics of structure search may look much more like the slow random hill climbing of evolution than the smooth, methodical progress of stochastic gradient descent.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;biological-plausibility-suggests-theories-of-intelligence-should-start-with-neural-networks&#34;&gt;Biological plausibility suggests theories of intelligence should start with neural networks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We have focused on how cognitive science can motivate and guide efforts to engineer human-like AI, in contrast to some advocates of deep neural networks who cite neuro-science for inspiration.&lt;/li&gt;
&lt;li&gt;Unfortunately, what we “know” about the brain is not all that clear-cut.&lt;/li&gt;
&lt;li&gt;Hebbian learning is another case in point. In the form of long-term potentiation (LTP) and spike-timing dependent plasticity (STDP), Hebbian learning mechanisms are often cited as biologically supported.&lt;/li&gt;
&lt;li&gt;Most relevantly for our focus, it would be especially challenging to try to implement the ingredients described in this article using purely Hebbian mechanisms.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;language-is-essential-for-human-intelligence-why-is-it-not-more-prominent-here&#34;&gt;Language is essential for human intelligence. Why is it not more prominent here?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We have said little in this article about people’s ability to communicate and think in natural language, a distinctively human cognitive capacity where machine capabilities strikingly lag.&lt;/li&gt;
&lt;li&gt;We believe that understanding language and its role in intelligence goes hand-in-hand with understanding the building blocks discussed in this article.&lt;/li&gt;
&lt;li&gt;These capacities are in place before children master language, and they provide the building blocks for linguistic meaning and language acquisition&lt;/li&gt;
&lt;li&gt;Is it recursion, or some new kind of recursive structure uilding ability? Is it the ability to re-use symbols by name ? Is it the ability to understand others intentionally and build shared intentionality ? Is it some new version of these things, or is it just more of the aspects of these capacities that are already present in infants?&lt;/li&gt;
&lt;li&gt;But with language, older children become able to reason about a much wider range of physical and psychological situations . Language also facilitates more powerful learning-to-learn and compositionality.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;looking-forward&#34;&gt;Looking forward&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Machine performance may rival or exceed human performance on particular tasks, and algorithms may take inspiration from neuroscience or aspects of psychology, but it does not follow that the algorithm learns or thinks like a person. This is a higher bar worth reaching for, potentially leading to more powerful algorithms, while also helping unlock the mysteries of the human mind.&lt;/li&gt;
&lt;li&gt;When comparing people with the current best algorithms in AI and machine learning, people learn from fewer data and generalize in richer and more flexible ways.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;promising-directions-in-deep-learning&#34;&gt;Promising directions in deep learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;There has been recent interest in integrating psychological ingredients with deep neural networks, especially selective   attention, augmented working memory, and experience replay.&lt;/li&gt;
&lt;li&gt;Paralleling the human perceptual apparatus, selective  attention forces deep learning models to process raw, perceptual data as a series of high-resolution “foveal glimpses” rather than all at once.&lt;/li&gt;
&lt;li&gt;Attention may help these models in several ways. It helps to coordinate complex, often sequential, outputs by attending to only specific aspects of the input, allowing the model to focus on smaller sub-tasks rather than solving an entire problem in one shot.&lt;/li&gt;
&lt;li&gt;Attention also allows larger models to be trained without requiring every model parameter to affect every output or action.&lt;/li&gt;
&lt;li&gt;Researchers are also developing neural networks with “working memories” that augment the shorter-term memory provided by unit activation and the longer-term memory provided by the connection weights&lt;/li&gt;
&lt;li&gt;Each model seems to learn genuine programs from examples, albeit in a representation more like assembly language than a high-level programming language.&lt;/li&gt;
&lt;li&gt;differentiable programming suggests the intriguing possibility of combining the best of program induction and deep learning.&lt;/li&gt;
&lt;li&gt;Another example of combining pattern recognition and model-based search comes from recent AI research into the game Go.&lt;/li&gt;
&lt;li&gt;One worthy goal would be to build an AI system that beats a world-class player with the amount and kind of training human champions receive, rather than overpowering them with Google-scale computational resources.&lt;/li&gt;
&lt;li&gt;Although techniques for handling variable-sized inputs in ConvNets may help in playing on different board sizes, the value functions and policies that AlphaGo learns seem unlikely to generalize as flexibly and automatically as people.&lt;/li&gt;
&lt;li&gt;the fact that it cannot even conceive of these variants, let alone adapt to them autonomously, is a sign that it does not understand the game as humans do.&lt;/li&gt;
&lt;li&gt;Humans represent their strategies as a response to these constraints, such that if the game changes, they can begin to adjust their strategies accordingly.&lt;/li&gt;
&lt;li&gt;We believe it would be richly rewarding for AI and cognitive science to pursue this challenge together and that such systems could be a compelling testbed for the principles this article suggests, as well as building on all of the progress to date that AlphaGo represents.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;future-applications-to-practical-ai-problems&#34;&gt;Future applications to practical AI problems&lt;/h2&gt;
&lt;h3 id=&#34;scene-understanding&#34;&gt;Scene understanding&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Deep learning is moving beyond object recognition and toward scene understanding, as evidenced by a flurry of recent work focused on generating natural language captions for images&lt;/li&gt;
&lt;li&gt;Yet current algorithms are still better at recognizing objects than understanding scenes, often getting the key objects right but their causal relationships wrong&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;autonomous-agents-and-intelligent-devices&#34;&gt;Autonomous agents and intelligent devices&lt;/h3&gt;
&lt;h3 id=&#34;autonomous-driving&#34;&gt;Autonomous driving&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Similarly, other drivers on the road have similarly complex mental states underlying their behavior&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;creative-design&#34;&gt;Creative design&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Although we are still far from developing AI systems that can tackle these types of tasks, we see compositionality and causality as central to this goal.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;toward-more-human-like-learning-and-thinking-machines&#34;&gt;Toward more human-like learning and thinking machines&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;we suggest that deep learning and other computational paradigms should aim to tackle these tasks using as few training data as people need, and also to evaluate models on a range of human-like generalizations beyond the one task on which the model was trained.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;note&#34;&gt;Note&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;The Atari games are deterministic, raising the possibility that a learner can succeed by memorizing long sequences of actions without learning to generalize&lt;/li&gt;
&lt;li&gt;Although it is unclear if the DQN also memorizes action sequences, an alternative “human starts” metric provides a stronger test of generalization&lt;/li&gt;
&lt;li&gt;Although connectionist networks have been used to model the general transition that children undergo between the ages of 3 and 4 regarding false belief,&lt;/li&gt;
&lt;li&gt;A new approach using convolutional “matching networks” achieves good one-shot classification performance when discriminating between characters from different alphabets&lt;/li&gt;
&lt;li&gt;In the interest of brevity, we do not discuss here another important vein of work linking neural circuits to variational approximations (Bastos et al. 2012), which have received less attention in the psychological literature.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;the-three-major-paradigms-of-ai-symbolicism-connectionism-behaviorism&#34;&gt;The three major paradigms of AI: symbolicism, connectionism, behaviorism&lt;/h1&gt;
&lt;h2 id=&#34;symbolicism&#34;&gt;Symbolicism&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;symbolicism=Logicism=Psychlogism=Computerism&lt;/li&gt;
&lt;li&gt;The main principles are the hypothesis of a physical symbol system (i.e., a system that manipulates symbols) and the principle of limited rationality.&lt;/li&gt;
&lt;li&gt;This category included most of the pioneers of artificial intelligence research.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;main-points&#34;&gt;Main points&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Symbolism is the foundation of human cognitive and mental activities&lt;/li&gt;
&lt;li&gt;Computers operate as physical systems that manipulate symbols&lt;/li&gt;
&lt;li&gt;Cognition involves performing computations on symbolic representations&lt;/li&gt;
&lt;li&gt;Computers can emulate or approximate human cognitive functions.&lt;/li&gt;
&lt;li&gt;In 1957, Newell, Simon and their colleagues created a program named &amp;ldquo;Logic Theorist&amp;rdquo; that could prove mathematical theorems. It verified 38 out of the first 52 propositions in Whitehead and Russell&amp;rsquo;s &amp;ldquo;Principia Mathematica&amp;rdquo;, and subsequently confirmed some more.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;connectionism&#34;&gt;Connectionism&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Connectionism is an approach in cognitive science that hopes to explain mental phenomena with artificial neural networks (ANNs).&lt;/li&gt;
&lt;li&gt;The central principle of connectionism is to use simple and often consistent units interconnected networks, to describe psychological phenomena. Different models of connections and unit forms may vary. For example, the units and connections of the network can represent neurons and synapses, as in the human brain.&lt;/li&gt;
&lt;li&gt;Lack of neuroscientific rationality.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;parallel-distributed-processing-pdp&#34;&gt;Parallel distributed processing, PDP&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;It is a method of artificial neural networks that emphasizes the parallelism of neural processing and the distributedness of neural representations, providing researchers with a general mathematical framework. It mainly includes eight aspects:&lt;/li&gt;
&lt;li&gt;A set of processing units, represented by a set of integers.&lt;/li&gt;
&lt;li&gt;The activation of the units, represented by a vector of time-dependent functions.&lt;/li&gt;
&lt;li&gt;The output function of the units, represented by a vector of activation functions.&lt;/li&gt;
&lt;li&gt;The connectivity pattern between units, represented by a real matrix indicating connection strengths.&lt;/li&gt;
&lt;li&gt;The propagation rule for propagating activation through connections, expressed as a function on unit outputs.&lt;/li&gt;
&lt;li&gt;The activation rule for combining inputs sent to units to determine new activations for units, represented by current activations and propagation functions.&lt;/li&gt;
&lt;li&gt;The learning rule for modifying connections based on experience, expressed as weight changes based on any number of variables.&lt;/li&gt;
&lt;li&gt;The environment that provides experience for the system, represented by a set of activation vectors for some subsets of units.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;experimentalism&#34;&gt;Experimentalism&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We can learn almost everything we know from the statistical patterns of sensory input.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;actionism&#34;&gt;Actionism&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Actionism (Behaviorism), also known as evolutionary or cybernetic school, is based on cybernetics and perception-action control systems.&lt;/li&gt;
&lt;li&gt;It argues that artificial intelligence originates from cybernetics.&lt;/li&gt;
&lt;li&gt;Cybernetic ideas became an important part of the zeitgeist in the 1940s and 1950s, influencing early artificial intelligence researchers. The cybernetics and self-organizing systems proposed by Wiener, McCulloch and others, as well as the engineering cybernetics and biological cybernetics proposed by Qian Xuesen and others, affected many fields. Cybernetics linked the working principles of neural systems with information theory, control theory, logic and computer science.&lt;/li&gt;
&lt;li&gt;And it assumes that all behaviors are produced by stimuli from the environment or shaped by individual life history; especially individual punishment, incentives, stimuli and behavioral outcomes caused by reinforcement in environment and life history. Therefore, although behaviorists generally accept that genetic factors are important determinants of behavior, they still pay more attention to environmental influences.&lt;/li&gt;
&lt;li&gt;The early research work focused on simulating human intelligent behavior and role in control processes, such as research on cybernetic systems such as self-optimization, self-adaptation, self-stabilization, self-organization and self-learning , And carried out research on &amp;ldquo;cybernetic animals&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;By the 1960s and 1970s , some progress had been made in these studies of cybernetic systems , sowing seeds for intelligent control and intelligent robots , which gave birth to intelligent control systems . Intelligent robot system .&lt;/li&gt;
&lt;li&gt;Behaviorism did not appear until the end of the 20th century as a new school of artificial intelligence , attracting many people&amp;rsquo;s interest . The representative author of this school was Brooks&amp;rsquo;s six-legged walking robot , which was regarded as a new generation of &amp;ldquo;cybernetic animals&amp;rdquo; . It is a control system based on perception-action mode to simulate insect behavior .&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
