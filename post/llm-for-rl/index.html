<!DOCTYPE html>
<!-- This site was created with Wowchemy. https://www.wowchemy.com -->
<!-- Last Published: May 4, 2023 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.7.0 for Hugo" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css" integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.0f229d4b7ebad1917a9a357cba2effab.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  


























  
  
  






  <meta name="author" content="Haofei Hou" />





  

<meta name="description" content="Articles Summary about LLM/MMM for RL" />



<link rel="alternate" hreflang="en-us" href="https://example.com/post/llm-for-rl/" />
<link rel="canonical" href="https://example.com/post/llm-for-rl/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hu88cdd4b7a12988e7d5bb56be50e4f21a_10097_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hu88cdd4b7a12988e7d5bb56be50e4f21a_10097_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#1565c0" />










  
  






<meta property="twitter:card" content="summary" />
<meta property="twitter:image" content="https://example.com/media/icon_hu88cdd4b7a12988e7d5bb56be50e4f21a_10097_512x512_fill_lanczos_center_3.png" />
<meta property="og:site_name" content="Haofei Hou | HUST" />
<meta property="og:url" content="https://example.com/post/llm-for-rl/" />
<meta property="og:title" content="LLM/MMM for RL | Haofei Hou | HUST" />
<meta property="og:description" content="Articles Summary about LLM/MMM for RL" /><meta property="og:image" content="https://example.com/media/icon_hu88cdd4b7a12988e7d5bb56be50e4f21a_10097_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />

  
    <meta
      property="article:published_time"
      content="2023-04-10T00:00:00&#43;00:00"
    />
  
  
    <meta property="article:modified_time" content="2023-04-10T00:00:00&#43;00:00">
  






    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://example.com/post/llm-for-rl/"
  },
  "headline": "LLM/MMM for RL",
  
  "datePublished": "2023-04-10T00:00:00Z",
  "dateModified": "2023-04-10T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Haofei Hou"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Haofei Hou | HUST",
    "logo": {
      "@type": "ImageObject",
      "url": "https://example.com/media/icon_hu88cdd4b7a12988e7d5bb56be50e4f21a_10097_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "Articles Summary about LLM/MMM for RL"
}
</script>

  

  




  
  
  

  
  

  


  
  <title>LLM/MMM for RL | Haofei Hou | HUST</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="63c2f496d8a3605deef0eb2ba6524b45" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Haofei Hou | HUST</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Haofei Hou | HUST</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#featured"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#projects"><span>Projects</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#posts"><span>Posts</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article">

  













  

  
  
  
<div class="article-container pt-3">
  <h1>LLM/MMM for RL</h1>

  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      <a href="/authors/admin/">Haofei Hou</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Apr 10, 2023
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    6 min read
  </span>
  

  
  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/category/summary/">Summary</a></span>
  

</div>

    





  
</div>



  <div class="article-container">

    <div class="article-style">
      <ul>
<li><strong>Large Language Models</strong>: GPT BERT</li>
<li><strong>Multi-Modal Models</strong>: CLIP</li>
</ul>
<h1 id="reasoning">Reasoning</h1>
<ul>
<li>
<p><strong>RT-1</strong>: &ldquo;RT-1: Robotics Transformer for Real-World Control at Scale&rdquo;, <em>arXiv, Dec 2022</em>. [<a href="https://arxiv.org/abs/2212.06817" target="_blank" rel="noopener">Paper</a>]  [<a href="https://github.com/google-research/robotics_transformer" target="_blank" rel="noopener">GitHub</a>] [<a href="https://robotics-transformer.github.io/" target="_blank" rel="noopener">Website</a>]</p>
</li>
<li>
<p><strong>Code-As-Policies</strong>: &ldquo;Code as Policies: Language Model Programs for Embodied Control&rdquo;, <em>arXiv, Sept 2022</em>. [<a href="https://arxiv.org/abs/2209.07753" target="_blank" rel="noopener">Paper</a>]  [<a href="https://github.com/google-research/google-research/tree/master/code_as_policies" target="_blank" rel="noopener">Colab</a>] [<a href="https://code-as-policies.github.io/" target="_blank" rel="noopener">Website</a>]</p>
</li>
<li>
<p><strong>Say-Can</strong>: &ldquo;Do As I Can, Not As I Say: Grounding Language in Robotic Affordances&rdquo;, <em>arXiv, Apr 2021</em>. [<a href="https://arxiv.org/abs/2204.01691" target="_blank" rel="noopener">Paper</a>]  [<a href="https://say-can.github.io/#open-source" target="_blank" rel="noopener">Colab</a>] [<a href="https://say-can.github.io/" target="_blank" rel="noopener">Website</a>]</p>
</li>
<li>
<p><strong>Socratic</strong>: &ldquo;Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language&rdquo;, <em>arXiv, Apr 2021</em>. [<a href="https://arxiv.org/abs/2204.00598" target="_blank" rel="noopener">Paper</a>] [<a href="https://socraticmodels.github.io/#code" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://socraticmodels.github.io/" target="_blank" rel="noopener">Website</a>]</p>
</li>
<li>
<p><strong>PIGLeT</strong>: &ldquo;PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World&rdquo;, <em>ACL, Jun 2021</em>. [<a href="https://arxiv.org/abs/2201.07207" target="_blank" rel="noopener">Paper</a>] [<a href="http://github.com/rowanz/piglet" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://rowanzellers.com/piglet/" target="_blank" rel="noopener">Website</a>]</p>
</li>
</ul>
<hr>
<h1 id="planning">Planning</h1>
<ul>
<li>
<p><strong>LM-Nav</strong>: &ldquo;Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action&rdquo;, <em>arXiv, July 2022</em>. [<a href="https://arxiv.org/abs/2207.04429" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/blazejosinski/lm_nav" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://sites.google.com/view/lmnav" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230410013423928.png" alt="image-20230410013423928" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>LLM: GPT-3 parse textual instructions into a sequence of landmarks ($t$).</li>
<li>VLM: CLIP compute likelihood that image $i_k$ corresponds to the string $t$</li>
<li>VNM: ViNG  uses a goal-conditioned distance function to infer connectivity between the set of raw observations and constructs a topological graph</li>
<li>Navigation</li>
</ul>
</li>
<li>
<p><strong>InnerMonlogue</strong>: &ldquo;Inner Monologue: Embodied Reasoning through Planning with Language Models&rdquo;, <em>arXiv, July 2022</em>. [<a href="https://arxiv.org/abs/2207.05608" target="_blank" rel="noopener">Paper</a>] [<a href="https://innermonologue.github.io/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>system Inner Monologue chains together these various components (perception models, robotic skills, and human feedback) in a shared language prompt, enabling it to successfully perform user instructions.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230410093435285.png" alt="image-20230410093435285" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Inner Monologue(Left): InstructGPT</li>
<li>pick-and-place primitive: CLIPort</li>
<li>Manipulation</li>
</ul>
</li>
<li>
<p><strong>Housekeep</strong>: &ldquo;Housekeep: Tidying Virtual Households using Commonsense Reasoning&rdquo;, <em>arXiv, May 2022</em>. [<a href="https://arxiv.org/abs/2205.10712" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/yashkant/housekeep" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://yashkant.github.io/housekeep/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>New benchmark: In Housekeep, an embodied agent must tidy a house by rearranging misplaced objects without explicit instructions specifying which objects need to be rearranged</li>
<li>A new dataset for an object (e.g. salt-shaker), a room (e.g. kitchen) in context, to classify into the following categories: misplaced, correct, implausible.</li>
<li>Each Housekeep episode is created by instantiating 7-10 objects within a scene, out of which 3-5 objects are misplaced and the remaining are placed correct</li>
<li>Object Room $[OR] &ndash; P(room|object)$ : Generate compatibility scores for rooms for a given object.</li>
<li>Object Room Receptacle $[ORR] &ndash; P(receptacle|object, room)$: Generate compatibility scores for receptacles within a given room and for a given object</li>
<li>Method1: Finetuning by Contrastive Matching (CM). We train a 3-layered MLP on top of language embeddings and compute pairwise cosine similarity between any two embeddings</li>
<li>Method2: Zero-Shot Ranking via MLM (ZS-MLM).</li>
<li>LLM: BERT for word embeddings</li>
<li>MLM(Masked Language Model)</li>
<li>Similarity calculation</li>
</ul>
</li>
<li>
<p><strong>LID</strong>: &ldquo;Pre-Trained Language Models for Interactive Decision-Making&rdquo;, <em>arXiv, Feb 2022</em>. [<a href="https://arxiv.org/abs/2202.01771" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/ShuangLI59/Language-Model-Pre-training-Improves-Generalization-in-Policy-Learning" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://shuangli-project.github.io/Pre-Trained-Language-Models-for-Interactive-Decision-Making/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409121632758.png" alt="image-20230409121632758" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Fine-tuning</li>
<li>Mid-level actions</li>
<li>LLM</li>
<li>As a part of policy network.</li>
</ul>
</li>
<li>
<p><strong>ZSP</strong>: &ldquo;Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents&rdquo;, <em>ICML, Jan 2022</em>. [<a href="https://arxiv.org/abs/2201.07207" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/huangwl18/language-planner" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://wenlong.page/language-planner/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230410095827203.png" alt="image-20230410095827203" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>VirtualHome Executability Correctness</li>
<li>Planning LM (Causal LLM): GPT-3 $\hat{a}$</li>
<li>Translation LM(Masked LLM): BERT</li>
<li>for each admissible environment action $a_e$ $max\ C(f(\hat{a}), f(a_e)) = \frac{f(\hat{a})\cdot f(a_e)}{||f(\hat{a})||\ ||f(a_e)||}$</li>
<li>Mid-level actions</li>
</ul>
</li>
</ul>
<hr>
<h1 id="manipulation">Manipulation</h1>
<ul>
<li>
<p><strong>DIAL</strong>:&ldquo;Robotic Skill Acquistion via Instruction Augmentation with Vision-Language Models&rdquo;, &ldquo;arXiv, Nov 2022&rdquo;, [<a href="https://arxiv.org/abs/2211.11736" target="_blank" rel="noopener">Paper</a>] [<a href="https://instructionaugmentation.github.io/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409120027347.png" alt="image-20230409120027347" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>CLIP Model</li>
<li>Few Shot/Fine-tuning</li>
<li>Manipulation trajectories</li>
</ul>
</li>
<li>
<p><strong>CLIP-Fields</strong>:&ldquo;CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory&rdquo;, &ldquo;arXiv, Oct 2022&rdquo;, [<a href="https://arxiv.org/abs/2210.05663" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/notmahi/clip-fields" target="_blank" rel="noopener">PyTorch Code</a>] [<a href="https://mahis.life/clip-fields/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>weakly supervised semantic neural fields, called CLIP-Fields.</li>
<li>dataset comes from LLM/MMM</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409211255049.png" alt="image-20230409211255049" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>$g(x, y, z) : R_3 → R_d$ serves as a generic differentiable spatial database.  trained.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409211327053.png" alt="image-20230409211327053" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>CLIP BERT</li>
<li>EVALUATION:  Instance and semantic segmentation in scene images.  for visual encoder</li>
<li>EVALUATION:   Semantic Navigation on Robot with CLIP-Fields as Semantic-Spatial Memory.  maximizing their similarity.</li>
</ul>
</li>
<li>
<p><strong>LaTTe</strong>: &ldquo;LaTTe: Language Trajectory TransformEr&rdquo;, <em>arXiv, Aug 2022</em>. [<a href="https://arxiv.org/abs/2208.02918" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/arthurfenderbucker/NL_trajectory_reshaper" target="_blank" rel="noopener">TensorFlow Code</a>] [<a href="https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/robot-language/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409122254407.png" alt="image-20230409122254407" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>BERT CLIP</li>
<li>trained and evaluated the model described in Section III over a dataset containing 100k examples of procedurally generated trajectory modification.</li>
<li>Trajectory reshaping obeying user’s constraints.</li>
</ul>
</li>
<li>
<p><strong>Robots Enact Malignant Stereotypes</strong>: &ldquo;Robots Enact Malignant Stereotypes&rdquo;, <em>FAccT, Jun 2022</em>. [<a href="https://arxiv.org/abs/2207.11569" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/ahundt/RobotsEnactMalignantStereotypes" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://sites.google.com/view/robots-enact-stereotypes/home" target="_blank" rel="noopener">Website</a>] [<a href="https://www.washingtonpost.com/technology/2022/07/16/racist-robots-ai/" target="_blank" rel="noopener">Washington Post</a>] [<a href="https://www.wired.com/story/how-to-stop-robots-becoming-racist/" target="_blank" rel="noopener">Wired</a>] (code access on request)</p>
<ul>
<li>experiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy.</li>
<li>Ethical experiments on LLM/MMM</li>
</ul>
</li>
<li>
<p><strong>ATLA</strong>: &ldquo;Leveraging Language for Accelerated Learning of Tool Manipulation&rdquo;, <em>CoRL, Jun 2022</em>. [<a href="https://arxiv.org/abs/2206.13074" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>combining linguistic information and meta-learning significantly accelerates tool learning</li>
<li>tools $ \mathscr{T} = { \tau_i }<em>{i=1}^K$     corresponding language descriptions $L_i = {l</em>{ij}}<em>{j=1}^{N_i}\ l</em>{ij} \in  \mathscr{L}$ by LLM</li>
<li>Goal $\pi_{\theta}:  \mathscr{O} \times  \mathscr{L} \rightarrow  \mathscr{A}$ that can be quickly fine-tuned at test time.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230410005237562.png" alt="image-20230410005237562" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>GPT</li>
<li>RL: PPO Meta-training</li>
</ul>
</li>
<li>
<p><strong>ZeST</strong>: &ldquo;Can Foundation Models Perform Zero-Shot Task Specification For Robot Manipulation?&rdquo;, <em>L4DC, Apr 2022</em>. [<a href="https://arxiv.org/abs/2204.11134" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409120237050.png" alt="image-20230409120237050" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Zero Shot</li>
<li>CLIP Model</li>
<li>Similarity calculation</li>
<li>As Reward Function in offline RL</li>
</ul>
</li>
<li>
<p><strong>LSE-NGU</strong>: &ldquo;Semantic Exploration from Language Abstractions and Pretrained Representations&rdquo;, <em>arXiv, Apr 2022</em>. [<a href="https://arxiv.org/abs/2204.05080" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>Encourage to do exploration $max\ E[\sum_{t=0}^H\gamma^t(r_t^e+\beta r_t^i)]$</li>
<li>exploration method calculates the intrinsic reward from $O_L$ or $O_V$ .</li>
<li>CLIP Bert ALM</li>
<li>Zero-Shot for calculating the intrinsic reward (NGP: L2 distances between the current state and the k-nearest neighbor representations stored in the memory buffer, RND:  mean squared error $|| f_V(O_V) - \hat{f_V}(O_V)|| $ , $f_V(O_V)$ is trainable, $\hat{f_V}(O_V)$ is pre-trained model)</li>
</ul>
</li>
<li>
<p><strong>Embodied-CLIP</strong>: &ldquo;Simple but Effective: CLIP Embeddings for Embodied AI &ldquo;, <em>CVPR, Nov 2021</em>. [<a href="https://arxiv.org/abs/2111.09888" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/allenai/embodied-clip" target="_blank" rel="noopener">Pytorch Code</a>]</p>
<ul>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409144829744.png" alt="image-20230409144829744" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Embodied AI tasks: agents that learn to navigate and interact with their environments.</li>
<li>CLIP for visual encoder</li>
<li>RL: PPO</li>
</ul>
</li>
<li>
<p><strong>CLIPort</strong>: &ldquo;CLIPort: What and Where Pathways for Robotic Manipulation&rdquo;, <em>CoRL, Sept 2021</em>. [<a href="https://arxiv.org/abs/2109.12098" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/cliport/cliport" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://cliport.github.io/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230410010919998.png" alt="image-20230410010919998" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Transporter for Pick-and-Place: The same architecture has three different networks $f_{pick}, \Psi_{query}, \Psi_{key}$</li>
<li>CLIP</li>
<li>Trajectory Imitative learning</li>
</ul>
</li>
<li>
<p><strong>VIMA</strong>:&ldquo;VIMA: General Robot Manipulation with Multimodal Prompts&rdquo;, &ldquo;arXiv, Oct 2022&rdquo;, [<a href="https://arxiv.org/abs/2210.03094" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/vimalabs/VIMA" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://vimalabs.github.io/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>a transformer-based generalist robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409213812956.png" alt="image-20230409213812956" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Mask R-CNN ViT for T5 Encoder</li>
<li>Build his own MMM</li>
<li>Trajectory Imitative learning on discretization action space</li>
</ul>
</li>
<li>
<p><strong>Perceiver-Actor</strong>:&ldquo;A Multi-Task Transformer for Robotic Manipulation&rdquo;, <em>CoRL, Sep 2022</em>. [<a href="https://peract.github.io/paper/peract_corl2022.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/peract/peract" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://peract.github.io/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>PERACT encodes a sequence of RGB-D voxel(3D info) patches and predicts discretized translations,</li>
<li>PERACT is essentially a classifier trained with supervised learning to detect actions</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409220107716.png" alt="image-20230409220107716" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Language Encoder: CLIP</li>
<li>Voxel Encoder pre-trained 3D convolution network</li>
<li>Build his own MMM</li>
<li>Trajectory Imitative learning</li>
</ul>
</li>
</ul>
<hr>
<h1 id="instructions-and-navigation">Instructions and Navigation</h1>
<ul>
<li>
<p><strong>ADAPT</strong>: &ldquo;ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts&rdquo;, <em>CVPR, May 2022</em>. [<a href="https://arxiv.org/abs/2205.15509" target="_blank" rel="noopener">Paper</a>]</p>
</li>
<li>
<p>&ldquo;The Unsurprising Effectiveness of Pre-Trained Vision Models for Control&rdquo;, <em>ICML, Mar 2022</em>. [<a href="https://arxiv.org/abs/2203.03580" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/sparisi/pvr_habitat" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://sites.google.com/view/pvr-control" target="_blank" rel="noopener">Website</a>]</p>
</li>
<li>
<p><strong>CoW</strong>: &ldquo;CLIP on Wheels: Zero-Shot Object Navigation as Object Localization and Exploration&rdquo;, <em>arXiv, Mar 2022</em>. [<a href="https://arxiv.org/abs/2203.10421" target="_blank" rel="noopener">Paper</a>]</p>
</li>
<li>
<p><strong>Recurrent VLN-BERT</strong>: &ldquo;A Recurrent Vision-and-Language BERT for Navigation&rdquo;, <em>CVPR, Jun 2021</em> [<a href="https://arxiv.org/abs/2011.13922" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/YicongHong/Recurrent-VLN-BERT" target="_blank" rel="noopener">Pytorch Code</a>]</p>
</li>
<li>
<p><strong>VLN-BERT</strong>: &ldquo;Improving Vision-and-Language Navigation with Image-Text Pairs from the Web&rdquo;, <em>ECCV, Apr 2020</em> [<a href="https://arxiv.org/abs/2004.14973" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/arjunmajum/vln-bert" target="_blank" rel="noopener">Pytorch Code</a>]</p>
</li>
<li>
<p>&ldquo;Interactive Language: Talking to Robots in Real Time&rdquo;, <em>arXiv, Oct 2022</em> [<a href="https://arxiv.org/abs/2210.06407" target="_blank" rel="noopener">Paper</a>] [<a href="https://interactive-language.github.io/" target="_blank" rel="noopener">Website</a>]</p>
</li>
</ul>
<hr>
<h1 id="simulation-frameworks">Simulation Frameworks</h1>
<ul>
<li><strong>MineDojo</strong>: &ldquo;MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge&rdquo;, <em>arXiv, Jun 2022</em>. [<a href="https://arxiv.org/abs/2206.08853" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/MineDojo/MineDojo" target="_blank" rel="noopener">Code</a>] [<a href="https://minedojo.org/" target="_blank" rel="noopener">Website</a>] [<a href="https://minedojo.org/knowledge_base.html" target="_blank" rel="noopener">Open Database</a>]</li>
<li><strong>Habitat 2.0</strong>: &ldquo;Habitat 2.0: Training Home Assistants to Rearrange their Habitat&rdquo;, <em>NeurIPS, Dec 2021</em>. [<a href="https://arxiv.org/abs/2106.14405" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/facebookresearch/habitat-sim" target="_blank" rel="noopener">Code</a>] [<a href="https://aihabitat.org/" target="_blank" rel="noopener">Website</a>]</li>
<li><strong>BEHAVIOR</strong>: &ldquo;BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments&rdquo;, <em>CoRL, Nov 2021</em>. [<a href="https://arxiv.org/abs/2108.03332" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/StanfordVL/behavior" target="_blank" rel="noopener">Code</a>] [<a href="https://behavior.stanford.edu/" target="_blank" rel="noopener">Website</a>]</li>
<li><strong>iGibson 1.0</strong>: &ldquo;iGibson 1.0: a Simulation Environment for Interactive Tasks in Large Realistic Scenes&rdquo;, <em>IROS, Sep 2021</em>. [<a href="https://arxiv.org/abs/2012.02924" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/StanfordVL/iGibson" target="_blank" rel="noopener">Code</a>] [<a href="https://svl.stanford.edu/igibson/" target="_blank" rel="noopener">Website</a>]</li>
<li><strong>ALFRED</strong>: &ldquo;ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks&rdquo;, <em>CVPR, Jun 2020</em>. [<a href="https://arxiv.org/abs/1912.01734" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/askforalfred/alfred" target="_blank" rel="noopener">Code</a>] [<a href="https://askforalfred.com/" target="_blank" rel="noopener">Website</a>]</li>
<li><strong>BabyAI</strong>: &ldquo;BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning&rdquo;, <em>ICLR, May 2019</em>. [<a href="https://openreview.net/pdf?id=rJeXCo0cYX" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/mila-iqia/babyai/tree/iclr19" target="_blank" rel="noopener">Code</a>]</li>
</ul>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/llm/">LLM</a>
  
  <a class="badge badge-light" href="/tag/rl/">RL</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fexample.com%2Fpost%2Fllm-for-rl%2F&amp;text=LLM%2FMMM&#43;for&#43;RL" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fexample.com%2Fpost%2Fllm-for-rl%2F&amp;t=LLM%2FMMM&#43;for&#43;RL" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
        
      
      <li>
        <a href="mailto:?subject=LLM%2FMMM%20for%20RL&amp;body=https%3A%2F%2Fexample.com%2Fpost%2Fllm-for-rl%2F" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fexample.com%2Fpost%2Fllm-for-rl%2F&amp;title=LLM%2FMMM&#43;for&#43;RL" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="whatsapp://send?text=LLM%2FMMM&#43;for&#43;RL%20https%3A%2F%2Fexample.com%2Fpost%2Fllm-for-rl%2F" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Fexample.com%2Fpost%2Fllm-for-rl%2F&amp;title=LLM%2FMMM&#43;for&#43;RL" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  
    



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://example.com/"><img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hufbf25d63bcf03f0b8722ada9c3154189_60776_270x270_fill_q75_lanczos_center.jpg" alt="Haofei Hou"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://example.com/">Haofei Hou</a></h5>
      <h6 class="card-subtitle">Undergraduate</h6>
      <p class="card-text">My research interests include Computational Cognitive Science, Artificial Intelligence.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:yuechuhaoxi020609@outlook.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/yuechuhaoxi020609" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  
















  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2023 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js"></script>




  

  
  

  













  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  







<script id="page-data" type="application/json">{"use_headroom":true}</script>



  <script src="/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js" type="module"></script>









  
  


<script src="/en/js/wowchemy.min.e8ee06ba8371980ffde659871dd593b0.js"></script>







  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js" type="module"></script>


















</body>
</html>
