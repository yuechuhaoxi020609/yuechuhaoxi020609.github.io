<!DOCTYPE html>
<!-- This site was created with Wowchemy. https://www.wowchemy.com -->
<!-- Last Published: July 16, 2023 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.7.0 for Hugo" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css" integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.0f229d4b7ebad1917a9a357cba2effab.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  


























  
  
  






  <meta name="author" content="Haofei Hou" />





  

<meta name="description" content="Articles Summary about LLM/MMM for RL" />



<link rel="alternate" hreflang="en-us" href="https://example.com/post/llm-for-rl/" />
<link rel="canonical" href="https://example.com/post/llm-for-rl/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hu88cdd4b7a12988e7d5bb56be50e4f21a_10097_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hu88cdd4b7a12988e7d5bb56be50e4f21a_10097_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#1565c0" />










  
  






<meta property="twitter:card" content="summary" />
<meta property="twitter:image" content="https://example.com/media/icon_hu88cdd4b7a12988e7d5bb56be50e4f21a_10097_512x512_fill_lanczos_center_3.png" />
<meta property="og:site_name" content="Haofei Hou | HUST" />
<meta property="og:url" content="https://example.com/post/llm-for-rl/" />
<meta property="og:title" content="LLM/MMM for RL | Haofei Hou | HUST" />
<meta property="og:description" content="Articles Summary about LLM/MMM for RL" /><meta property="og:image" content="https://example.com/media/icon_hu88cdd4b7a12988e7d5bb56be50e4f21a_10097_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />

  
    <meta
      property="article:published_time"
      content="2023-04-10T00:00:00&#43;00:00"
    />
  
  
    <meta property="article:modified_time" content="2023-07-16T00:00:00&#43;00:00">
  






    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://example.com/post/llm-for-rl/"
  },
  "headline": "LLM/MMM for RL",
  
  "datePublished": "2023-04-10T00:00:00Z",
  "dateModified": "2023-07-16T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Haofei Hou"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Haofei Hou | HUST",
    "logo": {
      "@type": "ImageObject",
      "url": "https://example.com/media/icon_hu88cdd4b7a12988e7d5bb56be50e4f21a_10097_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "Articles Summary about LLM/MMM for RL"
}
</script>

  

  




  
  
  

  
  

  


  
  <title>LLM/MMM for RL | Haofei Hou | HUST</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="63c2f496d8a3605deef0eb2ba6524b45" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Haofei Hou | HUST</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Haofei Hou | HUST</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#featured"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#projects"><span>Projects</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#posts"><span>Posts</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article">

  













  

  
  
  
<div class="article-container pt-3">
  <h1>LLM/MMM for RL</h1>

  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      <a href="/authors/admin/">Haofei Hou</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Jul 16, 2023
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    22 min read
  </span>
  

  
  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/category/summary/">Summary</a></span>
  

</div>

    





  
</div>



  <div class="article-container">

    <div class="article-style">
      <ul>
<li><strong>Large Language Models</strong>: GPT BERT</li>
<li><strong>Multi-Modal Models</strong>: CLIP</li>
</ul>
<h1 id="reasoning">Reasoning</h1>
<ul>
<li><strong>TidyBot</strong>: &ldquo;Personalized Robot Assistance with Large Language Models&rdquo;, <em>arXiv, May 2023</em>. [<a href="https://arxiv.org/abs/2305.05658" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/jimmyyhwu/tidybot/tree/main/robot" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://tidybot.cs.princeton.edu/" target="_blank" rel="noopener">Website</a>]
<ul>
<li>personalization of household cleanup with robots</li>
<li>where objects should go is highly personal, and depends on cultural norms and individual preferences</li>
<li>Our approach is to utilize the summarization capabilities of large language models (LLMs) to provide generalization from a small number of example preferences</li>
<li>a publicly released benchmark dataset for evaluating generalization of receptacle selection preferences</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714222509970.png" alt="image-20230714222509970" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li><strong>PIGLeT</strong>: &ldquo;PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World&rdquo;, <em>ACL, Jun 2021</em>. [<a href="https://arxiv.org/abs/2106.00188" target="_blank" rel="noopener">Paper</a>] [<a href="http://github.com/rowanz/piglet" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://rowanzellers.com/piglet/" target="_blank" rel="noopener">Website</a>]
<ul>
<li>factorize PIGLeT into a physical dynamics model, and a separate language model.</li>
<li>read a sentence, simulate neurally what might happen next, and then communicate that result through a literal symbolic representation, or natural language.</li>
<li>Environment: THOR</li>
<li>$ (o_1, o_2 &hellip;) \times \boldsymbol{a} \rightarrow (o^{\prime}_1, o^{\prime}_2 &hellip;)$ trained by data</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715003231256.png" alt="image-20230715003231256" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li><strong>Matcha</strong>: &ldquo;Chat with the Environment: Interactive Multimodal Perception using Large Language Models&rdquo;, Accepted in IROS, 2023. [<a href="https://arxiv.org/pdf/2303.08268.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/xf-zhao/Matcha" target="_blank" rel="noopener">Github</a>] [<a href="https://matcha-model.github.io/" target="_blank" rel="noopener">Website</a>]
<ul>
<li>able to interactively perceive (“chat” with) the environment through multimodal perception when the information from passive visual perception is insufficient for completing an instructed task.</li>
<li>The epistemic actions are executed autoregressively until the agent is confident enough about the information sufficiency in that situation.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715002541087.png" alt="image-20230715002541087" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li><strong>LMPrior</strong>: &ldquo;LMPriors: Pre-Trained Language Models as Task-Specific Priors&rdquo;, <em>arXiv, Oct 2022</em>. [<a href="https://arxiv.org/abs/2210.12530" target="_blank" rel="noopener">Paper</a>]
<ul>
<li><strong>LMPrior as a function transform</strong>. We define LMPriors as a family of functions $\mathcal{P}$ which take some relevant metadata $D_{\text{meta}}$ which is not used by the traditional learning process $f$. The LMPrior then transforms $f$ to $\tilde{f}$ which exhibits a bias towards outputs which are consistent with the metadata $\mathcal{D}_{\text {meta}}$</li>
<li>Use the name and brief description of the label to help complete the task Feature selection, Reinforcement learning, Causal discovery</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230710120308271.png" alt="image-20230710120308271" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230710120424484.png" alt="image-20230710120424484" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li><strong>ReAct</strong>: &ldquo;ReAct: Synergizing Reasoning and Acting in Language Models&rdquo;, <em>ICLR, 2023</em>. [<a href="https://arxiv.org/abs/2210.03629" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/ysymyth/ReAct" target="_blank" rel="noopener">Github</a>] [<a href="https://react-lm.github.io/" target="_blank" rel="noopener">Website</a>]
<ul>
<li>their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics.</li>
<li>combine reasoning and acting with language models for solving diverse language reasoning and decision making tasks.</li>
<li>Knowledge-Intensive Reasoning Tasks</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230710172745445.png" alt="image-20230710172745445" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li><strong>Generative Agents</strong>: &ldquo;Generative Agents: Interactive Simulacra of Human Behavior&rdquo;, <em>arXiv, Apr 2023</em>. [<a href="https://arxiv.org/abs/2304.03442v1" target="_blank" rel="noopener">Paper</a>]
<ul>
<li>generative agents—agents that draw on generative models to simulate believable human behavior</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711003847934.png" alt="image-20230711003847934" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>&ldquo;Large Language Models as Zero-Shot Human Models for Human-Robot Interaction&rdquo;, <em>arXiv, Mar 2023</em>. [<a href="https://arxiv.org/abs/2303.03548v1" target="_blank" rel="noopener">Paper</a>]
<ul>
<li>crafting good human models is challenging</li>
<li>how LLM-based human models can be integrated into a social robot’s planning process and applied in HRI scenarios</li>
<li>LLM do social inference tasks</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711013413057.png" alt="image-20230711013413057" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>&ldquo;Translating Natural Language to Planning Goals with Large-Language Models&rdquo;, <em>arXiv, Feb 2023</em>. [<a href="https://arxiv.org/abs/2302.05128" target="_blank" rel="noopener">Paper</a>]
<ul>
<li>LLMs are unable to perform accurate reasoning nor solve planning problems, which may limit their usefulness for robotics-related tasks</li>
<li>LLMs are able to translate goals specified in natural language to a structured planning language</li>
<li>Planning Domain Definition Language (PDDL)</li>
<li>Few-shots</li>
<li>Sub benchmark: Domain Understanding, Goal Inference, PDDL Goal Specification</li>
</ul>
</li>
<li>&ldquo;PDDL Planning with Pretrained Large Language Models&rdquo;, <em>NeurlPS, 2022</em>. [<a href="https://openreview.net/forum?id=1QMMUB4zfl" target="_blank" rel="noopener">Paper</a>] [<a href="https://tinyurl.com/llm4pddl" target="_blank" rel="noopener">Github</a>]
<ul>
<li>In certain PDDL domains, LLMs are capable of solving some nontrivial problems.</li>
<li>However, in many other cases, LLMs cannot yet solve problems on their own.</li>
<li>LLMs are sensitive to the semantics of the English terms used in the PDDL problems.</li>
<li>we prompt with the original PDDL syntax, rather than domain-specific natural language encodings of the PDDL.  it makes better result.</li>
<li>use Codex — a version of GPT-3 fine-tuned on general code to get a better result</li>
</ul>
</li>
<li><strong>LLM+P</strong>:&ldquo;LLM+P: Empowering Large Language Models with Optimal Planning Proficiency&rdquo;, <em>arXiv, Apr 2023</em>, [<a href="https://arxiv.org/abs/2304.11477" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/Cranial-XIX/llm-pddl" target="_blank" rel="noopener">Code</a>]
<ul>
<li>the first framework that incorporates the strengths of classical planners into LLMs.</li>
<li>LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL),</li>
<li>then leveraging classical planners to quickly find a solution,</li>
<li>and then translating the found solution back into natural language.</li>
</ul>
</li>
<li>&ldquo;Foundation Models for Decision Making: Problems, Methods, and Opportunities&rdquo;, <em>arXiv, Mar 2023</em>, [<a href="https://arxiv.org/abs/2303.04129" target="_blank" rel="noopener">Paper</a>]
<ul>
<li>foundation models can serve as generative models of behavior (e.g., skill discovery) and generative models of the environment</li>
<li>foundation models can serve as representation learners of states, actions, rewards, and transition dynamics</li>
<li>language foundation models can serve as interactive agents and environments, enabling new problems and applications to be considered under a sequential decision making framework</li>
</ul>
</li>
</ul>
<hr>
<h1 id="planning">Planning</h1>
<ul>
<li><strong>PromptCraft</strong>: &ldquo;ChatGPT for Robotics: Design Principles and Model Abilities&rdquo;, <em>Blog, Feb 2023</em>, [<a href="https://www.microsoft.com/en-us/research/uploads/prod/2023/02/ChatGPT___Robotics.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/chatgpt-for-robotics/" target="_blank" rel="noopener">Website</a>]
<ul>
<li>evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks</li>
<li>use free-form dialog,</li>
<li>parse XML tags,</li>
<li>and to synthesize code</li>
<li>use of task-specific prompting functions and closed-loop reasoning through dialogues</li>
<li>PromptCraft, which contains a platform where researchers can collaboratively upload and vote on examples of good prompting schemes for robotics applications</li>
</ul>
</li>
<li><strong>FILM</strong>: &ldquo;FILM: Following Instructions in Language with Modular Methods&rdquo;, <em>ICLR, 2022</em>. [<a href="https://arxiv.org/abs/2110.07342" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/soyeonm/FILM" target="_blank" rel="noopener">Code</a>] [<a href="https://soyeonm.github.io/FILM_webpage/" target="_blank" rel="noopener">Website</a>]
<ul>
<li>embodied instruction</li>
<li>(1) builds a semantic map of the scene</li>
<li>(2) performs exploration with a semantic search policy</li>
<li>ALFRED env</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715155943746.png" alt="image-20230715155943746" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li><strong>Don&rsquo;t Copy the Teacher</strong>: &ldquo;Don’t Copy the Teacher: Data and Model Challenges in Embodied Dialogue&rdquo;, <em>EMNLP, 2022</em>. [<a href="https://arxiv.org/abs/2210.04443" target="_blank" rel="noopener">Paper</a>] [<a href="https://www.youtube.com/watch?v=qGPC65BDJw4&amp;t=2s" target="_blank" rel="noopener">Website</a>]
<ul>
<li>The article discusses TEACh, a new task system that focuses on Entire Dialogue History (EDH) and Trajectory from Dialogue (TfD), in which agents complete household tasks based on text and visual cues.</li>
<li>In EDH, agents predict actions from partial dialogue history, while in TfD, they&rsquo;re expected to complete a whole task from a full dialogue history.</li>
<li>The authors present three models for these tasks: ET, a direct sequence imitation transformer, FILM, a four-module system originally designed for ALFRED tasks, and Symbiote, a competitive modular method for EDH.</li>
<li>FILM was adapted for TEACh by refactoring the code to accept dialogue history as input, and retraining the model.</li>
<li>The paper primarily investigates how to improve task execution by agents using dialogue history and how to evaluate their performance.</li>
</ul>
</li>
<li><strong>PaLM-E</strong>: &ldquo;PaLM-E: An Embodied Multimodal Language Model&rdquo;, <em>arXiv, Mar 2023</em>, [<a href="https://arxiv.org/abs/2303.03378" target="_blank" rel="noopener">Paper</a>] [<a href="https://palm-e.github.io/" target="_blank" rel="noopener">Webpage</a>]
<ul>
<li>grounding end-to-end model</li>
<li>Input :multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings.</li>
<li>in conjunction with a pretrained large language mode</li>
<li>robotic manipulation planning, visual question answering, and captioning</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714234352286.png" alt="image-20230714234352286" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li><strong>RT-1</strong>: &ldquo;RT-1: Robotics Transformer for Real-World Control at Scale&rdquo;, <em>arXiv, Dec 2022</em>. [<a href="https://arxiv.org/abs/2212.06817" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/google-research/robotics_transformer" target="_blank" rel="noopener">GitHub</a>] [<a href="https://robotics-transformer.github.io/" target="_blank" rel="noopener">Website</a>]
<ul>
<li>open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data</li>
<li>based on a large-scale data collection on real robots performing real-world tasks</li>
<li>output is plan</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714235720100.png" alt="image-20230714235720100" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
</ul>
<ul>
<li>
<p><strong>InnerMonlogue</strong>: &ldquo;Inner Monologue: Embodied Reasoning through Planning with Language Models&rdquo;, <em>arXiv, July 2022</em>. [<a href="https://arxiv.org/abs/2207.05608" target="_blank" rel="noopener">Paper</a>] [<a href="https://innermonologue.github.io/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>system Inner Monologue chains together these various components (perception models, robotic skills, and human feedback) in a shared language prompt, enabling it to successfully perform user instructions.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230410093435285.png" alt="image-20230410093435285" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Inner Monologue(Left): InstructGPT</li>
<li>pick-and-place primitive: CLIPort</li>
<li>Manipulation</li>
</ul>
</li>
<li>
<p><strong>Housekeep</strong>: &ldquo;Housekeep: Tidying Virtual Households using Commonsense Reasoning&rdquo;, <em>arXiv, May 2022</em>. [<a href="https://arxiv.org/abs/2205.10712" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/yashkant/housekeep" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://yashkant.github.io/housekeep/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>New benchmark: In Housekeep, an embodied agent must tidy a house by rearranging misplaced objects without explicit instructions specifying which objects need to be rearranged</li>
<li>A new dataset for an object (e.g. salt-shaker), a room (e.g. kitchen) in context, to classify into the following categories: misplaced, correct, implausible.</li>
<li>Each Housekeep episode is created by instantiating 7-10 objects within a scene, out of which 3-5 objects are misplaced and the remaining are placed correct</li>
<li>Object Room $[OR] &ndash; P(room|object)$ : Generate compatibility scores for rooms for a given object.</li>
<li>Object Room Receptacle $[ORR] &ndash; P(receptacle|object, room)$: Generate compatibility scores for receptacles within a given room and for a given object</li>
<li>Method1: Finetuning by Contrastive Matching (CM). We train a 3-layered MLP on top of language embeddings and compute pairwise cosine similarity between any two embeddings</li>
<li>Method2: Zero-Shot Ranking via MLM (ZS-MLM).</li>
<li>LLM: BERT for word embeddings</li>
<li>MLM(Masked Language Model)</li>
<li>Similarity calculation</li>
</ul>
</li>
<li>
<p><strong>LID</strong>: &ldquo;Pre-Trained Language Models for Interactive Decision-Making&rdquo;, <em>arXiv, Feb 2022</em>. [<a href="https://arxiv.org/abs/2202.01771" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/ShuangLI59/Language-Model-Pre-training-Improves-Generalization-in-Policy-Learning" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://shuangli-project.github.io/Pre-Trained-Language-Models-for-Interactive-Decision-Making/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409121632758.png" alt="image-20230409121632758" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Fine-tuning</li>
<li>Mid-level actions</li>
<li>LLM</li>
<li>As a part of policy network.</li>
</ul>
</li>
<li>
<p><strong>LLM-BRAIn</strong>: &ldquo;LLM-BRAIn: AI-driven Fast Generation of Robot Behaviour Tree based on Large Language Model&rdquo;, <em>arXiv, May 2023</em>. [<a href="https://arxiv.org/abs/2305.19352" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>LLM-BRAIn is a transformer-based Large Language Model (LLM) finetuned from Stanford Alpaca 7B model to generate robot behavior tree (BT) from the text description.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714211624993.png" alt="image-20230714211624993" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>MOO</strong>: &ldquo;Open-World Object Manipulation using Pre-Trained Vision-Language Models&rdquo;, <em>arXiv, Mar 2022</em>. [<a href="https://arxiv.org/abs/2303.00905" target="_blank" rel="noopener">Paper</a>] [<a href="https://robot-moo.github.io/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>To allow robots to generalize to new semantic concepts, we choose to leverage VLMs</li>
<li>position information to encode</li>
<li>imitation learning</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711180547447.png" alt="image-20230711180547447" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>CALM</strong>: &ldquo;Keep CALM and Explore: Language Models for Action Generation in Text-based Games&rdquo;, <em>arXiv, Oct 2020</em>. [<a href="https://arxiv.org/abs/2010.03903v1" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/princeton-nlp/calm-textgame" target="_blank" rel="noopener">Pytorch Code</a>]</p>
<ul>
<li>text-based games.</li>
<li>train a model for action choice</li>
<li>Deep Reinforcement Relevance Network (DRRN) . This allows our model to combine generic linguistic priors for action generation with the ability to adaptively choose actions that are best suited for the game.</li>
<li>dataset of 426 human gameplay transcripts for 590 different text-based games.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711102810528.png" alt="image-20230711102810528" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>ZSP</strong>: &ldquo;Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents&rdquo;, <em>ICML, Jan 2022</em>. [<a href="https://arxiv.org/abs/2201.07207" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/huangwl18/language-planner" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://wenlong.page/language-planner/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230410095827203.png" alt="image-20230410095827203" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>VirtualHome Executability Correctness</li>
<li>Planning LM (Causal LLM): GPT-3 $\hat{a}$</li>
<li>Translation LM(Masked LLM): BERT</li>
<li>for each admissible environment action $a_e$ $max\ C(f(\hat{a}), f(a_e)) = \frac{f(\hat{a})\cdot f(a_e)}{||f(\hat{a})||\ ||f(a_e)||}$</li>
<li>Mid-level actions.</li>
</ul>
</li>
<li>
<p><strong>ChatGPT-Prompts</strong>: &ldquo;ChatGPT Empowered Long-Step Robot Control in Various Environments: A Case Application&rdquo;, <em>arXiv, Jun 2023</em>. [<a href="https://arxiv.org/abs/2304.03893" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/microsoft/ChatGPT-Robot-Manipulation-Prompts" target="_blank" rel="noopener">Pytorch Code</a>]</p>
<ul>
<li>VirtualHome</li>
<li>ChatGPT prompt engineering
<ul>
<li>Output a sequence of predefined robot actions with explanations in a readable JSON format.</li>
<li>Represent the operating environment in a formalized style.</li>
<li>Infer and output the updated state of the operating environment, which can be reused as the next input, allowing ChatGPT to operate based solely on the memory of the latest operations.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>&ldquo;Planning with Large Language Models via Corrective Re-prompting&rdquo;, <em>arXiv, Nov 2022</em>. [<a href="https://arxiv.org/abs/2311.09935" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>VirtualHome</li>
<li>When an agent is unable to execute an action, our approach re-prompts the LLM with precondition error information to extract an executable corrective action to achieve the intended goal in the current context.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711162943164.png" alt="image-20230711162943164" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p>&ldquo;Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions&rdquo;, <em>arXiV, Oct 2020</em>, [<a href="https://arxiv.org/abs/2009.14259" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>The ALFRED challenge task: a virtual robotic agent to complete everyday tasks  executing complex visually-grounded semantic plans</li>
<li>successful plans from language directives alone without any visual input in 26% of unseen cases</li>
<li>models using input from a single modality often performed nearly as good as or better than their multi-modal counterparts</li>
</ul>
</li>
<li>
<p><strong>TIP</strong>: &ldquo;Multimodal Procedural Planning via Dual Text-Image Prompting&rdquo;, <em>arXiV, May 2023</em>, [<a href="https://arxiv.org/abs/2305.01795" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>the potential of providing instructions informed by texts and images to assist humans in completing tasks remains underexplored</li>
<li>given a high-level goal and generate plans of paired text-image steps, providing more complementary and informative guidance than unimodal plans.</li>
<li>Visual-Grounded Text Plan Text-Image Prompt</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714192447413.png" alt="image-20230714192447413" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714192511754.png" alt="image-20230714192511754" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>VLaMP</strong>: &ldquo;Pretrained Language Models as Visual Planners for Human Assistance&rdquo;, <em>arXiV, Apr 2023</em>, [<a href="https://arxiv.org/abs/2304.09179" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>‘Visual Planning for Assistance (VPA)’. Given a goal briefly described in natural language and a video of the user’s progress so far, the aim of VPA is to obtain a plan.</li>
<li>Forecasting in Videos</li>
<li>Fine-tune on LLM</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714193601697.png" alt="image-20230714193601697" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>Text2Motion</strong>: &ldquo;Text2Motion: From Natural Language Instructions to Feasible Plans&rdquo;, <em>arXiV, Mar 2023</em>, [<a href="https://arxiv.org/abs/2303.12153" target="_blank" rel="noopener">Paper</a>] [<a href="https://sites.google.com/stanford.edu/text2motion" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>interfaces an <strong>LLM</strong> with a library of learned skills and a <strong>geometric feasibility planner</strong> to solve complex sequential manipulation tasks</li>
<li>infers goal states from a natural language instruction</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230713225625008.png" alt="image-20230713225625008" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>LLM-planner</strong>: &ldquo;LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models&rdquo;, <em>arXiv, Mar 2023</em>. [<a href="https://arxiv.org/abs/2212.04088" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/OSU-NLP-Group/LLM-Planner/" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://dki-lab.github.io/LLM-Planner/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>different from SayCan, we use LLMs to directly generate plans instead of ranking admissible skills</li>
<li>its ability to dynamically re-plan based on what the agent observes in the current environment</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230713224102931.png" alt="image-20230713224102931" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>kNN to get nearest plan</li>
<li>ALFRED env</li>
</ul>
</li>
<li>
<p><strong>GD</strong>: &ldquo;Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control&rdquo;, <em>arXiv, Mar 2023</em>. [<a href="https://arxiv.org/abs/2303.00855" target="_blank" rel="noopener">Paper</a>] [<a href="https://grounded-decoding.github.io/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>decodes the token probability of an LLM and token probabilities from token-conditioned, robotic functions, such as <strong>affordance, safety, preference</strong>.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230713233627295.png" alt="image-20230713233627295" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Grounded Models: CLIPort, etc&hellip;</li>
</ul>
</li>
<li>
<p><strong>COWP</strong>: &ldquo;Robot Task Planning and Situation Handling in Open Worlds&rdquo;, <em>arXiv, Oct 2022</em>. [<a href="https://arxiv.org/abs/2210.01287" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/yding25/GPT-Planner" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://cowplanning.github.io/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>acquire common sense from LLMs, and aim to improve the task completion and situation handling skills of service robots.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230713205214315.png" alt="image-20230713205214315" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230713205228824-1.png" alt="image-20230713205228824" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>GLAM</strong>: &ldquo;Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning&rdquo;, <em>arXiv, May 2023</em>. [<a href="https://arxiv.org/abs/2302.02662" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/flowersteam/Grounding_LLMs_with_online_RL" target="_blank" rel="noopener">Pytorch Code</a>]</p>
<ul>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230709192428587.png" alt="image-20230709192428587" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>&ldquo;functional grounding&rdquo; problem, is relative to a particular environment which may be the human physical environment but also more abstract interactive environments simulated in computers (where abstract physics can differ from human environments.</li>
<li>textual worlds, Language-conditioned RL</li>
<li>finetune LLM</li>
</ul>
</li>
<li>
<p>&ldquo;Reward Design with Language Models&rdquo;, <em>ICML, Feb 2023</em>. [<a href="https://arxiv.org/abs/2303.00001v1" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/minaek/reward_design_with_llms" target="_blank" rel="noopener">Pytorch Code</a>]</p>
<ul>
<li>We evaluate whether our approach can train agents aligned with user objectives.</li>
<li>Given a few examples or a description demonstrating the user’s objective, an LLM should be able to provide an accurate instantiation of reward values on a new test example, allowing for easier generalization to new objectives.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230709205456525.png" alt="image-20230709205456525" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>This article directly constructs a reward generation system related to the expected reward prompt. Similar to previous works, this study is also subject to limitations in specific domains (those related to common-sense knowledge) and specific environments (those that are easy to express in text and have short decision cycles), making it difficult to use LLMs for reward design in general domains. However, this paper, along with the ELLM, demonstrates the power of pre-training large models and shows that LLMs can be well integrated into RL to perform various roles, such as assisting in exploration and reward design, among others.</li>
</ul>
</li>
<li>
<p><strong>LLM-MCTS</strong>: &ldquo;Large Language Models as Commonsense Knowledge for Large-Scale Task Planning&rdquo;, <em>arXiv, May 2023</em>. [<a href="https://arxiv.org/abs/2305.14078v1" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>whether the knowledge of LLMs regarding states of the world is more complete than their knowledge of policies for accomplishing daily tasks.</li>
<li>LLMs provide prior common sense beliefs of the world that can be used to sample likely states.</li>
<li>MCTS summarizes the useful information in searching the likely states through the estimated Q value (the expected reward after taking action) so as to make a reasonable decision.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230709221701427.png" alt="image-20230709221701427" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p>&ldquo;Collaborating with language models for embodied reasoning&rdquo;, <em>NeurIPS, Feb 2022</em>. [<a href="https://arxiv.org/abs/2302.00763v1" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>One major issue is that LSLMs are not embodied or grounded. They do not have a way to directly take actions in embodied environments, or of knowing what is happening in an environment.</li>
<li>The Planner is the LSLM—it reads the task description, does any required logical reasoning, and breaks the problem down into a sequence of simple instructions.</li>
<li>These instructions are passed to the Actor, which is an RL agent programmed to complete a small set of simple instructions in the environment.</li>
<li>Finally, to complete the feedback loop, we have the Reporter, which observes the environment and reports information back to the Planner so it can adjust the instructions it issues.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230710181542462.png" alt="image-20230710181542462" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>LLM-Brain</strong>: &ldquo;LLM as A Robotic Brain: Unifying Egocentric Memory and Control&rdquo;, arXiv, Apr 2023. [<a href="https://arxiv.org/abs/2304.09349v1" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>The LLM-Brain framework integrates multiple multimodal language models for robotic tasks















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711010819272.png" alt="image-20230711010819272" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Eye (VLM): BLIP2 [13], a VQA model;</li>
<li>Nerve (LLM): ChatGPT</li>
<li>Brain (LLM): ChatGPT action space: move_forward, turn_left, turn_right, stop</li>
<li>Active Exploration and Embodied Question Answering</li>
</ul>
</li>
<li>
<p><strong>Socratic</strong>: &ldquo;Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language&rdquo;, <em>arXiv, Apr 2021</em>. [<a href="https://arxiv.org/abs/2204.00598" target="_blank" rel="noopener">Paper</a>] [<a href="https://socraticmodels.github.io/#code" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://socraticmodels.github.io/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>different large pretrained models store different forms of commonsense knowledge</li>
<li>multiple pretrained models may be composed zero-shot i.e., via multimodalinformed prompting,</li>
<li>formulating video understanding as a reading comprehension problem</li>
<li>activity $=f_{\mathrm{LM}}\left(f_{\mathrm{VLM}}\left(f_{\mathrm{LM}}\left(f_{\mathrm{ALM}}\left(f_{\mathrm{LM}}\left(f_{\mathrm{VLM}}(\right.\right.\right.\right.\right.$$video$$\left.\left.\left.\left.\left.)\right)\right)\right)\right)\right)$</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711012319689.png" alt="image-20230711012319689" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>Code-As-Policies</strong>: &ldquo;Code as Policies: Language Model Programs for Embodied Control&rdquo;, <em>arXiv, Sept 2022</em>. [<a href="https://arxiv.org/abs/2209.07753" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/google-research/google-research/tree/master/code_as_policies" target="_blank" rel="noopener">Colab</a>] [<a href="https://code-as-policies.github.io/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>This leads us to ask: how can LLMs be applied beyond just planning a sequence of skills?</li>
<li>Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230710104335405.png" alt="image-20230710104335405" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>ProgPrompt</strong>: &ldquo;Generating Situated Robot Task Plans using Large Language Models&rdquo;, <em>arXiv, Sept 2022</em>. [<a href="https://arxiv.org/abs/2209.11302" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/progprompt/progprompt" target="_blank" rel="noopener">Github</a>] [<a href="https://progprompt.github.io/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>introduces situated-awareness in LLM-based robot task planning (program with assert)</li>
<li>open-loop without any environment interaction</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715001819415.png" alt="image-20230715001819415" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>Instruct2Act</strong>: &ldquo;Mapping Multi-modality Instructions to Robotic Actions with Large Language Model&rdquo;, <em>arXiv, May 2023</em>. [<a href="https://arxiv.org/pdf/2305.11176.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/OpenGVLab/Instruct2Act" target="_blank" rel="noopener">Pytorch Code</a>]</p>
<ul>
<li>CaP is restricted to what the perception APIs can provide and struggle to interpret longer and more complex commands due to the high precision requirements of the code.</li>
<li>Perception with off-the-shelf Foundation Models: Segment Anything Model (SAM) accurately locates candidate objects, and CLIP classifies them.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714221335753.png" alt="image-20230714221335753" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>Say-Can</strong>: &ldquo;Do As I Can, Not As I Say: Grounding Language in Robotic Affordances&rdquo;, <em>arXiv, Apr 2021</em>. [<a href="https://arxiv.org/abs/2204.01691" target="_blank" rel="noopener">Paper</a>] [<a href="https://say-can.github.io/#open-source" target="_blank" rel="noopener">Colab</a>] [<a href="https://say-can.github.io/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230710102809920.png" alt="image-20230710102809920" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li></li>
</ul>
<hr>
<h1 id="manipulation">Manipulation</h1>
<ul>
<li><strong>ProgramPort</strong>:&ldquo;Programmatically Grounded, Compositionally Generalizable Robotic Manipulation&rdquo;, <em>ICLR, Apr 2023</em>, [<a href="https://arxiv.org/abs/2304.13826" target="_blank" rel="noopener">Paper</a>] [<a href="https://progport.github.io/" target="_blank" rel="noopener">Website</a>]
<ul>
<li>Given the natural language instruction, we first use a Combinatory Categorial Grammar (CCG)</li>
<li>The program consists of functional modules that are either visual grounding modules (e.g., locate all objects of a given category) or action policies (e.g., produce a control parameter).</li>
<li>This enables us to directly leverage a pretrained VL model to ground singular, independent categories or attribute descriptors to their corresponding pixels, and thus disentangles the learning of visual grounding and action policies</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715211647390.png" alt="image-20230715211647390" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
</ul>
<ul>
<li>
<p><strong>DIAL</strong>:&ldquo;Robotic Skill Acquistion via Instruction Augmentation with Vision-Language Models&rdquo;, <em>arXiv, Nov 2022</em>, [<a href="https://arxiv.org/abs/2211.11736" target="_blank" rel="noopener">Paper</a>] [<a href="https://instructionaugmentation.github.io/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409120027347.png" alt="image-20230409120027347" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>CLIP Model</li>
<li>Few Shot/Fine-tuning</li>
<li>Manipulation trajectories</li>
</ul>
</li>
<li>
<p><strong>CLIP-Fields</strong>:&ldquo;CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory&rdquo;, <em>arXiv, Oct 2022</em>, [<a href="https://arxiv.org/abs/2210.05663" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/notmahi/clip-fields" target="_blank" rel="noopener">PyTorch Code</a>] [<a href="https://mahis.life/clip-fields/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>weakly supervised semantic neural fields, called CLIP-Fields.</li>
<li>dataset comes from LLM/MMM</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409211255049.png" alt="image-20230409211255049" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>$g(x, y, z) : R_3 → R_d$ serves as a generic differentiable spatial database.  trained.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409211327053.png" alt="image-20230409211327053" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>CLIP BERT</li>
<li>EVALUATION:  Instance and semantic segmentation in scene images.  for visual encoder</li>
<li>EVALUATION:   Semantic Navigation on Robot with CLIP-Fields as Semantic-Spatial Memory.  maximizing their similarity.</li>
</ul>
</li>
<li>
<p><strong>LaTTe</strong>: &ldquo;LaTTe: Language Trajectory TransformEr&rdquo;, <em>arXiv, Aug 2022</em>. [<a href="https://arxiv.org/abs/2208.02918" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/arthurfenderbucker/NL_trajectory_reshaper" target="_blank" rel="noopener">TensorFlow Code</a>] [<a href="https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/robot-language/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409122254407.png" alt="image-20230409122254407" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>BERT CLIP</li>
<li>trained and evaluated the model described in Section III over a dataset containing 100k examples of procedurally generated trajectory modification.</li>
<li>Trajectory reshaping obeying user’s constraints.</li>
</ul>
</li>
<li>
<p><strong>Robots Enact Malignant Stereotypes</strong>: &ldquo;Robots Enact Malignant Stereotypes&rdquo;, <em>FAccT, Jun 2022</em>. [<a href="https://arxiv.org/abs/2207.11569" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/ahundt/RobotsEnactMalignantStereotypes" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://sites.google.com/view/robots-enact-stereotypes/home" target="_blank" rel="noopener">Website</a>] [<a href="https://www.washingtonpost.com/technology/2022/07/16/racist-robots-ai/" target="_blank" rel="noopener">Washington Post</a>] [<a href="https://www.wired.com/story/how-to-stop-robots-becoming-racist/" target="_blank" rel="noopener">Wired</a>] (code access on request)</p>
<ul>
<li>experiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy.</li>
<li>Ethical experiments on LLM/MMM</li>
</ul>
</li>
<li>
<p><strong>ATLA</strong>: &ldquo;Leveraging Language for Accelerated Learning of Tool Manipulation&rdquo;, <em>CoRL, Jun 2022</em>. [<a href="https://arxiv.org/abs/2206.13074" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>combining linguistic information and meta-learning significantly accelerates tool learning</li>
<li>tools $ \mathscr{T} = { \tau_i }<em>{i=1}^K$     corresponding language descriptions $L_i = {l</em>{ij}}<em>{j=1}^{N_i}\ l</em>{ij} \in  \mathscr{L}$ by LLM</li>
<li>Goal $\pi_{\theta}:  \mathscr{O} \times  \mathscr{L} \rightarrow  \mathscr{A}$ that can be quickly fine-tuned at test time.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230410005237562.png" alt="image-20230410005237562" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>GPT</li>
<li>RL: PPO Meta-training</li>
</ul>
</li>
<li>
<p><strong>ZeST</strong>: &ldquo;Can Foundation Models Perform Zero-Shot Task Specification For Robot Manipulation?&rdquo;, <em>L4DC, Apr 2022</em>. [<a href="https://arxiv.org/abs/2204.11134" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409120237050.png" alt="image-20230409120237050" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Zero Shot</li>
<li>CLIP Model</li>
<li>Similarity calculation</li>
<li>As Reward Function in offline RL</li>
</ul>
</li>
<li>
<p><strong>LSE-NGU</strong>: &ldquo;Semantic Exploration from Language Abstractions and Pretrained Representations&rdquo;, <em>arXiv, Apr 2022</em>. [<a href="https://arxiv.org/abs/2204.05080" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>Encourage to do exploration $max\ E[\sum_{t=0}^H\gamma^t(r_t^e+\beta r_t^i)]$</li>
<li>exploration method calculates the intrinsic reward from $O_L$ or $O_V$ .</li>
<li>CLIP Bert ALM</li>
<li>Zero-Shot for calculating the intrinsic reward (NGP: L2 distances between the current state and the k-nearest neighbor representations stored in the memory buffer, RND:  mean squared error $|| f_V(O_V) - \hat{f_V}(O_V)|| $ , $f_V(O_V)$ is trainable, $\hat{f_V}(O_V)$ is pre-trained model)</li>
</ul>
</li>
<li>
<p><strong>Embodied-CLIP</strong>: &ldquo;Simple but Effective: CLIP Embeddings for Embodied AI &ldquo;, <em>CVPR, Nov 2021</em>. [<a href="https://arxiv.org/abs/2111.09888" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/allenai/embodied-clip" target="_blank" rel="noopener">Pytorch Code</a>]</p>
<ul>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409144829744.png" alt="image-20230409144829744" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Embodied AI tasks: agents that learn to navigate and interact with their environments.</li>
<li>CLIP for visual encoder</li>
<li>RL: PPO</li>
</ul>
</li>
<li>
<p><strong>CLIPort</strong>: &ldquo;CLIPort: What and Where Pathways for Robotic Manipulation&rdquo;, <em>CoRL, Sept 2021</em>. [<a href="https://arxiv.org/abs/2109.12098" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/cliport/cliport" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://cliport.github.io/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230410010919998.png" alt="image-20230410010919998" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Transporter for Pick-and-Place: The same architecture has three different networks $f_{pick}, \Psi_{query}, \Psi_{key}$</li>
<li>CLIP</li>
<li>Trajectory Imitative learning</li>
</ul>
</li>
<li>
<p><strong>R3M</strong>:&ldquo;R3M: A Universal Visual Representation for Robot Manipulation&rdquo;, * arXiv, Nov 2022*, [<a href="https://arxiv.org/abs/2203.12601" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/facebookresearch/r3m" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://tinyurl.com/robotr3m" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>can visual representations pre-trained on diverse human videos enable efficient downstream learning of robotic manipulation skills?</li>
<li>(1) time contrastive learning to learn a representation that captures temporal dynamics, (2) video-language alignment to capture semantically relevant features of the scene, and (3) L1 and L2 penalties to encourage sparsity.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230713210643467.png" alt="image-20230713210643467" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>VIP</strong>:&ldquo;Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training&rdquo;, <em>ICLR, 2023</em>, [<a href="https://arxiv.org/abs/2210.00030" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/facebookresearch/vip" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://sites.google.com/view/vip-rl" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>Pre-trained on large-scale, in-the-wild human videos, frozen VIP network can provide visual reward and representation for downstream unseen robotics tasks and enable diverse visuomotor control strategies without any task-specific fine-tuning.</li>
</ul>
</li>
<li>
<p><strong>LIV</strong>:&ldquo;LIV: Language-Image Representations and Rewards for Robotic Control&rdquo;, <em>arXiv, Jun 2023</em>, [<a href="https://arxiv.org/abs/2306.00958" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/penn-pal-lab/LIV" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://penn-pal-lab.github.io/LIV/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>a multi-modal representation that implicitly encodes a universal value function for tasks specified as language or image goals.</li>
<li>pretrain and can be fine-tuned</li>
<li>LIV’s multi-modal value representations enable diverse visuomotor control applications</li>
</ul>
</li>
<li>
<p><strong>LILAC</strong>:&ldquo;No, to the Right – Online Language Corrections for Robotic Manipulation via Shared Autonomy&rdquo;, <em>arXiv, Jan 2023</em>, [<a href="https://arxiv.org/abs/2301.02555" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/Stanford-ILIAD/lilac" target="_blank" rel="noopener">Pytorch Code</a>]</p>
<ul>
<li>human gives an instruction, then the robot executes autonomously &ndash; lack of adaptivity</li>
<li>natural language corrections</li>
<li>Learning from Language &amp; Demonstrations</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230713163941618.png" alt="image-20230713163941618" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>NLMap</strong>:&ldquo;Open-vocabulary Queryable Scene Representations for Real World Planning&rdquo;, <em>arXiv, Oct 2023</em>, [<a href="https://arxiv.org/abs/2209.09874" target="_blank" rel="noopener">Paper</a>] [<a href="https://nlmap-saycan.github.io/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>establishes a natural language queryable scene representation with VLMs(CLIP VLiD)</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714005719623.png" alt="image-20230714005719623" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>NLMap + SayCan</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714005745857.png" alt="image-20230714005745857" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>VIMA</strong>:&ldquo;VIMA: General Robot Manipulation with Multimodal Prompts&rdquo;, <em>arXiv, Oct 2022</em>, [<a href="https://arxiv.org/abs/2210.03094" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/vimalabs/VIMA" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://vimalabs.github.io/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>a transformer-based generalist robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409213812956.png" alt="image-20230409213812956" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Mask R-CNN ViT for T5 Encoder</li>
<li>Build his own MMM</li>
<li>Trajectory Imitative learning on discretization action space</li>
</ul>
</li>
<li>
<p><strong>Perceiver-Actor</strong>:&ldquo;A Multi-Task Transformer for Robotic Manipulation&rdquo;, <em>CoRL, Sep 2022</em>. [<a href="https://peract.github.io/paper/peract_corl2022.pdf" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/peract/peract" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://peract.github.io/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>PERACT encodes a sequence of RGB-D voxel(3D info) patches and predicts discretized translations,</li>
<li>PERACT is essentially a classifier trained with supervised learning to detect actions</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230409220107716.png" alt="image-20230409220107716" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Language Encoder: CLIP</li>
<li>Voxel Encoder pre-trained 3D convolution network</li>
<li>Build his own MMM</li>
<li>Trajectory Imitative learning</li>
</ul>
</li>
<li>
<p><strong>LLM-GROP</strong>:&ldquo;Task and Motion Planning with Large Language Models for Object Rearrangement&rdquo;, <em>arXiv, May 2023</em>. [<a href="https://arxiv.org/pdf/2303.06247" target="_blank" rel="noopener">Paper</a>] [<a href="https://sites.google.com/view/llm-grop" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>Multi-object rearrangement</li>
<li>How does a robot figure out a fork should be placed on the left of a plate and a knife on the right? Considerable commonsense knowledge is needed.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230713203801153.png" alt="image-20230713203801153" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p>&ldquo;Towards a Unified Agent with Foundation Models&rdquo;, <em>ICLR, 2023</em>. [<a href="https://www.semanticscholar.org/paper/TOWARDS-A-UNIFIED-AGENT-WITH-FOUNDATION-MODELS-Palo-Byravan/67188a50e1d8a601896f1217451b99f646af4ac8" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>LLM + VLM + Grounding Instructions into Actions(trained from scratch within an RL loop)</li>
<li>get inner reward: CLIP computing the similarity, as dot product, between observations and text descriptions.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711112333962.png" alt="image-20230711112333962" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>ELLM</strong>:&ldquo;Guiding Pretraining in Reinforcement Learning with Large Language Models&rdquo;, <em>arXiv, Feb 2023</em>. [<a href="https://arxiv.org/pdf/2302.06692.pdf" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>Intrinsically motivated exploration methods address this limitation by rewarding agents for visiting novel states or transitions, but these methods offer limited benefits in large environments where most discovered novelty is irrelevant for downstream tasks.</li>
<li>Reward design for Intrinsically motivated exploration.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230709201834168.png" alt="image-20230709201834168" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>The reward is determined by the cosine similarity between the transition and each sub_goal. The closer it is to a certain goal, the greater the reward.</li>
</ul>
</li>
<li>
<p>&ldquo;Language Instructed Reinforcement Learning for Human-AI Coordination&rdquo;, <em>arXiv, Jun 2023</em>. [<a href="https://arxiv.org/pdf/2304.07297" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>multi-agent reinforcement learning (RL) often converges to different equilibria from the ones that humans prefer.</li>
<li>use pretrained large language models to generate a prior policy conditioned on the human instruction and use the prior to regularize the RL objective</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714210709287.png" alt="image-20230714210709287" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p>&ldquo;Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents&rdquo;, <em>arXiv, Jun 2023</em>. [<a href="https://arxiv.org/pdf/2306.03314" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>The objective of this paper is to explore and demonstrate the potential of having multiple agents within a black-box environment to enhance collaboration and problem-solving.</li>
<li>The specific goals include: introducing a general framework that paves the way for the creation of more powerful AGI models;</li>
<li>introducing an adaptive and dynamic system that can adjust itself to suit the tasks at hand;</li>
<li>and exploring the possibility of using multiple LLMs in a collaborative manner.</li>
</ul>
</li>
<li>
<p><strong>Co-LLM-Agents</strong>:&ldquo;Building Cooperative Embodied Agents Modularly with Large Language Models&rdquo;, <em>arXiv, Jul 2023</em>. [<a href="https://arxiv.org/pdf/2307.02485" target="_blank" rel="noopener">Paper</a>] [<a href="https://vis-www.cs.umass.edu/Co-LLM-Agents/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>investigate whether LLMs can help build cooperative embodied agents that can collaborate with other agents and humans to accomplish complex tasks through collaborative planning and communication</li>
<li>human subjects to perform the experiments under four scenarios: cooperating with the HP Agent2 , LLM Agent, LLM Agent w/o communication, and doing the task alone.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230716170146348.png" alt="image-20230716170146348" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p>&ldquo;Wireless Multi-Agent Generative AI: From Connected Intelligence to Collective Intelligence&rdquo;, <em>arXiv, Jul 2023</em>. [<a href="https://arxiv.org/pdf/2307.02757" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>cloud-based LLMs vs On-device LLMs MAS</li>
<li>power allocation on mobile users to minimize network power consumption</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230716171126769.png" alt="image-20230716171126769" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p>&ldquo;Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems&rdquo;, <em>arXiv, Jul 2023</em>. [<a href="https://arxiv.org/pdf/2307.06187" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>anchor our methodology on the MAPE-K model, which is renowned for its robust support in monitoring, analyzing, planning, and executing system adaptations in response to dynamic environments</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230716172856825.png" alt="image-20230716172856825" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>VoxPoser</strong>:&ldquo;VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models&rdquo;, <em>arXiv, Jul 2023</em>. [<a href="extension://idghocbbahafpfhjnfhpbfbmpegphmmp/assets/pdf/web/viewer.html?file=https%3A%2F%2Fvoxposer.github.io%2Fvoxposer.pdf">Paper</a>] [<a href="https://voxposer.github.io/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>extracts language-conditioned affordances and constraints from LLMs and grounds them to the perceptual space using VLMs, using a code interface and without additional training to either component.</li>
<li>zero-shot</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711121324849.png" alt="image-20230711121324849" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>DEPS</strong>:&ldquo;Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents&rdquo;, <em>arXiv, Feb 2023</em>. [<a href="https://arxiv.org/abs/2302.01560" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/CraftJarvis/MC-Planner" target="_blank" rel="noopener">Pytorch Code</a>]</p>
<ul>
<li>MC open-world</li>
<li>LLM for plan, RL for manipulation (MineCLIP)</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711122150969.png" alt="image-20230711122150969" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>Plan4MC</strong>:&ldquo;Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks&rdquo;, <em>arXiv, Mar 2023</em>. [<a href="https://arxiv.org/abs/2303.16563" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/PKU-RL/Plan4MC" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://sites.google.com/view/plan4mc" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>Static Stochastic Choice</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230711122243542.png" alt="image-20230711122243542" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>VOYAGER</strong>:&ldquo;VOYAGER: An Open-Ended Embodied Agent with Large Language Models&rdquo;, <em>arXiv, May 2023</em>. [<a href="https://arxiv.org/abs/2305.16291" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/MineDojo/Voyager" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://voyager.minedojo.org/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>an automatic curriculum that maximizes exploration</li>
<li>an ever-growing skill library of executable code for storing and retrieving complex behaviors</li>
<li>a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230714194824768.png" alt="image-20230714194824768" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
</ul>
<hr>
<h1 id="instructions-and-navigation">Instructions and Navigation</h1>
<ul>
<li>
<p><strong>LM-Nav</strong>: &ldquo;Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action&rdquo;, <em>arXiv, July 2022</em>. [<a href="https://arxiv.org/abs/2207.04429" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/blazejosinski/lm_nav" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://sites.google.com/view/lmnav" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230410013423928.png" alt="image-20230410013423928" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>LLM: GPT-3 parse textual instructions into a sequence of landmarks ($t$).</li>
<li>VLM: CLIP compute likelihood that image $i_k$ corresponds to the string $t$</li>
<li>VNM: ViNG  uses a goal-conditioned distance function to infer connectivity between the set of raw observations and constructs a topological graph</li>
<li>Navigation</li>
</ul>
</li>
<li>
<p><strong>VLMaps</strong>: &ldquo;Visual Language Maps for Robot Navigation&rdquo;, <em>arXiv, Mar 2023</em>. [<a href="https://arxiv.org/abs/2210.05714" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/vlmaps/vlmaps" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://vlmaps.github.io/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>fuses pretrained visual-language features with a 3D reconstruction of the physical world</li>
<li>Our goal is to build a spatial visual-language map representation, in which landmarks (“the sofa”) or spatial references (“between the sofa and the TV”) can be directly localized using natural language.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230713214658725.png" alt="image-20230713214658725" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>CoW</strong>: &ldquo;CLIP on Wheels: Zero-Shot Object Navigation as Object Localization and Exploration&rdquo;, <em>arXiv, Mar 2022</em>. [<a href="https://arxiv.org/abs/2203.10421" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>A simple CoW, with CLIP-based object localization and classical exploration—and no additional training—matches the navigation efficiency of a state-of-the-art ZSON method</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715212626119.png" alt="image-20230715212626119" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>ADAPT</strong>: &ldquo;ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts&rdquo;, <em>CVPR, May 2022</em>. [<a href="https://arxiv.org/abs/2205.15509" target="_blank" rel="noopener">Paper</a>]</p>
<ul>
<li>The image sub-prompt is a single-view observation that highlights a significant visual object or location.</li>
<li>The text sub-prompt is a phrase related to the object, describing an action</li>
<li>Before utilizing these prompts, they need to be retrieved from a pre-built action prompt database, which correlates with the instructions.</li>
<li>These prompts are then encoded via a prompt encoder, and the output feature is concatenated with the original instruction feature.</li>
<li>The concatenated feature, along with the visual feature, is then fed into a multi-layer transformer to make the action decision.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715213931916.png" alt="image-20230715213931916" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p>&ldquo;The Unsurprising Effectiveness of Pre-Trained Vision Models for Control&rdquo;, <em>ICML, Mar 2022</em>. [<a href="https://arxiv.org/abs/2203.03580" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/sparisi/pvr_habitat" target="_blank" rel="noopener">Pytorch Code</a>] [<a href="https://sites.google.com/view/pvr-control" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>It was found that pre-trained visual representations (PVRs) trained on entirely out-of-domain datasets can compete with or surpass ground-truth state features for policy training.</li>
<li>The study also revealed the superiority of self-supervised learning (SSL) over supervised learning in providing better features for control policies.</li>
<li>Residual Network, Momentum Contrast, CLIP, Random Features, From Scratch, Ground-Truth Features</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715214826827.png" alt="image-20230715214826827" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>Recurrent VLN-BERT</strong>: &ldquo;A Recurrent Vision-and-Language BERT for Navigation&rdquo;, <em>CVPR, Jun 2021</em> [<a href="https://arxiv.org/abs/2011.13922" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/YicongHong/Recurrent-VLN-BERT" target="_blank" rel="noopener">Pytorch Code</a>]</p>
<ul>
<li>reinforcement learning (RL) and imitation learning (IL)</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715215714278.png" alt="image-20230715215714278" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p><strong>VLN-BERT</strong>: &ldquo;Improving Vision-and-Language Navigation with Image-Text Pairs from the Web&rdquo;, <em>ECCV, Apr 2020</em> [<a href="https://arxiv.org/abs/2004.14973" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/arjunmajum/vln-bert" target="_blank" rel="noopener">Pytorch Code</a>]</p>
<ul>
<li>VLN-BERT, a visiolinguistic transformer-based model for scoring the compatibility between an instruction (‘&hellip;stop at the brown sofa’) and a sequence of panoramic RGB images captured by the agent.</li>
<li>We demonstrate that pretraining VLN-BERT on image-text pairs from the web before fine-tuning on embodied path-instruction data significantly improves performance on VLN</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715220528690.png" alt="image-20230715220528690" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
<li>
<p>&ldquo;Interactive Language: Talking to Robots in Real Time&rdquo;, <em>arXiv, Oct 2022</em> [<a href="https://arxiv.org/abs/2210.06407" target="_blank" rel="noopener">Paper</a>] [<a href="https://interactive-language.github.io/" target="_blank" rel="noopener">Website</a>]</p>
<ul>
<li>The dataset we release comprises nearly 600,000 language-labeled trajectories, an order of magnitude larger than prior available datasets (video, actions, language).</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230715221039329.png" alt="image-20230715221039329" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
</li>
</ul>
<hr>
<h1 id="simulation-frameworks">Simulation Frameworks</h1>
<ul>
<li><strong>MineDojo</strong>: &ldquo;MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge&rdquo;, <em>arXiv, Jun 2022</em>. [<a href="https://arxiv.org/abs/2206.08853" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/MineDojo/MineDojo" target="_blank" rel="noopener">Code</a>] [<a href="https://minedojo.org/" target="_blank" rel="noopener">Website</a>] [<a href="https://minedojo.org/knowledge_base.html" target="_blank" rel="noopener">Open Database</a>]</li>
<li><strong>Habitat 2.0</strong>: &ldquo;Habitat 2.0: Training Home Assistants to Rearrange their Habitat&rdquo;, <em>NeurIPS, Dec 2021</em>. [<a href="https://arxiv.org/abs/2106.14405" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/facebookresearch/habitat-sim" target="_blank" rel="noopener">Code</a>] [<a href="https://aihabitat.org/" target="_blank" rel="noopener">Website</a>]</li>
<li><strong>BEHAVIOR</strong>: &ldquo;BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments&rdquo;, <em>CoRL, Nov 2021</em>. [<a href="https://arxiv.org/abs/2108.03332" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/StanfordVL/behavior" target="_blank" rel="noopener">Code</a>] [<a href="https://behavior.stanford.edu/" target="_blank" rel="noopener">Website</a>]</li>
<li><strong>iGibson 1.0</strong>: &ldquo;iGibson 1.0: a Simulation Environment for Interactive Tasks in Large Realistic Scenes&rdquo;, <em>IROS, Sep 2021</em>. [<a href="https://arxiv.org/abs/2012.02924" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/StanfordVL/iGibson" target="_blank" rel="noopener">Code</a>] [<a href="https://svl.stanford.edu/igibson/" target="_blank" rel="noopener">Website</a>]</li>
<li><strong>ALFRED</strong>: &ldquo;ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks&rdquo;, <em>CVPR, Jun 2020</em>. [<a href="https://arxiv.org/abs/1912.01734" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/askforalfred/alfred" target="_blank" rel="noopener">Code</a>] [<a href="https://askforalfred.com/" target="_blank" rel="noopener">Website</a>]</li>
<li><strong>BabyAI</strong>: &ldquo;BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning&rdquo;, <em>ICLR, May 2019</em>. [<a href="https://openreview.net/pdf?id=rJeXCo0cYX" target="_blank" rel="noopener">Paper</a>] [<a href="https://github.com/mila-iqia/babyai/tree/iclr19" target="_blank" rel="noopener">Code</a>]</li>
</ul>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/llm/">LLM</a>
  
  <a class="badge badge-light" href="/tag/rl/">RL</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fexample.com%2Fpost%2Fllm-for-rl%2F&amp;text=LLM%2FMMM&#43;for&#43;RL" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fexample.com%2Fpost%2Fllm-for-rl%2F&amp;t=LLM%2FMMM&#43;for&#43;RL" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
        
      
      <li>
        <a href="mailto:?subject=LLM%2FMMM%20for%20RL&amp;body=https%3A%2F%2Fexample.com%2Fpost%2Fllm-for-rl%2F" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fexample.com%2Fpost%2Fllm-for-rl%2F&amp;title=LLM%2FMMM&#43;for&#43;RL" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="whatsapp://send?text=LLM%2FMMM&#43;for&#43;RL%20https%3A%2F%2Fexample.com%2Fpost%2Fllm-for-rl%2F" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Fexample.com%2Fpost%2Fllm-for-rl%2F&amp;title=LLM%2FMMM&#43;for&#43;RL" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  
    



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://example.com/"><img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu7ec2cd83d6766f6978214d3ef6436361_1727589_270x270_fill_lanczos_center_3.png" alt="Haofei Hou"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://example.com/">Haofei Hou</a></h5>
      <h6 class="card-subtitle">Undergraduate</h6>
      <p class="card-text">My research interests include Robotics, Computational Cognitive Science, Artificial Intelligence.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:yuechuhaoxi020609@outlook.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/yuechuhaoxi020609" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  
















  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2023 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js"></script>




  

  
  

  













  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  







<script id="page-data" type="application/json">{"use_headroom":true}</script>



  <script src="/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js" type="module"></script>









  
  


<script src="/en/js/wowchemy.min.e8ee06ba8371980ffde659871dd593b0.js"></script>







  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js" type="module"></script>


















</body>
</html>
