<!DOCTYPE html>
<!-- This site was created with Wowchemy. https://www.wowchemy.com -->
<!-- Last Published: April 14, 2023 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.7.0 for Hugo" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css" integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.0f229d4b7ebad1917a9a357cba2effab.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  


























  
  
  






  <meta name="author" content="Haofei Hou" />





  

<meta name="description" content="A quick read of ToM&#39;s literature around article &#34;Artificial Social Intelligence - A Comparative and Holistic View&#34;, this blog is the summary. Humans are distinguished from their closest primate cousins by their social cognitive skills as opposed to their physical counterparts. Article believe that artificial social intelligence (ASI) will play a crucial role in shaping the future of artificial intelligence (AI). This article begins with a review of ASI from a cognitive science standpoint, including social perception, theory of mind (ToM), and social interaction. Next, article examine the recently-emerged computational counterpart in the AI community. Finally, this article provide an in-depth discussion on topics related to ASI." />



<link rel="alternate" hreflang="en-us" href="https://example.com/post/tom-summary/" />
<link rel="canonical" href="https://example.com/post/tom-summary/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hu88cdd4b7a12988e7d5bb56be50e4f21a_10097_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hu88cdd4b7a12988e7d5bb56be50e4f21a_10097_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#1565c0" />










  






<meta property="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://example.com/post/tom-summary/featured.png" />
<meta property="og:site_name" content="Haofei Hou | HUST" />
<meta property="og:url" content="https://example.com/post/tom-summary/" />
<meta property="og:title" content="Artificial Social Intelligence - A Comparative and Holistic View | Haofei Hou | HUST" />
<meta property="og:description" content="A quick read of ToM&#39;s literature around article &#34;Artificial Social Intelligence - A Comparative and Holistic View&#34;, this blog is the summary. Humans are distinguished from their closest primate cousins by their social cognitive skills as opposed to their physical counterparts. Article believe that artificial social intelligence (ASI) will play a crucial role in shaping the future of artificial intelligence (AI). This article begins with a review of ASI from a cognitive science standpoint, including social perception, theory of mind (ToM), and social interaction. Next, article examine the recently-emerged computational counterpart in the AI community. Finally, this article provide an in-depth discussion on topics related to ASI." /><meta property="og:image" content="https://example.com/post/tom-summary/featured.png" /><meta property="og:locale" content="en-us" />

  
    <meta
      property="article:published_time"
      content="2023-04-14T00:00:00&#43;00:00"
    />
  
  
    <meta property="article:modified_time" content="2023-04-14T00:00:00&#43;00:00">
  






    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://example.com/post/tom-summary/"
  },
  "headline": "Artificial Social Intelligence - A Comparative and Holistic View",
  
  "image": [
    "https://example.com/post/tom-summary/featured.png"
  ],
  
  "datePublished": "2023-04-14T00:00:00Z",
  "dateModified": "2023-04-14T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Haofei Hou"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Haofei Hou | HUST",
    "logo": {
      "@type": "ImageObject",
      "url": "https://example.com/media/icon_hu88cdd4b7a12988e7d5bb56be50e4f21a_10097_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "A quick read of ToM's literature around article \"Artificial Social Intelligence - A Comparative and Holistic View\", this blog is the summary. Humans are distinguished from their closest primate cousins by their social cognitive skills as opposed to their physical counterparts. Article believe that artificial social intelligence (ASI) will play a crucial role in shaping the future of artificial intelligence (AI). This article begins with a review of ASI from a cognitive science standpoint, including social perception, theory of mind (ToM), and social interaction. Next, article examine the recently-emerged computational counterpart in the AI community. Finally, this article provide an in-depth discussion on topics related to ASI."
}
</script>

  

  




  
  
  

  
  

  


  
  <title>Artificial Social Intelligence - A Comparative and Holistic View | Haofei Hou | HUST</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="dd18eae3f9092f1c3bf8080798608bbd" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Haofei Hou | HUST</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Haofei Hou | HUST</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#featured"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#projects"><span>Projects</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#posts"><span>Posts</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article">

  






















  
  


<div class="article-container pt-3">
  <h1>Artificial Social Intelligence - A Comparative and Holistic View</h1>

  

  


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      <a href="/authors/admin/">Haofei Hou</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Apr 14, 2023
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    27 min read
  </span>
  

  
  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/category/summary/">Summary</a></span>
  

</div>

  





</div>


<div class="article-header container featured-image-wrapper mt-4 mb-4" style="max-width: 523px; max-height: 287px;">
  <div style="position: relative">
    <img src="/post/tom-summary/featured_huf87d9248615d5ba335823e36fb555a4a_11138_1200x2500_fit_q75_h2_lanczos_3.webp" width="523" height="287" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <h1 id="abstract">Abstract</h1>
<ul>
<li>humans possess a high social intelligence—the intelligence that senses social events, infers the goals and intents of others, and facilitates social interaction.</li>
<li>cognitive science standpoint: social perception, theory of mind (ToM), and social interaction.</li>
</ul>
<h1 id="dawn-of-artificial-social-intelligence">Dawn of Artificial Social Intelligence</h1>
<ul>
<li>efforts towards human-like intelligence can be divided into physical intelligence and social intelligence , analogous to the developmental psychology ideas of intuitive physics and intuitive psychology</li>
</ul>
<h2 id="unique-challenges-of-context">Unique challenges of context</h2>
<ul>
<li>ASI is distinct and challenging compared to our physical understanding of the world; it is highly context-dependent</li>
<li>Here, context could be as large as culture and common sense or as little as two friends&rsquo; shared experiences</li>
<li>it begins with nonverbal communication</li>
</ul>
<h2 id="overview-of-the-article">Overview of the article</h2>
<ul>
<li>In Section 2, which covers social perception, theory of mind, and social interaction, we begin with experimental evidence and theoretical hypotheses of human social intelligence from the standpoint of cognitive science.</li>
<li>In Section 3, we present the AI community’s computational counterpart, focused on social perception, theory of mind, and social interaction, with an added topic on social robot and cognitive architectures.</li>
<li>In Section 4, we explore significant challenges that impede the development of the ASI and recommend potential future trends.</li>
</ul>
<h1 id="human-social-intelligence">Human Social Intelligence</h1>
<ul>
<li>We concentrate on the three most important aspects of social intelligence: social perception, ToM, and social interaction.</li>
<li>Social perception is the basis for ToM and social interaction. It consists primarily of the perception of social features, such as animacy and agency, and provides low-level, automatic, instantaneous, and non-conscious visual perception. ToM, in contrast, is concerned with sophisticated, analytic, and logical cognitive reasoning, involving a general cognitive system with several essential components, including belief, intent, and desire. Social interaction emphasizes more multi-agent interactive activities, such as communication and cooperation, than social perception and ToM.</li>
</ul>
<h2 id="social-perception">Social perception</h2>
<ul>
<li>Heider-Simmel stimuli</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230412213753647.png" alt="image-20230412213753647" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Even when told explicitly that these are merely simple shapes, participants still make a rapid, spontaneous, and consistent perception of animate social agents with various complex mental states, including desires, goals, emotions, personalities, and coalitions.</li>
<li>The Heider-Simmel experiment demonstrates two essential aspects of human social perception: the perception of animacy and agency. Animacy denotes that the perceived entities are animate as opposed to inanimate (e.g., physical objects), whereas agency refers to animate beings who are goal-oriented and capable of planning to achieve their goals rationally and efficiently.</li>
</ul>
<h3 id="animacy">Animacy</h3>
<ul>
<li>In this experiment, participants were shown two small squares separated by several inches and arranged in a line.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230412215127371.png" alt="image-20230412215127371" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>In the first scenario, the first square (A) moves in a straight line until it reaches the second square (B), at which point A stops moving and B begins moving in the same direction (also called the launching effect).</li>
<li>In case two, the first square (A) approaches the second square (B). While A approaches, B moves away from A quickly and stops when it is several inches away again.</li>
<li>In the first instance, observers observe A physically causing B&rsquo;s motion</li>
<li>in the second case, A and B are perceived as alive with their own intentions.</li>
<li>spatiotemporal parameters mediate causal perceptions, such as relative velocity, speed–mass interaction, path length, and spatial and temporal gap.</li>
<li>spatiotemporal parameters mediate causal perceptions, such as relative velocity, speed–mass interaction, path length, and spatial and temporal gap.</li>
</ul>
<h3 id="agency">Agency</h3>
<ul>
<li>An agent is rationally controlled because it has an internal energy source, whereas an object is not.</li>
<li>The perception of agency is frequently studied in tandem with animacy for more complex social phenomena.</li>
<li>Gao et al[81] study a particularly salient form of perceived animacy and agency via tasks based on dynamic visual search (the Find-the-Chase task)</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230412215212952.png" alt="image-20230412215212952" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>They used two cues to evaluate the objective accuracy of such perceptions: (1) chasing subtlety—the degree to which the wolf deviates from a perfectly heat-seeking pursuit, and (2) directionality—whether and how the shapes face each other.</li>
<li>the researchers discovered that temporal dynamics could lead the visual system to either construct or actively reject interpretations of chasing</li>
<li>van Buren et al[84] depict one disc (the “wolf”) pursuing another disc (the “sheep”) amidst several distractor discs that are moving.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230412215259122.png" alt="image-20230412215259122" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>In the Unconnected condition, both lines connected distractors in pairs. In the Connected condition, however, one line connected the wolf to a distractor, and the other line connected the sheep to a different distractor. Observers in the Connected condition were markedly less likely to describe these behaviors in terms of mental state.</li>
<li>According to the outcomes of their experiments, discrete visual objects are the fundamental units of social perception.</li>
</ul>
<h3 id="summary">Summary</h3>
<ul>
<li>specific bottomup perceptual processing is specialized and difficult to be “penetrated” by higher-level cognition. This type of social perception may be at the intersection of perceptual and cognitive processing, where basic stimuli are transformed into causal, animate, or even intentional qualities, which are strongly linked to higher-level cognitive processing.</li>
</ul>
<h2 id="tom">ToM</h2>
<ul>
<li>
<p>chimpanzee Sarah was shown a brief clip of an experimenter attempting to perform simple tasks. Subsequently, Sarah observed images of several objects, one of which solved the experimenter’s</p>
</li>
<li>
<p>Their findings highlight two essential components of ToM: a representation of the affair state and a representation of an individual&rsquo;s motivational link to the state, i.e., belief and intention</p>
</li>
<li>
<p>ToM entails attributing mental states (such as beliefs, intents, or desires) to oneself and others</p>
</li>
<li>
<p>Perspective taking in an internal simulation process is one of the defining characteristics of ToM</p>
</li>
<li>
<p>The infamous Sally-Anne test[90,91] , a classic first-order false belief task, is a well-known experiment on perspective taking.</p>
</li>
<li>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414191524787.png" alt="image-20230414191524787" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
</li>
<li>
<p>Later in the development trajectory[95] is the establishment of second-order ToM, which entails predicting what one person thinks or feels about what another person thinks or feels</p>
</li>
</ul>
<h3 id="intent">Intent</h3>
<ul>
<li>In fact, research has shown that humans do not encode the entirety of action details but rather observe and interpret actions in terms of their intentions and store these interpretations for later retrieval.</li>
<li>The developmental psychology literature indicates that sixmonth-old infants view human actions as goal-directed behavior. By the age of 10 months, infants segment continuous behavior streams into discrete units that correspond to what adults would perceive as distinct goal-directed acts . After their first birthday, infants begin to comprehend that an agent may explore multiple plans to achieve a goal and choose one based on environmental conditions. 18-month-old children can deduce and reproduce an action’s intended purpose, even if the activity frequently fails to achieve the aim[102] . In addition, infants can replicate behaviors rationally and effectively based on an evaluation of the environmental restrictions, as opposed to just duplicating movements, indicating that they understand the relationships between the environment, action, and underlying intent.</li>
<li>intentions are hierarchically arranged across extensive spatiotemporal ranges as a sequence of goals . Infants are already capable of perceiving intentions on multiple levels, including concrete action goals, higher order plans, and collaborative goals</li>
</ul>
<h3 id="categorization">Categorization</h3>
<ul>
<li>Cognitive ToM can be further divided into ToM for motivation (i.e., another organism’s valuation, intention, purpose, and goal) and ToM for knowledge (i.e., another organism&rsquo;s belief states or taught schemas/scripts) .</li>
<li>Individual differences in cognitive strategies are also present. The theory-theory method and simulation-theory approach are examples of these diverse ToM strategies. The theory-theory approach may be based on a set of intrinsic rules or on causal and probabilistic reasoning models, which may be analogous to cold cognition in which mental states are inferred through intellectual processes. The simulation-theory approach relies on the individual’s own motivations and deductive reasoning.</li>
</ul>
<h2 id="social-interaction">Social interaction</h2>
<ul>
<li>equip ASI with more sophisticated human-like communication and collaboration capabilities</li>
</ul>
<h3 id="social-cues">Social cues</h3>
<ul>
<li>paralinguistic (voice prosody and non-language sounds),</li>
<li>facial expression (motion and position of facial muscles),</li>
<li>gaze (motion and position of the eyes and predicted sight-line),</li>
<li>kinematics (motion, position, and posture of the body),</li>
<li>proxemics (use of interpersonal space)</li>
</ul>
<h3 id="gaze-communication">Gaze communication</h3>
<ul>
<li>Psychological evidence suggests that eyes are stimuli with distinct “hardwired” neural pathways in the brain for their interpretation.</li>
</ul>
<h3 id="joint-attention">Joint attention</h3>
<ul>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230412215415220.png" alt="image-20230412215415220" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>layers of human gaze communication dynamics: atomic-level and event-level.</li>
<li>Event-level gaze communication refers to high-level, complex social communication events, such as non-communicative, mutual gaze, gaze aversion, gaze following, and joint attention.</li>
<li>Atomic-level gaze communication describes the granular structures of human gaze interactions, including single, mutual, avoid, refer, follow, and share.</li>
<li>Joint attention is the most advanced sort of gaze communication, as it requires two agents (1) to have the same intention to share attention on common stimuli and (2) to be aware that they are sharing a common ground.</li>
</ul>
<h3 id="pointing">Pointing</h3>
<ul>
<li>According to Tomasello, pointing is one of the earliest forms of communication exclusive to the human species (the other is pantomiming).</li>
<li>Pointing is also an indicator of particular cognitive abilities, such as being an intentional actor and having ToM</li>
<li>Bates et al and Brinck are credited with introducing the distinction between imperative pointing and declarative pointing.</li>
<li>Declarative pointing is primarily intersubjective with a signaling function, whereas imperative pointing is based on behaviorally motivated regularities and is used to request the addressee to do something for the subject.</li>
<li>Levinson developed the concept of interaction engine, which allows communication intentions to be conveyed and recognized in both linguistic and nonlinguistic encounters.</li>
</ul>
<h3 id="cooperation">Cooperation</h3>
<ul>
<li>Cooperation is a type of social interaction that is more complex than simple communication, as it requires a psychological infrastructure of shared intentionality.</li>
<li>This infrastructure is comprised of two crucial factors: (1) socialcognitive skills for creating common conceptual ground with others, such as joint attention and joint intention, and (2) prosocial motivations and norms to help and share with others</li>
<li>communication: the exchange of information between agents</li>
<li>coordination: the alignment of multiple agents towards the achievement of specific common goals through the efforts of individual agents</li>
<li>cooperation: each individual agent/robot exchanges relevant information and resources in support of each other’s goals, rather than a shared common goal,</li>
<li>collaboration: requires agents to exchange information and knowledge in support of a shared task.</li>
<li>Tomasello believes that  “shared cooperative actions” have two essential characteristics: (1) the participants have a joint goal in the sense that we (in mutual knowledge) do X together; and (2) the participants coordinate their interdependent roles—their plans and sub-plans of action, including helping one another as needed in their respective roles.</li>
<li>Tomasello also proposed a dual-level attentional structure (the shared focus of attention at a higher level, differentiated into perspectives at a lower level) and a dual-level intentional structure (shared goal with individual roles), arguing that the former is directly parallel to the latter and may ultimately derive from it.</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413183329097.png" alt="image-20230413183329097" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
<h1 id="artificial-social-intelligence">Artificial Social Intelligence</h1>
<h2 id="social-perception-in-simulated-scenarios">Social perception in simulated scenarios</h2>
<ul>
<li>Shu et al present a unified theory that describes the interrelationships between the perception of physical and social events</li>
<li>Specifically, the model learns to identify latent forces by inferring a family of potential functions capturing physical laws and value functions of agent goals, thereby projecting the animations into a sociophysical space with two psychological dimensions: an intuitive sense of whether physical laws are violated and an impression of whether an agent possesses intentions to perform goal-directed actions.</li>
<li><a href="https://doi.org/10.1016/j.cogpsych.2021.101398" target="_blank" rel="noopener">https://doi.org/10.1016/j.cogpsych.2021.101398</a></li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413190203086.png" alt="image-20230413190203086" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Tang et al investigate the problem of simultaneously perceiving physics and mind using a leash-chasing display, in which a disc (“sheep”) is being chased by another disc (“wolf”) that is physically constrained by a leash tied to a third disc (“master”). They discover that (1) an intuitive physical system, such as a leash, can significantly mitigate the detrimental effects of spatial deviation and the diminishing object-hood on perceived chasing, thereby enhancing its robustness, and (2) a mutual dependency exists between physics and mind, where disrupting one will inevitably impair the perception on the other, supporting a joint perception of physics and mind.</li>
<li><a href="http://www.stat.ucla.edu/~taogao/static/pdf/Jointly%20Perceiving%20Physics%20and%20Mind_TangGongGao_2021.pdf" target="_blank" rel="noopener">http://www.stat.ucla.edu/~taogao/static/pdf/Jointly%20Perceiving%20Physics%20and%20Mind_TangGongGao_2021.pdf</a></li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413191340154.png" alt="image-20230413191340154" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Flatland is a new experimental paradigm introduced by Shu et al for exploring social inference in physical situations. Results demonstrate that human interpretations of interactive events in Flatland can be accounted for by a computational model that combines inverse hierarchical planning with a physical simulation engine to reason about objects and agents.</li>
<li><a href="https://cocodev.fas.harvard.edu/publications/adventures-in-flatland-perceiving-social-interactions-under-physical-dynamics/2020-shu-adventures.pdf" target="_blank" rel="noopener">https://cocodev.fas.harvard.edu/publications/adventures-in-flatland-perceiving-social-interactions-under-physical-dynamics/2020-shu-adventures.pdf</a></li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413191815934.png" alt="image-20230413191815934" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413191646740.png" alt="image-20230413191646740" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Shu et al examine the perception of social interaction using decontextualized motion trajectories, in which stimuli are extracted from drone-recorded aerial films of a real-world setting. To account for human judgments of interactiveness between two moving dots and the dynamic change of such judgments over time, they construct a hierarchical model that represents interactivity using latent variables and learns the distribution of critical movement features that signal potential interactivity.</li>
<li><a href="https://doi.org/10.1111/tops.12313" target="_blank" rel="noopener">https://doi.org/10.1111/tops.12313</a></li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413204239814.png" alt="image-20230413204239814" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413192231676.png" alt="image-20230413192231676" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>To investigate the cognitive architecture of perceived animacy, Gao et al devise Bayesian models that integrate domain-specific hypotheses of social agency with domain-general cognitive constraints on sensory, memory, and attentional processing. The proposed model posits that perceived animacy combines a bottomup, feature-based, parallel search for goal-directed movements with architecturally distinct processes that make perceived animacy fast, flexible, and cognitively efficient. By distinguishing target agents from distractor objects in the “wolf-chasing-sheep” setting, they demonstrate that a Bayesian ideal observer model may explain the efficacy of human perceived animacy with realistic cognitive constraints.</li>
<li><a href="https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/cogs.12775" target="_blank" rel="noopener">The Cognitive Architecture of Perceived Animacy: Intention, Attention, and Memory (wiley.com)</a></li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413205304659.png" alt="image-20230413205304659" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
<h2 id="social-perception-in-real-world-scenarios">Social perception in real-world scenarios</h2>
<ul>
<li>Fan et al investigate the topic of inferring shared attention in their collected third-person social scene video dataset VideoCoAtt by employing a spatiotemporal neural network utilizing human gaze directions and potential target boxes extracted from the context. In their subsequent study. the authors systematically investigate the subject of human gaze communication by constructing spatiotemporal graphs for realworld social scenarios in the collected VACATION video dataset. the authors systematically investigate the subject of human gaze communication by constructing spatiotemporal graphs for realworld social scenarios in the collected VACATION video dataset.</li>
<li>[<a href="https://arxiv.org/abs/1909.02144" target="_blank" rel="noopener">1909.02144] Understanding Human Gaze Communication by Spatio-Temporal Graph Reasoning (arxiv.org)</a></li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413205604467.png" alt="image-20230413205604467" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>To jointly infer human attention, intention, and task from videos, Wei et al introduce a hierarchical model of humanattention-object (HAO) and a beam search algorithm. According to their definition, the intention consists of the human pose, attention, and objects, whereas the task is represented as a series of intentions.</li>
<li><a href="https://ieeexplore.ieee.org/document/8578809" target="_blank" rel="noopener">Where and Why are They Looking? Jointly Inferring Human Attention and Intentions in Complex Tasks | IEEE Conference Publication | IEEE Xplore</a></li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413205946790.png" alt="image-20230413205946790" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Xie et al offer an unsupervised method for localizing functional objects and predicting human intents and trajectories from surveillance footage of public places. Agents are influenced by the attractive or repulsive “fields” of functioning objects, referred to as “dark matter”. In addition to estimating the agent’s intent, the model can also derive the agent’s trajectory via agent-based Lagrangian mechanics</li>
<li><a href="https://ieeexplore.ieee.org/document/7984896" target="_blank" rel="noopener">Learning and Inferring “Dark Matter” and Predicting Human Intents and Trajectories in Videos | IEEE Journals &amp; Magazine | IEEE Xplore</a></li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413210621863.png" alt="image-20230413210621863" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Holtzen et al[137] present a method that enables robots to infer a person&rsquo;s hierarchical intent from partially observed RGB-D videos. They represent intent as a novel hierarchical, compositional, and probabilistic And-Or-Graph structure that describes a relationship between actions and plans. Human intent is inferred by reverseengineering a person’s decision-making and action-planning processes under a Bayesian probabilistic programming framework.</li>
<li><a href="https://ieeexplore.ieee.org/abstract/document/7759242" target="_blank" rel="noopener">Inferring human intent from video by sampling hierarchical plans | IEEE Conference Publication | IEEE Xplore</a></li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413214642238.png" alt="image-20230413214642238" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413214625377.png" alt="image-20230413214625377" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
<h2 id="tom-1">ToM</h2>
<ul>
<li>The computational modeling of ToM may concentrate on different components, such as belief, intent, and desire.</li>
<li>Gonzalez and Chang divide computational models of ToM into several broad categories, including <strong>Game ToM , Observational (RL) , Inverse RL , and Bayesian ToM</strong>. These models contain modules for representing the goals and desires of an agent, inferring the mental states of other agents (e.g., beliefs, goals, desires, intentions, and feelings), and integrating these goals and mentalizing computations to generate optimal policies.</li>
</ul>
<h3 id="different-tom-components-and-modeling-methods">different ToM components and modeling methods</h3>
<ul>
<li>
<p>Yuan et al jointly infer object states, robot knowledge, and human beliefs using parse graphs, which accurately identify human (false-)beliefs.</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2004.12248" target="_blank" rel="noopener">https://arxiv.org/pdf/2004.12248</a></p>
</li>
<li>
<p>To answer this Sally-Anne correctly, an agent should understand and disentangle the object state (observation from the current frame), the (accumulated) knowledge, the belief of other agents, the ground-truth/reality of the world, and importantly, the concept of false-belief.</p>
</li>
<li>
<p>Fan et al incorporate different nonverbal communication cues (e.g., gaze, human poses, and gestures) to infer agents’ mental states based solely on visual inputs. By aggregating beliefs and physical-world states, their approach effectively forms five minds during the interactions between two agents. In particular, they construct a common mind to avoid the infinite recursion commonly used in prior works. In addition, they devise a hierarchical energy-based model</p>
</li>
<li>
<p>[<a href="https://arxiv.org/abs/2104.02841" target="_blank" rel="noopener">2104.02841] Learning Triadic Belief Dynamics in Nonverbal Communication from Videos (arxiv.org)</a></p>
</li>
<li>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413224240325.png" alt="image-20230413224240325" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
</li>
<li>
<p>Arslan investigate how 5-year-olds choose and revise reasoning strategies in second-order false belief tasks by constructing two computational cognitive models of this process: an instance-based learning model and a RL model.</p>
</li>
<li>
<p><a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2017.00275/pdf" target="_blank" rel="noopener">https://www.frontiersin.org/articles/10.3389/fpsyg.2017.00275/pdf</a></p>
</li>
<li>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413224634548.png" alt="image-20230413224634548" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
</li>
<li>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414003302108.png" alt="image-20230414003302108" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
</li>
<li>
<p>Oguntola develop an interpretable modular neural framework for modeling the intentions of other observed entities, demonstrating the model&rsquo;s efficacy in a Minecraft search and rescue task. They also demonstrate that, under the right conditions, integrating interpretability can dramatically improve prediction performance.</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2104.02938" target="_blank" rel="noopener">https://arxiv.org/pdf/2104.02938</a></p>
</li>
<li>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414003958427.png" alt="image-20230414003958427" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
</li>
<li>
<p>Zeng et al suggest a brain-inspired model of belief ToM, leveraging high-level knowledge of brain regions’ functions relevant to ToM. Although tested on false belief tasks, such cognitive architecture may be difficult to motivate at the computational level</p>
</li>
<li>
<p><a href="https://doi.org/10.3389/fnbot.2020.00060" target="_blank" rel="noopener">https://doi.org/10.3389/fnbot.2020.00060</a></p>
</li>
<li>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414004300635.png" alt="image-20230414004300635" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
</li>
<li>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414004338055.png" alt="image-20230414004338055" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
</li>
<li>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414004359629.png" alt="image-20230414004359629" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
</li>
</ul>
<h3 id="bayesian-methods">Bayesian methods</h3>
<ul>
<li>Baker et al investigate the rational quantitative attribution of beliefs, desires, and percepts in human mentalizing from agents&rsquo; movement in a local spatial environment. They devise a Bayesian theory of mind (BToM) model in a partially observable Markov decision process (POMDP) setting for rational planning and state estimation, which extends classical expected-utility agent models to sequential actions in complex, partially observable domains.</li>
<li><a href="https://compdevlab.yale.edu/docs/2017/Bakeretal2017.pdf" target="_blank" rel="noopener">https://compdevlab.yale.edu/docs/2017/Bakeretal2017.pdf</a></li>
<li><a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41562-017-0064/MediaObjects/41562_2017_BFs415620170064_MOESM348_ESM.pdf" target="_blank" rel="noopener">https://static-content.springer.com/esm/art%3A10.1038%2Fs41562-017-0064/MediaObjects/41562_2017_BFs415620170064_MOESM348_ESM.pdf</a></li>
<li><a href="https://github.com/clbaker/BToM" target="_blank" rel="noopener">clbaker/BToM (github.com)</a></li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414085632574.png" alt="image-20230414085632574" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
<h3 id="rl-method">RL method</h3>
<ul>
<li>Wen et al and Moreno et al are examples of recursive reasoning models for higher-order ToM in a RL framework.</li>
<li>[<a href="https://arxiv.org/abs/1901.09207" target="_blank" rel="noopener">1901.09207] Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning (arxiv.org)</a></li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414095228222.png" alt="image-20230414095228222" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>[<a href="https://arxiv.org/abs/2102.02274" target="_blank" rel="noopener">2102.02274] Neural Recursive Belief States in Multi-Agent Reinforcement Learning (arxiv.org)</a></li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414105134026.png" alt="image-20230414105134026" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Hakimzadeh contend that RL plays a crucial role in human intuition and cognition, and theories such as the language of thought hypothesis, script theory, and Piaget’s theory of cognitive development offer complementary approaches.</li>
<li>ToM can indeed be formulated as an inverse reinforcement learning (IRL) problem, where expectations for how mental states produce behavior are represented by a RL model.</li>
<li>RL models, such as IRL and multi-agent reinforcement learning (MARL), are highly scalable but computationally intensive and less interpretable</li>
<li>Under a POMDP setting, Yuan et al argue that misalignment of values could impede group performance in cooperation; hence, communication plays a vital role during which a robot needs to serve as an effective listener and an expressive speaker. In the context of value alignment, they investigate how to foster effective bidirectional human-robot communications and propose an explainable artificial intelligence (XAI) system in which a collection of robots anticipates human values by using in-situ feedback while explaining their decisionmaking processes to users (see Fig. 11). Their XAI system integrates a cooperative communication model to infer human values associated with multiple desirable goals, mimic human mental dynamics, and predict optimal explanations using graphical models.</li>
<li><a href="https://www.science.org/stoken/author-tokens/ST-617/full" target="_blank" rel="noopener">In situ bidirectional human-robot value alignment | Science Robotics</a></li>
<li><a href="https://www.science.org/doi/suppl/10.1126/scirobotics.abm4183/suppl_file/scirobotics.abm4183_sm.pdf" target="_blank" rel="noopener">https://www.science.org/doi/suppl/10.1126/scirobotics.abm4183/suppl_file/scirobotics.abm4183_sm.pdf</a></li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414112527794.png" alt="image-20230414112527794" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414112620204.png" alt="image-20230414112620204" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
<h3 id="game-tom--which-leverages-concepts-like-nash-equilibria">game ToM , which leverages concepts like Nash equilibria</h3>
<ul>
<li>de Weerd et al employ a combination of computational agents and Bayesian model selection to determine the extent to which individuals use higher-order ToM reasoning in a particularly competitive game known as matching pennies. Their findings suggest that humans do not primarily employ their high-order ToM abilities.</li>
<li>In a case study of the paperscissors-rock game, Kanwal et al develop a ToM-based agent, capable of using gestures for non-verbal communication</li>
<li><a href="https://lgurjcsit.lgu.edu.pk/index.php/lgurjcsit/article/view/96/90" target="_blank" rel="noopener">View of A Step Towards the Development of Socio-cognitive Agent (lgu.edu.pk)</a></li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414113611571.png" alt="image-20230414113611571" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Tejwani formalize a theory of social interactions, encompassing cooperation, conflict, coercion, competition, and trade, by extending a nested Markov decision process (MDP) where agents reason about arbitrary functions of each other&rsquo;s hidden rewards.</li>
<li>[<a href="https://arxiv.org/abs/2110.10298" target="_blank" rel="noopener">2110.10298] Incorporating Rich Social Interactions Into MDPs (arxiv.org)</a></li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414114753785.png" alt="image-20230414114753785" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414114919915.png" alt="image-20230414114919915" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>In a follow-up study, Tejwani expand the reward function to incorporate both physical and social goals. Their method permits more complex behaviors, such as politely hindering or aggressively assisting another agent.</li>
<li><a href="https://proceedings.mlr.press/v164/tejwani22a/tejwani22a.pdf" target="_blank" rel="noopener">https://proceedings.mlr.press/v164/tejwani22a/tejwani22a.pdf</a></li>
<li>Panella and Gmytrasiewicz devise a new computational framework, interactive partially observable Markov decision process (I-POMDP), wherein the agent does not explicitly model the beliefs and preferences of other agents but rather represents them as stochastic processes implemented by probabilistic deterministic finite-state controllers (PDFCs). Using Bayesian inference, the agent updates its belief over the PDFCs models of other agents.</li>
<li><a href="https://link.springer.com/article/10.1007/s10458-016-9359-z" target="_blank" rel="noopener">Interactive POMDPs with finite-state models of other agents | SpringerLink</a></li>
</ul>
<h3 id="deep-learning-dl">Deep learning (DL)</h3>
<ul>
<li>Aru et al. point out that the problems of existing DL methods are taking shortcuts rather than learning ToM; the system may learn a much simpler decision rule . DL for ToM is explored predominantly with deep reinforcement learning (DRL), wherein the agent&rsquo;s experiences and objectives are intertwined. Usually, the task&rsquo;s reward structure determines what the agent accomplishes and learns. However, in the case of ToM, there may not exist a straightforward cost function or reward structure that would necessitate the emergence of ToM.</li>
<li>[<a href="https://arxiv.org/abs/2203.16540" target="_blank" rel="noopener">2203.16540] Mind the gap: Challenges of deep learning approaches to Theory of Mind (arxiv.org)</a></li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414120438385.png" alt="image-20230414120438385" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>Crucially, Zhao et al[160] demonstrate in a multi-agent setting that rewards may simply be a byproduct of ToM, not playing a causal role in establishing effective coordination.</li>
</ul>
<h2 id="social-communication-and-cooperation">Social communication and cooperation</h2>
<h3 id="nonverbal-communication">Nonverbal communication</h3>
<ul>
<li>Jiang et al model pointing as a communicative act between agents who have a mutual understanding that the pointed observation must be relevant and interpretable; the act of pointing is an invitation to jointly attend to an object, which elicits mutual inference between agents of each other&rsquo;s minds. The proposed model measures relevance by defining a Smithian value of information (SVI) as the utility gain of a pointing signal. By integrating SVI into rational speech act (RSA), their pragmatic model of pointing permits contextually flexible interpretations.</li>
<li>[<a href="https://arxiv.org/abs/2106.02003" target="_blank" rel="noopener">2106.02003] Individual vs. Joint Perception: a Pragmatic Model of Pointing as Communicative Smithian Helping (arxiv.org)</a></li>
<li><a href="https://pku.ai/publication/pointing2022cogsci/paper.pdf" target="_blank" rel="noopener">https://pku.ai/publication/pointing2022cogsci/paper.pdf</a></li>
<li>Tang et al demonstrate that agents can successfully and robustly employ bootstrapping to converge to a joint intention from randomness under an Imagined We framework, leveraging a real-time cooperative hunting task subject to various setting manipulations.</li>
<li><a href="https://cognitivesciencesociety.org/cogsci20/papers/0584/0584.pdf" target="_blank" rel="noopener">https://cognitivesciencesociety.org/cogsci20/papers/0584/0584.pdf</a></li>
<li>Stacy et al propose a computational account of overloaded signaling from a shared agency perspective, which we refer to as the Imagined We for communication. Within this framework, communication is a means for cooperators to coordinate their perspectives, allowing them to act in concert to achieve shared objectives. In a series of simulations, the model performs effectively under growing ambiguity and increasing levels of reasoning, highlighting how shared knowledge and cooperative logic may perform the majority of the heavy lifting in language</li>
<li>[<a href="https://arxiv.org/abs/2106.02164" target="_blank" rel="noopener">2106.02164] Modeling Communication to Coordinate Perspectives in Cooperation (arxiv.org)</a></li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414153526746.png" alt="image-20230414153526746" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
<h3 id="cooperation-1">Cooperation</h3>
<ul>
<li>
<p>Wang et al introduce ToM to build socially intelligent agents, who can communicate and cooperate effectively to accomplish challenging tasks. These agents determine when and with whom to reveal their intentions and sub-goals based on the inferred mental states of others.</p>
</li>
<li>
<p>[<a href="https://arxiv.org/abs/2111.09189" target="_blank" rel="noopener">2111.09189] ToM2C: Target-oriented Multi-agent Communication and Cooperation with Theory of Mind (arxiv.org)</a></p>
</li>
<li>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414154521125.png" alt="image-20230414154521125" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
</li>
<li>
<p>Pöppel et al study how efficient, automatic coordination mechanisms at the level of mental states (intentions, objectives), also known as belief resonance, may lead to collaborative situated problem-solving. They describe a model of hierarchical active inference for collaborative agent (HAICA) that blends Bayesian ToM with a perception-action system based on predictive processing and active inference. Belief resonance is realized by allowing the inferred mental states of one agent influence another agent&rsquo;s prediction beliefs regarding its own goals and intentions, hence influencing the agent&rsquo;s task behavior without explicit collaborative reasoning.</p>
</li>
<li>
<p><a href="https://doi.org/10.1007/s12559-021-09960-4" target="_blank" rel="noopener">https://doi.org/10.1007/s12559-021-09960-4</a></p>
</li>
<li>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414161510468.png" alt="image-20230414161510468" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
</li>
<li>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414161532796.png" alt="image-20230414161532796" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
</li>
</ul>
<h3 id="llm">LLM</h3>
<ul>
<li>Michal tested several language models using 40 classic false-belief tasks widely used to test ToM in humans. These findings suggest that ToM-like ability (thus far considered to be uniquely human) may have spontaneously emerged as a byproduct of language models’ improving language skills</li>
<li>The important role of language in cognition</li>
<li>Backwards cognition from language</li>
<li>[<a href="https://arxiv.org/abs/2302.02083" target="_blank" rel="noopener">2302.02083] Theory of Mind May Have Spontaneously Emerged in Large Language Models (arxiv.org)</a></li>
</ul>
<h2 id="social-robot-and-cognitive-architectures">Social robot and cognitive architectures</h2>
<ul>
<li>The social robot is an interdisciplinary research field that requires comprehensive studies of social perception, ToM, and social interaction</li>
<li>A social robot is expected to</li>
<li>(1) develop adaptive behavioral models ,</li>
<li>(2) be socially adept,</li>
<li>(3) establish a natural, fluent, and effective human-like communication and interaction with humans</li>
<li>(4) establish empathetic relationships with humans and be perceived as a teammate or a colleague rather than a tool,</li>
<li>(5) offer proactive and parental help based on the observations and understanding of the human situation</li>
<li>(6) build trust with humans.</li>
<li>However, there are still many obstacles to overcome before constructing an ideal social robot.</li>
<li>It is difficult to incorporate behavioral adaption techniques, cognitive architectures, persuasive communication strategies, and empathy into a single solution for understanding nonverbal phenomena in social interactions, as contexts are constantly changing.</li>
<li>A common limitation of current research is that researchers have focused on a particular aspect of a social robot, such as</li>
<li>(1) emphasizing a communication strategy,</li>
<li>(2) studying a particular behavior as a response to human action, or</li>
<li>(3) conducting experimental studies that include only partial factors.</li>
</ul>
<h3 id="cognitive-architecture">Cognitive architecture</h3>
<ul>
<li>ASI in robotic agents relies heavily on the construction of cognitive architecture, which involves both abstract models of cognition and software instantiations of such models</li>
<li>three classic cognitive architecture</li>
<li>LIDA (Learning Intelligent Distribution Agent) is a cognitive architecture proposed by Stan Franklin et al. in 2005. Its core idea is to view cognitive processes as distributed processing of information, achieved through modules such as perception, attention, working memory, and long-term memory at different levels.</li>
<li>Soar (State, Operator, and Result) is a cognitive architecture proposed by Allen Newell and Paul Rosenbloom et al. in 1983. Its core idea is to view cognitive processes as a series of state transitions for problem solving, achieved through mechanisms such as state representation, rule matching, and subgoals.</li>
<li>ACT-R (Adaptive Control of Thought - Rational) is a cognitive architecture proposed by John Anderson et al. in 1983. Its core idea is to view cognitive processes as a series of rule-based symbol processing, achieved through the interaction between declarative and procedural knowledge.</li>
<li>Therefore, LIDA was proposed in 2005, Soar was proposed in 1983, and ACT-R was also proposed in 1983.</li>
</ul>
<h4 id="learning-intelligent-distribution-agent-lida">Learning intelligent distribution agent (LIDA)</h4>
<ul>
<li>cognitive architecture is an integrated artificial cognitive system that models a broad spectrum of biological cognition, from low-level perception and action to high-level reasoning. Two hypotheses underlie the LIDA architecture and its corresponding conceptual model:</li>
<li>(1) Much of human cognition functions through cognitive cycles, which are interactions between conscious contents, memory systems, and action selection, occur frequently (10 Hz).</li>
<li>(2) Cognitive cycles serve as the cognitive atoms of which higherlevel cognitive processes are composed.</li>
<li><a href="https://link.springer.com/chapter/10.1007/978-3-642-22887-2_14" target="_blank" rel="noopener">The LIDA Framework as a General Tool for AGI | SpringerLink</a></li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414153338050.png" alt="image-20230414153338050" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
<h4 id="soar">Soar</h4>
<ul>
<li>
<p>The Soar cognitive architecture is composed of interacting task-independent modules, including short-term and long-term memories, processing modules, learning mechanisms, and interfaces between them. Since Soar hypothesizes that sufficient regularities exist above the neural level to capture the functionality of the human mind, the majority of knowledge representations in Soar are symbol structures, with architecturally maintained numeric metadata biasing the retrieval and learning of those structures</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Soar_%28cognitive_architecture%29" target="_blank" rel="noopener">Soar (cognitive architecture) - Wikipedia</a></p>
</li>
<li>
<p>J. E. Laird, The Soar Cognitive Architecture, Cambridge, MA, USA: MIT Press, 2019.</p>
</li>
</ul>
<h4 id="adaptive-control-of-thought-rationale-architecture-act-r">Adaptive control of thought-rationale architecture (ACT-R)</h4>
<ul>
<li>includes modules such as</li>
<li>(1) a visual module for identifying objects in the visual field,</li>
<li>(2) a manual module for controlling the hands,</li>
<li>(3) a declarative module for retrieving information from memory,</li>
<li>(4) a goal module for tracking current goals and intentions, and</li>
<li>(5) a central production system to coordinate these modules.</li>
<li>There are buffers within each module that transmit information back and forth to the central production system. The architecture assumes a mixture of serial and parallel processing</li>
<li><a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/actr-a-higherlevel-account-of-processing-capacity/73969664D69FB461CC04097B6D49FC77" target="_blank" rel="noopener">ACT-R: A higher-level account of processing capacity | Behavioral and Brain Sciences | Cambridge Core</a></li>
<li><a href="https://psycnet.apa.org/doiLanding?doi=10.1037%2F0033-295X.111.4.1036" target="_blank" rel="noopener">An Integrated Theory of the Mind. (apa.org)</a></li>
</ul>
<h3 id="cognitive-architectures-in-social-robots">Cognitive architectures in social robots</h3>
<ul>
<li>
<p>Wiltshire et al discuss the problem of engineering human social-cognitive mechanisms to enable robot social intelligence and provide an integrative perspective of social cognition as a systematic theoretical underpinning for computational instantiations of these mechanisms. They also provide a series of recommendations to facilitate the development of the perceptual, motor, and cognitive architecture.</p>
</li>
<li>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S1389041716300493?via%3Dihub" target="_blank" rel="noopener">Enabling robotic social intelligence by engineering human social-cognitive mechanisms - ScienceDirect</a></p>
</li>
<li>
<p>Breazeal et al provide an integrated sociocognitive architecture (see Fig. 14) to endow an anthropomorphic robot with the ability to infer mental states such as beliefs, intents, and desires from the observable behavior of its human partner via simulation-theoretic techniques.</p>
</li>
<li>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414173012629.png" alt="image-20230414173012629" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
</li>
<li>
<p>Kennedy et al describe an approach known as a like-me simulation, in which the agent uses its own knowledge and capabilities as a model of another agent to predict that agent’s actions. They present three examples of a likeme mental simulation in a social context implemented in the embodied version of the adaptive control of thought-rationale architecture (ACT-R) cognitive architecture, ACT-R Embodied (ACT-R/E), including perspective taking, teamwork, and dominant-submissive social behavior.</p>
</li>
<li>
<p><a href="https://link.springer.com/article/10.1007/s12369-009-0014-6" target="_blank" rel="noopener">“Like-Me” Simulation as an Effective and Cognitively Plausible Basis for Social Robotics | SpringerLink</a></p>
</li>
<li>
<p>Moulin-Frier et al suggest the DAC-h3 architecture, which incorporates a reactive interaction engine, a number of state-of-the-art perceptual and motor learning algorithms, planning capabilities, and an autobiographical memory. The architecture as a whole drives the robot’s behavior to solve the symbol grounding problem, acquire language capabilities, perform goal-oriented behavior, and articulate a verbal narrative of its own experience in the world.</p>
</li>
<li>
<p><a href="https://doi.org/10.1109/TCDS.2017.2754143" target="_blank" rel="noopener">https://doi.org/10.1109/TCDS.2017.2754143</a></p>
</li>
<li>
<p>Franchi et al[179] present a brain-inspired architecture, the intentional distributed robotic architecture (IDRA), which aims to permit the autonomous development of new goals in situated agents beginning with simple hard-coded instincts.</p>
</li>
<li>
<p><a href="https://www.tandfonline.com/doi/full/10.1080/01691864.2016.1172732" target="_blank" rel="noopener">From learning to new goal generation in a bioinspired robotic setup: Advanced Robotics: Vol 30, No 11-12 (tandfonline.com)</a></p>
</li>
</ul>
<h1 id="discussions">Discussions</h1>
<h2 id="recent-advances-in-datasets-and-environments">Recent advances in datasets and environments</h2>
<ul>
<li><strong>PHASE:</strong> Netanyahu et al[180] resemble a collection of physically-grounded abstract social events (PHASE) that simulates a wide variety of real-world social interactions by incorporating social concepts, such as helping another agent. PHASE is comprised of 2D animations of agent pairs, moving in continuous space with multiple objects and landmarks, generated procedurally by a physics engine and a hierarchical planner.</li>
<li>[<a href="https://arxiv.org/abs/2103.01933" target="_blank" rel="noopener">2103.01933] PHASE: PHysically-grounded Abstract Social Events for Machine Social Perception (arxiv.org)</a></li>
<li><strong>AGENT:</strong> Inspired by intuitive psychology, Shu et al[181] present a benchmark consisting of a large dataset of procedurally generated 3D animations, Action, Goal, Efficiency, coNstraint, uTility (AGENT), structured around four scenarios (goal preferences, action efficiency, unobserved constraints, and cost-reward tradeoffs) that probe key concepts of core intuitive psychology</li>
<li>[<a href="https://arxiv.org/abs/2102.12321" target="_blank" rel="noopener">2102.12321] AGENT: A Benchmark for Core Psychological Reasoning (arxiv.org)</a></li>
<li><strong>WAH:</strong> Puig et al[182] introduced watch-and-help (WAH), a challenge for testing social intelligence in agents, wherein an AI agent is tasked to help a human-like agent perform a complex household task efficiently. They build VirtualHomeSocial, a multi-agent household environment, and provide a benchmark including both planning and learning-based baselines.</li>
<li>[<a href="https://arxiv.org/abs/2010.09890" target="_blank" rel="noopener">2010.09890] Watch-And-Help: A Challenge for Social Perception and Human-AI Collaboration (arxiv.org)</a></li>
<li>Sap et al proposed a dataset to evaluate language-based commonsense reasoning about social interactions, including reasoning about motivation and about emotional reactions .</li>
<li>[<a href="https://arxiv.org/abs/1904.09728" target="_blank" rel="noopener">1904.09728] SocialIQA: Commonsense Reasoning about Social Interactions (arxiv.org)</a></li>
<li>**Hanabi: **Bard et al propose the cooperative and imperfect information card game, Hanabi, as a challenging benchmark. It requires reasoning about the beliefs and the intentions of other players, focusing on the ad-hoc setting where an agent has to coordinate with a team they encounter for the first time.</li>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0004370219300116?via%3Dihub" target="_blank" rel="noopener">The Hanabi challenge: A new frontier for AI research - ScienceDirect</a></li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414181418930.png" alt="image-20230414181418930" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
</ul>
<h2 id="evaluation-protocols">Evaluation protocols</h2>
<ul>
<li>the formation of universally accepted criteria for the design and implementation of ASI benchmarks and the accompanying evaluation protocols is still in its infancy and represents a significant barrier to the field&rsquo;s continued progress.</li>
<li>Because human judgments can be ambiguous and difficult to express, many social intelligence tasks do not include requirements that can be easily captured using hand-crafted rules.</li>
<li>The Turing test is a test of a machine&rsquo;s ability to exhibit intelligent behavior equivalent to or indistinguishable from that of a human.</li>
<li>However, current systems that perform well on these tests typically do so by employing techniques that are not generalizable to other problems.</li>
<li>Other approaches for assessing social intelligence competency are often derived from various sources, such as peer-/superior-/self-ratings and observers’ behavioral assessments</li>
<li>Notably, the Animal-AI Olympics[187] is initiated by testing artificial agents on tasks derived directly from animal cognition research in an effort to establish common ground</li>
<li><a href="https://www.nature.com/articles/s42256-019-0050-3" target="_blank" rel="noopener">The Animal-AI Olympics | Nature Machine Intelligence</a></li>
<li>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414184518872.png" alt="image-20230414184518872" loading="lazy" data-zoomable /></div>
  </div></figure>
</li>
<li>An important aspect of the ASI is to measure cognitive skills, adaptability, and meta-level learning and reasoning ability rather than specific problem-solving ability.</li>
<li>Using more abstract cognitive processes, such as the ability to</li>
<li>(1) transfer information from one domain to another,</li>
<li>(2) retain information for extended periods, and</li>
<li>(3) correct errors in performance, may be future effective strategies for assessing ASI.</li>
</ul>
<h2 id="future-trends">Future trends</h2>
<ul>
<li><strong>A holistic approach</strong>: Cognitive and neuroscience research shows that while distinct brain regions are involved in specific tasks, a core network is involved in all ToM tasks, suggesting that humans take a more holistic approach to social intelligence than existing computational models, which often focus on a single aspect of the problem.</li>
<li><strong>Learning methods</strong>: Infants develop intelligence gradually. This suggests that learning, and in particular lifelong/continuous learning , is a crucial path for developing ASI. The objective of lifelong/continuous learning is to successively learn a model for a large number of activities without forgetting the knowledge acquired from the previous tasks. Other potentially effective learning strategies include multi-task learning, one-/few-shot learning, and meta-learning .</li>
<li><strong>Open-ended and interactive environment</strong></li>
<li><strong>Human biases</strong>:  We must also introduce better biases, even structural biases, as a form of built-in common sense, as there may be multiple biases and limits in the human brain that facilitate the acquisition of social intelligence. For instance, there may be innate biases of attention to the human face, speech, hands, eyes, gaze-direction, and biological motion, and these early biases ensure that the infant learns about the components of the world that provide information about the minds of other people.</li>
</ul>
<h1 id="conclusion">Conclusion</h1>
<ul>
<li>ASI is a crucial missing component for artificial general intelligence (AGI) on par with humans and symbolizes the future path of AI.</li>
</ul>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/tom/">ToM</a>
  
  <a class="badge badge-light" href="/tag/ai/">AI</a>
  
  <a class="badge badge-light" href="/tag/asi/">ASI</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fexample.com%2Fpost%2Ftom-summary%2F&amp;text=Artificial&#43;Social&#43;Intelligence&#43;-&#43;A&#43;Comparative&#43;and&#43;Holistic&#43;View" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fexample.com%2Fpost%2Ftom-summary%2F&amp;t=Artificial&#43;Social&#43;Intelligence&#43;-&#43;A&#43;Comparative&#43;and&#43;Holistic&#43;View" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
        
      
      <li>
        <a href="mailto:?subject=Artificial%20Social%20Intelligence%20-%20A%20Comparative%20and%20Holistic%20View&amp;body=https%3A%2F%2Fexample.com%2Fpost%2Ftom-summary%2F" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fexample.com%2Fpost%2Ftom-summary%2F&amp;title=Artificial&#43;Social&#43;Intelligence&#43;-&#43;A&#43;Comparative&#43;and&#43;Holistic&#43;View" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="whatsapp://send?text=Artificial&#43;Social&#43;Intelligence&#43;-&#43;A&#43;Comparative&#43;and&#43;Holistic&#43;View%20https%3A%2F%2Fexample.com%2Fpost%2Ftom-summary%2F" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Fexample.com%2Fpost%2Ftom-summary%2F&amp;title=Artificial&#43;Social&#43;Intelligence&#43;-&#43;A&#43;Comparative&#43;and&#43;Holistic&#43;View" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  
    



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://example.com/"><img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu78482ed6fd3911eb675c3aef45a3fb83_32147_270x270_fill_q75_lanczos_center.jpg" alt="Haofei Hou"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://example.com/">Haofei Hou</a></h5>
      <h6 class="card-subtitle">Undergraduate</h6>
      <p class="card-text">My research interests include Computational Cognitive Science, Artificial Intelligence.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:yuechuhaoxi020609@outlook.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/yuechuhaoxi020609" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  
















  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2023 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js"></script>




  

  
  

  













  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  







<script id="page-data" type="application/json">{"use_headroom":true}</script>



  <script src="/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js" type="module"></script>









  
  


<script src="/en/js/wowchemy.min.e8ee06ba8371980ffde659871dd593b0.js"></script>







  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js" type="module"></script>


















</body>
</html>
