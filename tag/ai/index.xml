<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI | Haofei Hou | HUST</title>
    <link>https://example.com/tag/ai/</link>
      <atom:link href="https://example.com/tag/ai/index.xml" rel="self" type="application/rss+xml" />
    <description>AI</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 14 Apr 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu88cdd4b7a12988e7d5bb56be50e4f21a_10097_512x512_fill_lanczos_center_3.png</url>
      <title>AI</title>
      <link>https://example.com/tag/ai/</link>
    </image>
    
    <item>
      <title>Artificial Social Intelligence - A Comparative and Holistic View</title>
      <link>https://example.com/post/tom-summary/</link>
      <pubDate>Fri, 14 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/tom-summary/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;humans possess a high social intelligence—the intelligence that senses social events, infers the goals and intents of others, and facilitates social interaction.&lt;/li&gt;
&lt;li&gt;cognitive science standpoint: social perception, theory of mind (ToM), and social interaction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;dawn-of-artificial-social-intelligence&#34;&gt;Dawn of Artificial Social Intelligence&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;efforts towards human-like intelligence can be divided into physical intelligence and social intelligence , analogous to the developmental psychology ideas of intuitive physics and intuitive psychology&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;unique-challenges-of-context&#34;&gt;Unique challenges of context&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ASI is distinct and challenging compared to our physical understanding of the world; it is highly context-dependent&lt;/li&gt;
&lt;li&gt;Here, context could be as large as culture and common sense or as little as two friends&amp;rsquo; shared experiences&lt;/li&gt;
&lt;li&gt;it begins with nonverbal communication&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview-of-the-article&#34;&gt;Overview of the article&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In Section 2, which covers social perception, theory of mind, and social interaction, we begin with experimental evidence and theoretical hypotheses of human social intelligence from the standpoint of cognitive science.&lt;/li&gt;
&lt;li&gt;In Section 3, we present the AI community’s computational counterpart, focused on social perception, theory of mind, and social interaction, with an added topic on social robot and cognitive architectures.&lt;/li&gt;
&lt;li&gt;In Section 4, we explore significant challenges that impede the development of the ASI and recommend potential future trends.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;human-social-intelligence&#34;&gt;Human Social Intelligence&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;We concentrate on the three most important aspects of social intelligence: social perception, ToM, and social interaction.&lt;/li&gt;
&lt;li&gt;Social perception is the basis for ToM and social interaction. It consists primarily of the perception of social features, such as animacy and agency, and provides low-level, automatic, instantaneous, and non-conscious visual perception. ToM, in contrast, is concerned with sophisticated, analytic, and logical cognitive reasoning, involving a general cognitive system with several essential components, including belief, intent, and desire. Social interaction emphasizes more multi-agent interactive activities, such as communication and cooperation, than social perception and ToM.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;social-perception&#34;&gt;Social perception&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Heider-Simmel stimuli&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230412213753647.png&#34; alt=&#34;image-20230412213753647&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Even when told explicitly that these are merely simple shapes, participants still make a rapid, spontaneous, and consistent perception of animate social agents with various complex mental states, including desires, goals, emotions, personalities, and coalitions.&lt;/li&gt;
&lt;li&gt;The Heider-Simmel experiment demonstrates two essential aspects of human social perception: the perception of animacy and agency. Animacy denotes that the perceived entities are animate as opposed to inanimate (e.g., physical objects), whereas agency refers to animate beings who are goal-oriented and capable of planning to achieve their goals rationally and efficiently.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;animacy&#34;&gt;Animacy&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In this experiment, participants were shown two small squares separated by several inches and arranged in a line.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230412215127371.png&#34; alt=&#34;image-20230412215127371&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;In the first scenario, the first square (A) moves in a straight line until it reaches the second square (B), at which point A stops moving and B begins moving in the same direction (also called the launching effect).&lt;/li&gt;
&lt;li&gt;In case two, the first square (A) approaches the second square (B). While A approaches, B moves away from A quickly and stops when it is several inches away again.&lt;/li&gt;
&lt;li&gt;In the first instance, observers observe A physically causing B&amp;rsquo;s motion&lt;/li&gt;
&lt;li&gt;in the second case, A and B are perceived as alive with their own intentions.&lt;/li&gt;
&lt;li&gt;spatiotemporal parameters mediate causal perceptions, such as relative velocity, speed–mass interaction, path length, and spatial and temporal gap.&lt;/li&gt;
&lt;li&gt;spatiotemporal parameters mediate causal perceptions, such as relative velocity, speed–mass interaction, path length, and spatial and temporal gap.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;agency&#34;&gt;Agency&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An agent is rationally controlled because it has an internal energy source, whereas an object is not.&lt;/li&gt;
&lt;li&gt;The perception of agency is frequently studied in tandem with animacy for more complex social phenomena.&lt;/li&gt;
&lt;li&gt;Gao et al[81] study a particularly salient form of perceived animacy and agency via tasks based on dynamic visual search (the Find-the-Chase task)&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230412215212952.png&#34; alt=&#34;image-20230412215212952&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;They used two cues to evaluate the objective accuracy of such perceptions: (1) chasing subtlety—the degree to which the wolf deviates from a perfectly heat-seeking pursuit, and (2) directionality—whether and how the shapes face each other.&lt;/li&gt;
&lt;li&gt;the researchers discovered that temporal dynamics could lead the visual system to either construct or actively reject interpretations of chasing&lt;/li&gt;
&lt;li&gt;van Buren et al[84] depict one disc (the “wolf”) pursuing another disc (the “sheep”) amidst several distractor discs that are moving.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230412215259122.png&#34; alt=&#34;image-20230412215259122&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;In the Unconnected condition, both lines connected distractors in pairs. In the Connected condition, however, one line connected the wolf to a distractor, and the other line connected the sheep to a different distractor. Observers in the Connected condition were markedly less likely to describe these behaviors in terms of mental state.&lt;/li&gt;
&lt;li&gt;According to the outcomes of their experiments, discrete visual objects are the fundamental units of social perception.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;specific bottomup perceptual processing is specialized and difficult to be “penetrated” by higher-level cognition. This type of social perception may be at the intersection of perceptual and cognitive processing, where basic stimuli are transformed into causal, animate, or even intentional qualities, which are strongly linked to higher-level cognitive processing.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tom&#34;&gt;ToM&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;chimpanzee Sarah was shown a brief clip of an experimenter attempting to perform simple tasks. Subsequently, Sarah observed images of several objects, one of which solved the experimenter’s&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Their findings highlight two essential components of ToM: a representation of the affair state and a representation of an individual&amp;rsquo;s motivational link to the state, i.e., belief and intention&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ToM entails attributing mental states (such as beliefs, intents, or desires) to oneself and others&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Perspective taking in an internal simulation process is one of the defining characteristics of ToM&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The infamous Sally-Anne test[90,91] , a classic first-order false belief task, is a well-known experiment on perspective taking.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414191524787.png&#34; alt=&#34;image-20230414191524787&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Later in the development trajectory[95] is the establishment of second-order ToM, which entails predicting what one person thinks or feels about what another person thinks or feels&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;intent&#34;&gt;Intent&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In fact, research has shown that humans do not encode the entirety of action details but rather observe and interpret actions in terms of their intentions and store these interpretations for later retrieval.&lt;/li&gt;
&lt;li&gt;The developmental psychology literature indicates that sixmonth-old infants view human actions as goal-directed behavior. By the age of 10 months, infants segment continuous behavior streams into discrete units that correspond to what adults would perceive as distinct goal-directed acts . After their first birthday, infants begin to comprehend that an agent may explore multiple plans to achieve a goal and choose one based on environmental conditions. 18-month-old children can deduce and reproduce an action’s intended purpose, even if the activity frequently fails to achieve the aim[102] . In addition, infants can replicate behaviors rationally and effectively based on an evaluation of the environmental restrictions, as opposed to just duplicating movements, indicating that they understand the relationships between the environment, action, and underlying intent.&lt;/li&gt;
&lt;li&gt;intentions are hierarchically arranged across extensive spatiotemporal ranges as a sequence of goals . Infants are already capable of perceiving intentions on multiple levels, including concrete action goals, higher order plans, and collaborative goals&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;categorization&#34;&gt;Categorization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cognitive ToM can be further divided into ToM for motivation (i.e., another organism’s valuation, intention, purpose, and goal) and ToM for knowledge (i.e., another organism&amp;rsquo;s belief states or taught schemas/scripts) .&lt;/li&gt;
&lt;li&gt;Individual differences in cognitive strategies are also present. The theory-theory method and simulation-theory approach are examples of these diverse ToM strategies. The theory-theory approach may be based on a set of intrinsic rules or on causal and probabilistic reasoning models, which may be analogous to cold cognition in which mental states are inferred through intellectual processes. The simulation-theory approach relies on the individual’s own motivations and deductive reasoning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;social-interaction&#34;&gt;Social interaction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;equip ASI with more sophisticated human-like communication and collaboration capabilities&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;social-cues&#34;&gt;Social cues&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;paralinguistic (voice prosody and non-language sounds),&lt;/li&gt;
&lt;li&gt;facial expression (motion and position of facial muscles),&lt;/li&gt;
&lt;li&gt;gaze (motion and position of the eyes and predicted sight-line),&lt;/li&gt;
&lt;li&gt;kinematics (motion, position, and posture of the body),&lt;/li&gt;
&lt;li&gt;proxemics (use of interpersonal space)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;gaze-communication&#34;&gt;Gaze communication&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Psychological evidence suggests that eyes are stimuli with distinct “hardwired” neural pathways in the brain for their interpretation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;joint-attention&#34;&gt;Joint attention&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230412215415220.png&#34; alt=&#34;image-20230412215415220&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;layers of human gaze communication dynamics: atomic-level and event-level.&lt;/li&gt;
&lt;li&gt;Event-level gaze communication refers to high-level, complex social communication events, such as non-communicative, mutual gaze, gaze aversion, gaze following, and joint attention.&lt;/li&gt;
&lt;li&gt;Atomic-level gaze communication describes the granular structures of human gaze interactions, including single, mutual, avoid, refer, follow, and share.&lt;/li&gt;
&lt;li&gt;Joint attention is the most advanced sort of gaze communication, as it requires two agents (1) to have the same intention to share attention on common stimuli and (2) to be aware that they are sharing a common ground.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pointing&#34;&gt;Pointing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;According to Tomasello, pointing is one of the earliest forms of communication exclusive to the human species (the other is pantomiming).&lt;/li&gt;
&lt;li&gt;Pointing is also an indicator of particular cognitive abilities, such as being an intentional actor and having ToM&lt;/li&gt;
&lt;li&gt;Bates et al and Brinck are credited with introducing the distinction between imperative pointing and declarative pointing.&lt;/li&gt;
&lt;li&gt;Declarative pointing is primarily intersubjective with a signaling function, whereas imperative pointing is based on behaviorally motivated regularities and is used to request the addressee to do something for the subject.&lt;/li&gt;
&lt;li&gt;Levinson developed the concept of interaction engine, which allows communication intentions to be conveyed and recognized in both linguistic and nonlinguistic encounters.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cooperation&#34;&gt;Cooperation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cooperation is a type of social interaction that is more complex than simple communication, as it requires a psychological infrastructure of shared intentionality.&lt;/li&gt;
&lt;li&gt;This infrastructure is comprised of two crucial factors: (1) socialcognitive skills for creating common conceptual ground with others, such as joint attention and joint intention, and (2) prosocial motivations and norms to help and share with others&lt;/li&gt;
&lt;li&gt;communication: the exchange of information between agents&lt;/li&gt;
&lt;li&gt;coordination: the alignment of multiple agents towards the achievement of specific common goals through the efforts of individual agents&lt;/li&gt;
&lt;li&gt;cooperation: each individual agent/robot exchanges relevant information and resources in support of each other’s goals, rather than a shared common goal,&lt;/li&gt;
&lt;li&gt;collaboration: requires agents to exchange information and knowledge in support of a shared task.&lt;/li&gt;
&lt;li&gt;Tomasello believes that  “shared cooperative actions” have two essential characteristics: (1) the participants have a joint goal in the sense that we (in mutual knowledge) do X together; and (2) the participants coordinate their interdependent roles—their plans and sub-plans of action, including helping one another as needed in their respective roles.&lt;/li&gt;
&lt;li&gt;Tomasello also proposed a dual-level attentional structure (the shared focus of attention at a higher level, differentiated into perspectives at a lower level) and a dual-level intentional structure (shared goal with individual roles), arguing that the former is directly parallel to the latter and may ultimately derive from it.&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413183329097.png&#34; alt=&#34;image-20230413183329097&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;artificial-social-intelligence&#34;&gt;Artificial Social Intelligence&lt;/h1&gt;
&lt;h2 id=&#34;social-perception-in-simulated-scenarios&#34;&gt;Social perception in simulated scenarios&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Shu et al present a unified theory that describes the interrelationships between the perception of physical and social events&lt;/li&gt;
&lt;li&gt;Specifically, the model learns to identify latent forces by inferring a family of potential functions capturing physical laws and value functions of agent goals, thereby projecting the animations into a sociophysical space with two psychological dimensions: an intuitive sense of whether physical laws are violated and an impression of whether an agent possesses intentions to perform goal-directed actions.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://doi.org/10.1016/j.cogpsych.2021.101398&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1016/j.cogpsych.2021.101398&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413190203086.png&#34; alt=&#34;image-20230413190203086&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Tang et al investigate the problem of simultaneously perceiving physics and mind using a leash-chasing display, in which a disc (“sheep”) is being chased by another disc (“wolf”) that is physically constrained by a leash tied to a third disc (“master”). They discover that (1) an intuitive physical system, such as a leash, can significantly mitigate the detrimental effects of spatial deviation and the diminishing object-hood on perceived chasing, thereby enhancing its robustness, and (2) a mutual dependency exists between physics and mind, where disrupting one will inevitably impair the perception on the other, supporting a joint perception of physics and mind.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.stat.ucla.edu/~taogao/static/pdf/Jointly%20Perceiving%20Physics%20and%20Mind_TangGongGao_2021.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.stat.ucla.edu/~taogao/static/pdf/Jointly%20Perceiving%20Physics%20and%20Mind_TangGongGao_2021.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413191340154.png&#34; alt=&#34;image-20230413191340154&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Flatland is a new experimental paradigm introduced by Shu et al for exploring social inference in physical situations. Results demonstrate that human interpretations of interactive events in Flatland can be accounted for by a computational model that combines inverse hierarchical planning with a physical simulation engine to reason about objects and agents.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cocodev.fas.harvard.edu/publications/adventures-in-flatland-perceiving-social-interactions-under-physical-dynamics/2020-shu-adventures.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cocodev.fas.harvard.edu/publications/adventures-in-flatland-perceiving-social-interactions-under-physical-dynamics/2020-shu-adventures.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413191815934.png&#34; alt=&#34;image-20230413191815934&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413191646740.png&#34; alt=&#34;image-20230413191646740&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Shu et al examine the perception of social interaction using decontextualized motion trajectories, in which stimuli are extracted from drone-recorded aerial films of a real-world setting. To account for human judgments of interactiveness between two moving dots and the dynamic change of such judgments over time, they construct a hierarchical model that represents interactivity using latent variables and learns the distribution of critical movement features that signal potential interactivity.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://doi.org/10.1111/tops.12313&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1111/tops.12313&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413204239814.png&#34; alt=&#34;image-20230413204239814&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413192231676.png&#34; alt=&#34;image-20230413192231676&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;To investigate the cognitive architecture of perceived animacy, Gao et al devise Bayesian models that integrate domain-specific hypotheses of social agency with domain-general cognitive constraints on sensory, memory, and attentional processing. The proposed model posits that perceived animacy combines a bottomup, feature-based, parallel search for goal-directed movements with architecturally distinct processes that make perceived animacy fast, flexible, and cognitively efficient. By distinguishing target agents from distractor objects in the “wolf-chasing-sheep” setting, they demonstrate that a Bayesian ideal observer model may explain the efficacy of human perceived animacy with realistic cognitive constraints.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/cogs.12775&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Cognitive Architecture of Perceived Animacy: Intention, Attention, and Memory (wiley.com)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413205304659.png&#34; alt=&#34;image-20230413205304659&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;social-perception-in-real-world-scenarios&#34;&gt;Social perception in real-world scenarios&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fan et al investigate the topic of inferring shared attention in their collected third-person social scene video dataset VideoCoAtt by employing a spatiotemporal neural network utilizing human gaze directions and potential target boxes extracted from the context. In their subsequent study. the authors systematically investigate the subject of human gaze communication by constructing spatiotemporal graphs for realworld social scenarios in the collected VACATION video dataset. the authors systematically investigate the subject of human gaze communication by constructing spatiotemporal graphs for realworld social scenarios in the collected VACATION video dataset.&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/1909.02144&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;1909.02144] Understanding Human Gaze Communication by Spatio-Temporal Graph Reasoning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413205604467.png&#34; alt=&#34;image-20230413205604467&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;To jointly infer human attention, intention, and task from videos, Wei et al introduce a hierarchical model of humanattention-object (HAO) and a beam search algorithm. According to their definition, the intention consists of the human pose, attention, and objects, whereas the task is represented as a series of intentions.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/8578809&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Where and Why are They Looking? Jointly Inferring Human Attention and Intentions in Complex Tasks | IEEE Conference Publication | IEEE Xplore&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413205946790.png&#34; alt=&#34;image-20230413205946790&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Xie et al offer an unsupervised method for localizing functional objects and predicting human intents and trajectories from surveillance footage of public places. Agents are influenced by the attractive or repulsive “fields” of functioning objects, referred to as “dark matter”. In addition to estimating the agent’s intent, the model can also derive the agent’s trajectory via agent-based Lagrangian mechanics&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/7984896&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning and Inferring “Dark Matter” and Predicting Human Intents and Trajectories in Videos | IEEE Journals &amp;amp; Magazine | IEEE Xplore&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413210621863.png&#34; alt=&#34;image-20230413210621863&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Holtzen et al[137] present a method that enables robots to infer a person&amp;rsquo;s hierarchical intent from partially observed RGB-D videos. They represent intent as a novel hierarchical, compositional, and probabilistic And-Or-Graph structure that describes a relationship between actions and plans. Human intent is inferred by reverseengineering a person’s decision-making and action-planning processes under a Bayesian probabilistic programming framework.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/7759242&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Inferring human intent from video by sampling hierarchical plans | IEEE Conference Publication | IEEE Xplore&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413214642238.png&#34; alt=&#34;image-20230413214642238&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413214625377.png&#34; alt=&#34;image-20230413214625377&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tom-1&#34;&gt;ToM&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The computational modeling of ToM may concentrate on different components, such as belief, intent, and desire.&lt;/li&gt;
&lt;li&gt;Gonzalez and Chang divide computational models of ToM into several broad categories, including &lt;strong&gt;Game ToM , Observational (RL) , Inverse RL , and Bayesian ToM&lt;/strong&gt;. These models contain modules for representing the goals and desires of an agent, inferring the mental states of other agents (e.g., beliefs, goals, desires, intentions, and feelings), and integrating these goals and mentalizing computations to generate optimal policies.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;different-tom-components-and-modeling-methods&#34;&gt;different ToM components and modeling methods&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Yuan et al jointly infer object states, robot knowledge, and human beliefs using parse graphs, which accurately identify human (false-)beliefs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2004.12248&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/2004.12248&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To answer this Sally-Anne correctly, an agent should understand and disentangle the object state (observation from the current frame), the (accumulated) knowledge, the belief of other agents, the ground-truth/reality of the world, and importantly, the concept of false-belief.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fan et al incorporate different nonverbal communication cues (e.g., gaze, human poses, and gestures) to infer agents’ mental states based solely on visual inputs. By aggregating beliefs and physical-world states, their approach effectively forms five minds during the interactions between two agents. In particular, they construct a common mind to avoid the infinite recursion commonly used in prior works. In addition, they devise a hierarchical energy-based model&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2104.02841&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2104.02841] Learning Triadic Belief Dynamics in Nonverbal Communication from Videos (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413224240325.png&#34; alt=&#34;image-20230413224240325&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Arslan investigate how 5-year-olds choose and revise reasoning strategies in second-order false belief tasks by constructing two computational cognitive models of this process: an instance-based learning model and a RL model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fpsyg.2017.00275/pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.frontiersin.org/articles/10.3389/fpsyg.2017.00275/pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230413224634548.png&#34; alt=&#34;image-20230413224634548&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414003302108.png&#34; alt=&#34;image-20230414003302108&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Oguntola develop an interpretable modular neural framework for modeling the intentions of other observed entities, demonstrating the model&amp;rsquo;s efficacy in a Minecraft search and rescue task. They also demonstrate that, under the right conditions, integrating interpretability can dramatically improve prediction performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2104.02938&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/2104.02938&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414003958427.png&#34; alt=&#34;image-20230414003958427&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Zeng et al suggest a brain-inspired model of belief ToM, leveraging high-level knowledge of brain regions’ functions relevant to ToM. Although tested on false belief tasks, such cognitive architecture may be difficult to motivate at the computational level&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.3389/fnbot.2020.00060&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.3389/fnbot.2020.00060&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414004300635.png&#34; alt=&#34;image-20230414004300635&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414004338055.png&#34; alt=&#34;image-20230414004338055&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414004359629.png&#34; alt=&#34;image-20230414004359629&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bayesian-methods&#34;&gt;Bayesian methods&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Baker et al investigate the rational quantitative attribution of beliefs, desires, and percepts in human mentalizing from agents&amp;rsquo; movement in a local spatial environment. They devise a Bayesian theory of mind (BToM) model in a partially observable Markov decision process (POMDP) setting for rational planning and state estimation, which extends classical expected-utility agent models to sequential actions in complex, partially observable domains.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://compdevlab.yale.edu/docs/2017/Bakeretal2017.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://compdevlab.yale.edu/docs/2017/Bakeretal2017.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://static-content.springer.com/esm/art%3A10.1038%2Fs41562-017-0064/MediaObjects/41562_2017_BFs415620170064_MOESM348_ESM.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://static-content.springer.com/esm/art%3A10.1038%2Fs41562-017-0064/MediaObjects/41562_2017_BFs415620170064_MOESM348_ESM.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/clbaker/BToM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;clbaker/BToM (github.com)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414085632574.png&#34; alt=&#34;image-20230414085632574&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rl-method&#34;&gt;RL method&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Wen et al and Moreno et al are examples of recursive reasoning models for higher-order ToM in a RL framework.&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/1901.09207&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;1901.09207] Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414095228222.png&#34; alt=&#34;image-20230414095228222&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/2102.02274&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2102.02274] Neural Recursive Belief States in Multi-Agent Reinforcement Learning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414105134026.png&#34; alt=&#34;image-20230414105134026&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Hakimzadeh contend that RL plays a crucial role in human intuition and cognition, and theories such as the language of thought hypothesis, script theory, and Piaget’s theory of cognitive development offer complementary approaches.&lt;/li&gt;
&lt;li&gt;ToM can indeed be formulated as an inverse reinforcement learning (IRL) problem, where expectations for how mental states produce behavior are represented by a RL model.&lt;/li&gt;
&lt;li&gt;RL models, such as IRL and multi-agent reinforcement learning (MARL), are highly scalable but computationally intensive and less interpretable&lt;/li&gt;
&lt;li&gt;Under a POMDP setting, Yuan et al argue that misalignment of values could impede group performance in cooperation; hence, communication plays a vital role during which a robot needs to serve as an effective listener and an expressive speaker. In the context of value alignment, they investigate how to foster effective bidirectional human-robot communications and propose an explainable artificial intelligence (XAI) system in which a collection of robots anticipates human values by using in-situ feedback while explaining their decisionmaking processes to users (see Fig. 11). Their XAI system integrates a cooperative communication model to infer human values associated with multiple desirable goals, mimic human mental dynamics, and predict optimal explanations using graphical models.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.science.org/stoken/author-tokens/ST-617/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;In situ bidirectional human-robot value alignment | Science Robotics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.science.org/doi/suppl/10.1126/scirobotics.abm4183/suppl_file/scirobotics.abm4183_sm.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.science.org/doi/suppl/10.1126/scirobotics.abm4183/suppl_file/scirobotics.abm4183_sm.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414112527794.png&#34; alt=&#34;image-20230414112527794&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414112620204.png&#34; alt=&#34;image-20230414112620204&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;game-tom--which-leverages-concepts-like-nash-equilibria&#34;&gt;game ToM , which leverages concepts like Nash equilibria&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;de Weerd et al employ a combination of computational agents and Bayesian model selection to determine the extent to which individuals use higher-order ToM reasoning in a particularly competitive game known as matching pennies. Their findings suggest that humans do not primarily employ their high-order ToM abilities.&lt;/li&gt;
&lt;li&gt;In a case study of the paperscissors-rock game, Kanwal et al develop a ToM-based agent, capable of using gestures for non-verbal communication&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lgurjcsit.lgu.edu.pk/index.php/lgurjcsit/article/view/96/90&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;View of A Step Towards the Development of Socio-cognitive Agent (lgu.edu.pk)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414113611571.png&#34; alt=&#34;image-20230414113611571&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Tejwani formalize a theory of social interactions, encompassing cooperation, conflict, coercion, competition, and trade, by extending a nested Markov decision process (MDP) where agents reason about arbitrary functions of each other&amp;rsquo;s hidden rewards.&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/2110.10298&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2110.10298] Incorporating Rich Social Interactions Into MDPs (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414114753785.png&#34; alt=&#34;image-20230414114753785&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414114919915.png&#34; alt=&#34;image-20230414114919915&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;In a follow-up study, Tejwani expand the reward function to incorporate both physical and social goals. Their method permits more complex behaviors, such as politely hindering or aggressively assisting another agent.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.mlr.press/v164/tejwani22a/tejwani22a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://proceedings.mlr.press/v164/tejwani22a/tejwani22a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Panella and Gmytrasiewicz devise a new computational framework, interactive partially observable Markov decision process (I-POMDP), wherein the agent does not explicitly model the beliefs and preferences of other agents but rather represents them as stochastic processes implemented by probabilistic deterministic finite-state controllers (PDFCs). Using Bayesian inference, the agent updates its belief over the PDFCs models of other agents.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://link.springer.com/article/10.1007/s10458-016-9359-z&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interactive POMDPs with finite-state models of other agents | SpringerLink&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deep-learning-dl&#34;&gt;Deep learning (DL)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Aru et al. point out that the problems of existing DL methods are taking shortcuts rather than learning ToM; the system may learn a much simpler decision rule . DL for ToM is explored predominantly with deep reinforcement learning (DRL), wherein the agent&amp;rsquo;s experiences and objectives are intertwined. Usually, the task&amp;rsquo;s reward structure determines what the agent accomplishes and learns. However, in the case of ToM, there may not exist a straightforward cost function or reward structure that would necessitate the emergence of ToM.&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/2203.16540&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2203.16540] Mind the gap: Challenges of deep learning approaches to Theory of Mind (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414120438385.png&#34; alt=&#34;image-20230414120438385&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Crucially, Zhao et al[160] demonstrate in a multi-agent setting that rewards may simply be a byproduct of ToM, not playing a causal role in establishing effective coordination.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;social-communication-and-cooperation&#34;&gt;Social communication and cooperation&lt;/h2&gt;
&lt;h3 id=&#34;nonverbal-communication&#34;&gt;Nonverbal communication&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Jiang et al model pointing as a communicative act between agents who have a mutual understanding that the pointed observation must be relevant and interpretable; the act of pointing is an invitation to jointly attend to an object, which elicits mutual inference between agents of each other&amp;rsquo;s minds. The proposed model measures relevance by defining a Smithian value of information (SVI) as the utility gain of a pointing signal. By integrating SVI into rational speech act (RSA), their pragmatic model of pointing permits contextually flexible interpretations.&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/2106.02003&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2106.02003] Individual vs. Joint Perception: a Pragmatic Model of Pointing as Communicative Smithian Helping (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pku.ai/publication/pointing2022cogsci/paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://pku.ai/publication/pointing2022cogsci/paper.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tang et al demonstrate that agents can successfully and robustly employ bootstrapping to converge to a joint intention from randomness under an Imagined We framework, leveraging a real-time cooperative hunting task subject to various setting manipulations.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cognitivesciencesociety.org/cogsci20/papers/0584/0584.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cognitivesciencesociety.org/cogsci20/papers/0584/0584.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Stacy et al propose a computational account of overloaded signaling from a shared agency perspective, which we refer to as the Imagined We for communication. Within this framework, communication is a means for cooperators to coordinate their perspectives, allowing them to act in concert to achieve shared objectives. In a series of simulations, the model performs effectively under growing ambiguity and increasing levels of reasoning, highlighting how shared knowledge and cooperative logic may perform the majority of the heavy lifting in language&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/2106.02164&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2106.02164] Modeling Communication to Coordinate Perspectives in Cooperation (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414153526746.png&#34; alt=&#34;image-20230414153526746&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cooperation-1&#34;&gt;Cooperation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Wang et al introduce ToM to build socially intelligent agents, who can communicate and cooperate effectively to accomplish challenging tasks. These agents determine when and with whom to reveal their intentions and sub-goals based on the inferred mental states of others.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2111.09189&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2111.09189] ToM2C: Target-oriented Multi-agent Communication and Cooperation with Theory of Mind (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414154521125.png&#34; alt=&#34;image-20230414154521125&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pöppel et al study how efficient, automatic coordination mechanisms at the level of mental states (intentions, objectives), also known as belief resonance, may lead to collaborative situated problem-solving. They describe a model of hierarchical active inference for collaborative agent (HAICA) that blends Bayesian ToM with a perception-action system based on predictive processing and active inference. Belief resonance is realized by allowing the inferred mental states of one agent influence another agent&amp;rsquo;s prediction beliefs regarding its own goals and intentions, hence influencing the agent&amp;rsquo;s task behavior without explicit collaborative reasoning.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.1007/s12559-021-09960-4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1007/s12559-021-09960-4&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414161510468.png&#34; alt=&#34;image-20230414161510468&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414161532796.png&#34; alt=&#34;image-20230414161532796&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;llm&#34;&gt;LLM&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Michal tested several language models using 40 classic false-belief tasks widely used to test ToM in humans. These findings suggest that ToM-like ability (thus far considered to be uniquely human) may have spontaneously emerged as a byproduct of language models’ improving language skills&lt;/li&gt;
&lt;li&gt;The important role of language in cognition&lt;/li&gt;
&lt;li&gt;Backwards cognition from language&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/2302.02083&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2302.02083] Theory of Mind May Have Spontaneously Emerged in Large Language Models (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;social-robot-and-cognitive-architectures&#34;&gt;Social robot and cognitive architectures&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The social robot is an interdisciplinary research field that requires comprehensive studies of social perception, ToM, and social interaction&lt;/li&gt;
&lt;li&gt;A social robot is expected to&lt;/li&gt;
&lt;li&gt;(1) develop adaptive behavioral models ,&lt;/li&gt;
&lt;li&gt;(2) be socially adept,&lt;/li&gt;
&lt;li&gt;(3) establish a natural, fluent, and effective human-like communication and interaction with humans&lt;/li&gt;
&lt;li&gt;(4) establish empathetic relationships with humans and be perceived as a teammate or a colleague rather than a tool,&lt;/li&gt;
&lt;li&gt;(5) offer proactive and parental help based on the observations and understanding of the human situation&lt;/li&gt;
&lt;li&gt;(6) build trust with humans.&lt;/li&gt;
&lt;li&gt;However, there are still many obstacles to overcome before constructing an ideal social robot.&lt;/li&gt;
&lt;li&gt;It is difficult to incorporate behavioral adaption techniques, cognitive architectures, persuasive communication strategies, and empathy into a single solution for understanding nonverbal phenomena in social interactions, as contexts are constantly changing.&lt;/li&gt;
&lt;li&gt;A common limitation of current research is that researchers have focused on a particular aspect of a social robot, such as&lt;/li&gt;
&lt;li&gt;(1) emphasizing a communication strategy,&lt;/li&gt;
&lt;li&gt;(2) studying a particular behavior as a response to human action, or&lt;/li&gt;
&lt;li&gt;(3) conducting experimental studies that include only partial factors.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cognitive-architecture&#34;&gt;Cognitive architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ASI in robotic agents relies heavily on the construction of cognitive architecture, which involves both abstract models of cognition and software instantiations of such models&lt;/li&gt;
&lt;li&gt;three classic cognitive architecture&lt;/li&gt;
&lt;li&gt;LIDA (Learning Intelligent Distribution Agent) is a cognitive architecture proposed by Stan Franklin et al. in 2005. Its core idea is to view cognitive processes as distributed processing of information, achieved through modules such as perception, attention, working memory, and long-term memory at different levels.&lt;/li&gt;
&lt;li&gt;Soar (State, Operator, and Result) is a cognitive architecture proposed by Allen Newell and Paul Rosenbloom et al. in 1983. Its core idea is to view cognitive processes as a series of state transitions for problem solving, achieved through mechanisms such as state representation, rule matching, and subgoals.&lt;/li&gt;
&lt;li&gt;ACT-R (Adaptive Control of Thought - Rational) is a cognitive architecture proposed by John Anderson et al. in 1983. Its core idea is to view cognitive processes as a series of rule-based symbol processing, achieved through the interaction between declarative and procedural knowledge.&lt;/li&gt;
&lt;li&gt;Therefore, LIDA was proposed in 2005, Soar was proposed in 1983, and ACT-R was also proposed in 1983.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;learning-intelligent-distribution-agent-lida&#34;&gt;Learning intelligent distribution agent (LIDA)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;cognitive architecture is an integrated artificial cognitive system that models a broad spectrum of biological cognition, from low-level perception and action to high-level reasoning. Two hypotheses underlie the LIDA architecture and its corresponding conceptual model:&lt;/li&gt;
&lt;li&gt;(1) Much of human cognition functions through cognitive cycles, which are interactions between conscious contents, memory systems, and action selection, occur frequently (10 Hz).&lt;/li&gt;
&lt;li&gt;(2) Cognitive cycles serve as the cognitive atoms of which higherlevel cognitive processes are composed.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-642-22887-2_14&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The LIDA Framework as a General Tool for AGI | SpringerLink&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414153338050.png&#34; alt=&#34;image-20230414153338050&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;soar&#34;&gt;Soar&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The Soar cognitive architecture is composed of interacting task-independent modules, including short-term and long-term memories, processing modules, learning mechanisms, and interfaces between them. Since Soar hypothesizes that sufficient regularities exist above the neural level to capture the functionality of the human mind, the majority of knowledge representations in Soar are symbol structures, with architecturally maintained numeric metadata biasing the retrieval and learning of those structures&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Soar_%28cognitive_architecture%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Soar (cognitive architecture) - Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;J. E. Laird, The Soar Cognitive Architecture, Cambridge, MA, USA: MIT Press, 2019.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;adaptive-control-of-thought-rationale-architecture-act-r&#34;&gt;Adaptive control of thought-rationale architecture (ACT-R)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;includes modules such as&lt;/li&gt;
&lt;li&gt;(1) a visual module for identifying objects in the visual field,&lt;/li&gt;
&lt;li&gt;(2) a manual module for controlling the hands,&lt;/li&gt;
&lt;li&gt;(3) a declarative module for retrieving information from memory,&lt;/li&gt;
&lt;li&gt;(4) a goal module for tracking current goals and intentions, and&lt;/li&gt;
&lt;li&gt;(5) a central production system to coordinate these modules.&lt;/li&gt;
&lt;li&gt;There are buffers within each module that transmit information back and forth to the central production system. The architecture assumes a mixture of serial and parallel processing&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/actr-a-higherlevel-account-of-processing-capacity/73969664D69FB461CC04097B6D49FC77&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ACT-R: A higher-level account of processing capacity | Behavioral and Brain Sciences | Cambridge Core&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://psycnet.apa.org/doiLanding?doi=10.1037%2F0033-295X.111.4.1036&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Integrated Theory of the Mind. (apa.org)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cognitive-architectures-in-social-robots&#34;&gt;Cognitive architectures in social robots&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Wiltshire et al discuss the problem of engineering human social-cognitive mechanisms to enable robot social intelligence and provide an integrative perspective of social cognition as a systematic theoretical underpinning for computational instantiations of these mechanisms. They also provide a series of recommendations to facilitate the development of the perceptual, motor, and cognitive architecture.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S1389041716300493?via%3Dihub&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Enabling robotic social intelligence by engineering human social-cognitive mechanisms - ScienceDirect&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Breazeal et al provide an integrated sociocognitive architecture (see Fig. 14) to endow an anthropomorphic robot with the ability to infer mental states such as beliefs, intents, and desires from the observable behavior of its human partner via simulation-theoretic techniques.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414173012629.png&#34; alt=&#34;image-20230414173012629&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kennedy et al describe an approach known as a like-me simulation, in which the agent uses its own knowledge and capabilities as a model of another agent to predict that agent’s actions. They present three examples of a likeme mental simulation in a social context implemented in the embodied version of the adaptive control of thought-rationale architecture (ACT-R) cognitive architecture, ACT-R Embodied (ACT-R/E), including perspective taking, teamwork, and dominant-submissive social behavior.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://link.springer.com/article/10.1007/s12369-009-0014-6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Like-Me” Simulation as an Effective and Cognitively Plausible Basis for Social Robotics | SpringerLink&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Moulin-Frier et al suggest the DAC-h3 architecture, which incorporates a reactive interaction engine, a number of state-of-the-art perceptual and motor learning algorithms, planning capabilities, and an autobiographical memory. The architecture as a whole drives the robot’s behavior to solve the symbol grounding problem, acquire language capabilities, perform goal-oriented behavior, and articulate a verbal narrative of its own experience in the world.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.1109/TCDS.2017.2754143&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1109/TCDS.2017.2754143&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Franchi et al[179] present a brain-inspired architecture, the intentional distributed robotic architecture (IDRA), which aims to permit the autonomous development of new goals in situated agents beginning with simple hard-coded instincts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/01691864.2016.1172732&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;From learning to new goal generation in a bioinspired robotic setup: Advanced Robotics: Vol 30, No 11-12 (tandfonline.com)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;discussions&#34;&gt;Discussions&lt;/h1&gt;
&lt;h2 id=&#34;recent-advances-in-datasets-and-environments&#34;&gt;Recent advances in datasets and environments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PHASE:&lt;/strong&gt; Netanyahu et al[180] resemble a collection of physically-grounded abstract social events (PHASE) that simulates a wide variety of real-world social interactions by incorporating social concepts, such as helping another agent. PHASE is comprised of 2D animations of agent pairs, moving in continuous space with multiple objects and landmarks, generated procedurally by a physics engine and a hierarchical planner.&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/2103.01933&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2103.01933] PHASE: PHysically-grounded Abstract Social Events for Machine Social Perception (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AGENT:&lt;/strong&gt; Inspired by intuitive psychology, Shu et al[181] present a benchmark consisting of a large dataset of procedurally generated 3D animations, Action, Goal, Efficiency, coNstraint, uTility (AGENT), structured around four scenarios (goal preferences, action efficiency, unobserved constraints, and cost-reward tradeoffs) that probe key concepts of core intuitive psychology&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/2102.12321&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2102.12321] AGENT: A Benchmark for Core Psychological Reasoning (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;WAH:&lt;/strong&gt; Puig et al[182] introduced watch-and-help (WAH), a challenge for testing social intelligence in agents, wherein an AI agent is tasked to help a human-like agent perform a complex household task efficiently. They build VirtualHomeSocial, a multi-agent household environment, and provide a benchmark including both planning and learning-based baselines.&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/2010.09890&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2010.09890] Watch-And-Help: A Challenge for Social Perception and Human-AI Collaboration (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sap et al proposed a dataset to evaluate language-based commonsense reasoning about social interactions, including reasoning about motivation and about emotional reactions .&lt;/li&gt;
&lt;li&gt;[&lt;a href=&#34;https://arxiv.org/abs/1904.09728&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;1904.09728] SocialIQA: Commonsense Reasoning about Social Interactions (arxiv.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;**Hanabi: ** Bard et al propose the cooperative and imperfect information card game, Hanabi, as a challenging benchmark. It requires reasoning about the beliefs and the intentions of other players, focusing on the ad-hoc setting where an agent has to coordinate with a team they encounter for the first time.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0004370219300116?via%3Dihub&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Hanabi challenge: A new frontier for AI research - ScienceDirect&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414181418930.png&#34; alt=&#34;image-20230414181418930&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation-protocols&#34;&gt;Evaluation protocols&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;the formation of universally accepted criteria for the design and implementation of ASI benchmarks and the accompanying evaluation protocols is still in its infancy and represents a significant barrier to the field&amp;rsquo;s continued progress.&lt;/li&gt;
&lt;li&gt;Because human judgments can be ambiguous and difficult to express, many social intelligence tasks do not include requirements that can be easily captured using hand-crafted rules.&lt;/li&gt;
&lt;li&gt;The Turing test is a test of a machine&amp;rsquo;s ability to exhibit intelligent behavior equivalent to or indistinguishable from that of a human.&lt;/li&gt;
&lt;li&gt;However, current systems that perform well on these tests typically do so by employing techniques that are not generalizable to other problems.&lt;/li&gt;
&lt;li&gt;Other approaches for assessing social intelligence competency are often derived from various sources, such as peer-/superior-/self-ratings and observers’ behavioral assessments&lt;/li&gt;
&lt;li&gt;Notably, the Animal-AI Olympics[187] is initiated by testing artificial agents on tasks derived directly from animal cognition research in an effort to establish common ground&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/s42256-019-0050-3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Animal-AI Olympics | Nature Machine Intelligence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/yuechuhaoxi020609/images/main/imgs/image-20230414184518872.png&#34; alt=&#34;image-20230414184518872&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;An important aspect of the ASI is to measure cognitive skills, adaptability, and meta-level learning and reasoning ability rather than specific problem-solving ability.&lt;/li&gt;
&lt;li&gt;Using more abstract cognitive processes, such as the ability to&lt;/li&gt;
&lt;li&gt;(1) transfer information from one domain to another,&lt;/li&gt;
&lt;li&gt;(2) retain information for extended periods, and&lt;/li&gt;
&lt;li&gt;(3) correct errors in performance, may be future effective strategies for assessing ASI.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;future-trends&#34;&gt;Future trends&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A holistic approach&lt;/strong&gt;: Cognitive and neuroscience research shows that while distinct brain regions are involved in specific tasks, a core network is involved in all ToM tasks, suggesting that humans take a more holistic approach to social intelligence than existing computational models, which often focus on a single aspect of the problem.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning methods&lt;/strong&gt;: Infants develop intelligence gradually. This suggests that learning, and in particular lifelong/continuous learning , is a crucial path for developing ASI. The objective of lifelong/continuous learning is to successively learn a model for a large number of activities without forgetting the knowledge acquired from the previous tasks. Other potentially effective learning strategies include multi-task learning, one-/few-shot learning, and meta-learning .&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Open-ended and interactive environment&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Human biases&lt;/strong&gt;:  We must also introduce better biases, even structural biases, as a form of built-in common sense, as there may be multiple biases and limits in the human brain that facilitate the acquisition of social intelligence. For instance, there may be innate biases of attention to the human face, speech, hands, eyes, gaze-direction, and biological motion, and these early biases ensure that the infant learns about the components of the world that provide information about the minds of other people.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;ASI is a crucial missing component for artificial general intelligence (AGI) on par with humans and symbolizes the future path of AI.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Building machines that learn and think like people</title>
      <link>https://example.com/post/build-humanlike-machine/</link>
      <pubDate>Mon, 20 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/build-humanlike-machine/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;h2 id=&#34;two-different-computational-approaches-to-intelligence&#34;&gt;two different computational approaches to intelligence&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;pattern recognition approach treats prediction as primary, usually in the context of a specific classification, regression, or control task.  discovering features that have high-value states in common across a large, diverse set of training data.&lt;/li&gt;
&lt;li&gt;model building. Cognition is about using these models to understand the world, to explain, to imagine what could have happened that didn’t, or what could be true that isn’t, and then planning.&lt;/li&gt;
&lt;li&gt;prediction and explanation, is central to our view of human intelligence.  pattern recognition can support model building, through “model-free” algorithms that learn through experience how to make essential inferences more computationally efficient&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-this-article-is-not&#34;&gt;What this article is not&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;we believe that reverse engineering human intelligence  can usefully inform AI and machine learning.&lt;/li&gt;
&lt;li&gt;avoiding cognitive or neural inspiration as well as claims of cognitive or neural plausibility  is a  approach to developing AI. But this article has little pertinence to this approach.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overview-of-the-key-ideas&#34;&gt;Overview of the key ideas&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;propose a set of core ingredients for building more human-like learning and thinking machines&lt;/li&gt;
&lt;li&gt;mainly in Section 4 Core ingredients of human intelligence&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;cognitive-and-neural-inspiration-in-artificial-intelligence&#34;&gt;Cognitive and neural inspiration in artificial intelligence&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;behaviorist view&lt;/li&gt;
&lt;li&gt;Cognitive science&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;pdp-approach&#34;&gt;PDP approach&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;knowledge is thus distributed across the collection of units rather than localized as in most symbolic data structures.&lt;/li&gt;
&lt;li&gt;Neural network models and the PDP approach offer a view of the mind (and intelligence more broadly) that is sub-symbolic and often populated with minimal constraints and inductive biases to guide learning.&lt;/li&gt;
&lt;li&gt;Proponents of this approach maintain that many classic types of structured knowledge, such as graphs, grammars can be useful yet misleading metaphors for characterizing thought.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;question&#34;&gt;Question&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;neural networks have such broad application in machine vision, language, and control, and they can be trained to emulate the rule-like and structured behaviors that characterize cognition&lt;/li&gt;
&lt;li&gt;do we need more to develop truly human-like learning and thinking machines?&lt;/li&gt;
&lt;li&gt;How far can relatively generic neural networks bring us toward this goal?&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;challenges-for-building-more-human-like-machines&#34;&gt;Challenges for building more human-like machines&lt;/h1&gt;
&lt;h2 id=&#34;the-characters-challenge&#34;&gt;The Characters Challenge&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;learning simple visual concepts&lt;/li&gt;
&lt;li&gt;People learn a lot more from a lot less, and capturing these human-level learning abilities in machines is the Characters Challenge&lt;/li&gt;
&lt;li&gt;Although humans and neural networks may perform equally well on the MNIST digit recognition task and other large-scale image classification tasks, it does not mean that they learn and think in the same way.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;two-important-differences-between-cnn-and-human-in-learning-simple-visual-concepts&#34;&gt;two important differences between CNN and human in learning simple visual concepts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;people learn from fewer examples and they learn richer representations&lt;/li&gt;
&lt;li&gt;people learn more than how to do pattern recognition: they learn a concept, that is, a model of the class that allows their acquired knowledge to be flexibly applied in new ways.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-difficulty&#34;&gt;Some difficulty&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A single example of a new visual concept (red box) can be enough information to support the&lt;/li&gt;
&lt;li&gt;classification of new examples&lt;/li&gt;
&lt;li&gt;generation of new examples&lt;/li&gt;
&lt;li&gt;parsing an object into parts and relations&lt;/li&gt;
&lt;li&gt;generation of new concepts from related concepts.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-frostbite-challenge&#34;&gt;The Frostbite Challenge&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;learning to play the Atari game Frostbite&lt;/li&gt;
&lt;li&gt;Failed on accomplishing a sub-goal (such as reaching an ice floe) and then safely proceed to the next sub-goal&lt;/li&gt;
&lt;li&gt;the policy are highly specialized for the games it was trained on&lt;/li&gt;
&lt;li&gt;considering the amount of experience required for learning&lt;/li&gt;
&lt;li&gt;non-professional humans can grasp the basics of the game after just a few minutes of play.&lt;/li&gt;
&lt;li&gt;people do this by inferring a general schema to describe the goals of the game and the object types and their interactions, using the kinds of intuitive theories, model-building abilities and model-based planning mechanisms we describe below.&lt;/li&gt;
&lt;li&gt;the game of Frostbite provides incremental rewards for reaching each active ice floe, providing the DQN with the relevant sub-goals for completing the larger task of building an igloo.&lt;/li&gt;
&lt;li&gt;Without these sub-goals, the DQN would have to take random actions until it accidentally builds an igloo and is rewarded for completing the entire level.&lt;/li&gt;
&lt;li&gt;Human is possible to figure out the higher-level goal of building an igloo without incremental feedback;&lt;/li&gt;
&lt;li&gt;sparse feedback is a source of difficulty in other Atari 2600 games such as Montezuma’s Revenge&lt;/li&gt;
&lt;li&gt;inflexible to changes in its inputs and goals. Changing the color or appearance of objects or changing the goals of the network would have devastating consequences on performance if the network is not retrained&lt;/li&gt;
&lt;li&gt;In contrast, people require little or no retraining or reconfiguration, adding new tasks and goals to their repertoire with relative ease.&lt;/li&gt;
&lt;li&gt;Humans as a result often have important domain-specific knowledge for these tasks, even before they ‘begin.’ The DQN is starting completely from scratch.&lt;/li&gt;
&lt;li&gt;How do we bring to bear rich prior knowledge to learn new tasks and solve new problems so quickly?&lt;/li&gt;
&lt;li&gt;What form does that prior knowledge take, and how is it constructed, from some combination of inbuilt capacities and previous experience?&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;core-ingredients-of-human-intelligence&#34;&gt;Core ingredients of human intelligence&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Whether learned, built in, or enriched, the key claim is that these ingredients play an active and important role in producing human-like learning and thought, in ways contemporary machine learning has yet to capture.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;developmental-start-up-software&#34;&gt;Developmental start-up software&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Early in development, humans have a foundational understanding of several core domains including number (numerical and set operations), space (geometry and navigation), physics (inanimate objects and mechanics), and psychology (agents and groups).&lt;/li&gt;
&lt;li&gt;The underlying cognitive representations can be understood as “intuitive theories,” with a causal structure resembling a scientific theory&lt;/li&gt;
&lt;li&gt;focus on the early understanding of objects and agents.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;intuitive-physics&#34;&gt;Intuitive physics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;At the age of 2 months, and possibly earlier, human infants expect inanimate objects to follow principles of persistence, continuity, cohesion, and solidity.&lt;/li&gt;
&lt;li&gt;At around 6 months, infants have already developed different expectations for rigid bodies, soft bodies, and liquids&lt;/li&gt;
&lt;li&gt;By their first birthday, infants have gone through several transitions of comprehending basic physical concepts such as inertia, support, containment, and collisions&lt;/li&gt;
&lt;li&gt;There is no single agreed-upon computational account of these early physical principles and concepts, and previous suggestions have ranged from decision trees, to cues, to lists of rules&lt;/li&gt;
&lt;li&gt;A promising recent approach sees intuitive physical reasoning as similar to inference over a physics software engine&lt;/li&gt;
&lt;li&gt;This “intuitive physics engine” approach enables flexible adaptation to a wide range of everyday scenarios and judgments in a way that goes beyond perceptual cues.&lt;/li&gt;
&lt;li&gt;What are the prospects for embedding or acquiring this kind of intuitive physics in deep learning systems?&lt;/li&gt;
&lt;li&gt;Connectionist models in psychology have previously been applied to physical reasoning tasks such as balance-beam rules (McClelland 1988; Shultz 2003) or rules relating to distance, velocity, and time in motion&lt;/li&gt;
&lt;li&gt;PhysNet In contrast, people require far less experience to perform any particular task, and can generalize to many novel judgments and complex scenes with no new training required (although they receive large amounts of physics experience through interacting with the world more generally).&lt;/li&gt;
&lt;li&gt;Could deep learning systems such as PhysNet capture this flexibility, without explicitly simulating the causal interactions between objects in three dimensions?&lt;/li&gt;
&lt;li&gt;Whether such models can be learned with the kind (and quantity) of data available to human infants is not clear&lt;/li&gt;
&lt;li&gt;But incorporating a physics-engine–based representation could help DQNs learn to play games such as Frostbite in a faster and more general way, whether the physics knowledge is captured implicitly in a neural network or more explicitly in a simulator.&lt;/li&gt;
&lt;li&gt;When a new object type such as a bear is introduced, as in the later levels of Frostbite (Fig. 2D), a network endowed with intuitive physics would also have an easier time adding this object type to its knowledge&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;challenges&#34;&gt;Challenges&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;For networks trained on object classification,deeper layers often become sensitive to successively higher-level features, from edges to textures to shapeparts to full objects.&lt;/li&gt;
&lt;li&gt;For deep networks trained on physics-related data,it remains to be seen whether higher layers will encode objects, general physical properties, forces, and approximately Newtonian dynamics.&lt;/li&gt;
&lt;li&gt;would it generalize broadly beyond training contexts as people’s more explicit physical concepts do?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;intuitive-psychology&#34;&gt;Intuitive psychology&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Pre-verbal infants distinguish animate agents from inanimate objects. This distinction is partially based on innate or early-present detectors for low-level cues, such as the presence of eyes, motion initiated from rest, and biological motion&lt;/li&gt;
&lt;li&gt;Beyond these low-level cues, infants also expect agents to act contingently and reciprocally, to have goals, and to take efficient actions toward those goals subject to constraints&lt;/li&gt;
&lt;li&gt;It is generally agreed that infants expect agents to act in a goal-directed, efficient, and socially sensitive fashion&lt;/li&gt;
&lt;li&gt;One possibility is that intuitive psychology is simply cues “all the way down” This inference could be captured by a cue that states &amp;ldquo;If an agent’s expected trajectory is prevented from completion, the blocking agent is given some negative association.&lt;/li&gt;
&lt;li&gt;One alternative to a cue-based account is to use generative models of action choice, as in the Bayesian inverse planning, or Bayesian theory of mind (ToM), models of Baker et al. (2009) or the naive utility calculus models of Jara-Ettinger et al.&lt;/li&gt;
&lt;li&gt;These models formalize explicitly mentalistic concepts such as “goal,” “agent,”  “planning,” “cost,” “efficiency,” and “belief,” used to describe core psychological  reasoning in infancy.&lt;/li&gt;
&lt;li&gt;They assume adults and children treat agents as approximately rational planners who choose the most efficient means to their goals.&lt;/li&gt;
&lt;li&gt;By simulating these planning processes, people can predict what agents might do next, or use inverse reasoning from observing a series of actions to infer the utilities and beliefs of agents in a scene.&lt;/li&gt;
&lt;li&gt;Importantly, unlike in intuitive physics, simulation-based reasoning in intuitive psychology can be nested recursively to understand social interactions. We can think about agents thinking about other agents.&lt;/li&gt;
&lt;li&gt;Although deep networks have not yet been applied to scenarios involving theory of mind and intuitive psychology, they could probably learn visual cues, heuristics and summary statistics of a scene that happens to involve agents.&lt;/li&gt;
&lt;li&gt;However, it seems to us that any full formal account of intuitive psychological reasoning needs to include representations of agency, goals, efficiency, and reciprocal relations.&lt;/li&gt;
&lt;li&gt;Connectionists have argued that innate constraints in the form of hard-wired cortical circuits are unlikely , but a simple inductive bias, for example, the tendency to notice things that move other things, can bootstrap reasoning about more abstract concepts of agency&lt;/li&gt;
&lt;li&gt;Similarly, a great deal of goal-directed and socially directed actions can also be boiled down to a simple utility calculus, in a way that could be shared with other cognitive abilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;learning-as-rapid-model-building&#34;&gt;Learning as rapid model building&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;There are many learning algorithms for neural networks, including the perceptron algorithm (Rosenblatt 1958), Hebbian learning (Hebb 1949), the BCM rule (Bienenstock et al. 1982), backpropagation (Rumelhart et al. 1986a), the wake-sleep algorithm (Hinton et al. 1995), and contrastive divergence (Hinton 2002).&lt;/li&gt;
&lt;li&gt;Human “one-shot” learning&lt;/li&gt;
&lt;li&gt;Neural network sdata hungry&lt;/li&gt;
&lt;li&gt;the types of things that children learn as the meanings of words – people are still far better learners than machines.&lt;/li&gt;
&lt;li&gt;Even with just a few examples, people can learn remarkably rich conceptual models.&lt;/li&gt;
&lt;li&gt;Beyond classification, concepts support prediction, action, communication, imagination , explanation, and composition.&lt;/li&gt;
&lt;li&gt;In addition to evaluating several types of deep learning models, we developed an algorithm using Bayesian program learning (BPL) that represents concepts as simple stochastic programs: structured procedures that generate new examples of a concept when executed&lt;/li&gt;
&lt;li&gt;These programs allow the model to express causal knowledge about how the raw data are formed, and the probabilistic semantics allow the model to handle noise and perform creative tasks. Structure sharing across concepts is accomplished by the compositional re-use of stochastic primitives that can combine in new ways to create new concepts.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;compositionality&#34;&gt;Compositionality&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Compositionality is the classic idea that new representations can be constructed through the combination of primitive elements.&lt;/li&gt;
&lt;li&gt;Compositionality is also at the core of productivity: an infinite number of representations can be constructed from a finite set of primitives&lt;/li&gt;
&lt;li&gt;To capture the full extent of the mind’s compositionality, a model must include explicit representations of objects, identity, and relations, all while maintaining a notion of “coherence” when understanding novel configurations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;causality&#34;&gt;Causality&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;To capture the full extent of the mind’s compositionality, a model must include explicit representations of objects, identity, and relations, all while maintaining a notion of “coherence” when understanding novel configurations.&lt;/li&gt;
&lt;li&gt;Concept learning and vision models that use causality are usually generative but not every generative model is also causal.&lt;/li&gt;
&lt;li&gt;Causality has been influential in theories of perception. “Analysis-by-synthesis” theories of perception maintain that sensory data can be more richly represented by modeling the process that generated it&lt;/li&gt;
&lt;li&gt;Causal knowledge has also been shown to influence how people learn new concepts; providing a learner with different types of causal knowledge changes how he or she learns and generalizes.&lt;/li&gt;
&lt;li&gt;To explain the role of causality in learning, conceptual representations have been likened to intuitive theories or explanations, providing the glue that lets core features stick, whereas other equally applicable features wash away.&lt;/li&gt;
&lt;li&gt;Beyond concept learning, people also understand scenes by building causal models.&lt;/li&gt;
&lt;li&gt;There have been steps toward deep neural networks and related approaches that learn causal models.&lt;/li&gt;
&lt;li&gt;Incorporating causality may greatly improve these deep learning models; they were trained without access to causal data about how characters are actually produced, and without any incentive to learn the true causal process.&lt;/li&gt;
&lt;li&gt;A causal model of Frostbite would have to be more complex, gluing together object representations and explaining their interactions with intuitive physics and intu-
itive psychology, much like the game engine that generates the game dynamics and, ultimately, the frames of pixel images.&lt;/li&gt;
&lt;li&gt;Inference is the process of inverting this causal generative model, explaining the raw pixels as objects and their interactions, such as the agent stepping on an ice floe to deactivate it or a crab pushing the agent into the water.&lt;/li&gt;
&lt;li&gt;Deep neural networks could play a role in two ways: by serving as a bottom-up proposer to make probabilistic inference more tractable in a structured generative model or by serving as the causal generative model if imbued with the right set of ingredients.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;learning-to-learn&#34;&gt;Learning-to-learn&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;When humans or machines make inferences that go far beyond the data, strong prior knowledge (or inductive biases or constraints) must be making up the difference.&lt;/li&gt;
&lt;li&gt;One way people acquire this prior knowledge is through “learning-to-learn,” a term introduced by Harlow (1949) and closely related to the machine learning notions of “transfer learning,” “multitask learning,” and “representation learning.”&lt;/li&gt;
&lt;li&gt;The strong priors, constraints, or inductive bias needed to learn a particular task quickly are often shared to some extent with other related tasks.&lt;/li&gt;
&lt;li&gt;In hierarchical Bayesian modeling, a general prior on concepts is shared by multiple specific concepts, and the prior itself is learned over the course of learning the specific concepts&lt;/li&gt;
&lt;li&gt;In machine vision, for deep convolutional networks or other discriminative methods that form the core of recent recognition systems, learning-to-learn can occur through the sharing of features between the models learned for old objects or old tasks and the models learned for new objects or new tasks&lt;/li&gt;
&lt;li&gt;We cannot be sure how people get to the knowledge they have in this domain, but we do understand how this works in BPL, and we think people might be similar.&lt;/li&gt;
&lt;li&gt;BPL transfers readily to new concepts because it learns about object parts, sub-parts, and relations, capturing learning about what each concept is like and what concepts are like in general.  It is crucial that learning-to-learn occurs at multiple levels of the hierarchical generative process.&lt;/li&gt;
&lt;li&gt;Further transfer occurs by learning about the typical levels of variability within a typical generative model. This provides knowledge about how far and in what ways to generalize when we have seen only one example of a new character, which on its own could not possibly carry any information about variance.&lt;/li&gt;
&lt;li&gt;In the Frostbite Challenge, and in video games more generally, there is a similar interdependence between the form of the representation and the effectiveness of learning-to-learn.&lt;/li&gt;
&lt;li&gt;general world knowledge and previous video games may help inform exploration and generalization in new scenarios, helping people learn maximally from a single mistake or avoid mistakes altogether&lt;/li&gt;
&lt;li&gt;Deep reinforcement learning systems for playing Atari games have had some impressive successes in transfer learning, but they still have not come close to learning to play new games as quickly as humans can. For example, Parisotto et al. (2016) present the “actor-mimic” algorithm&lt;/li&gt;
&lt;li&gt;In sum, the interaction between representation and previous experience may be key to building machines that learn as fast as people.&lt;/li&gt;
&lt;li&gt;if such a system aims to learn compositionally structured causal models of each game – built on a foundation of intuitive physics and psychology – it could transfer knowledge more efficiently and thereby learn new games much more quickly.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;thinking-fast&#34;&gt;Thinking Fast&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In general, richer and more structured models require more complex and slower inference algorithms, similar to how complex models require more data, making the speed of perception and thought all the more remarkable.&lt;/li&gt;
&lt;li&gt;The combination of rich models with efficient inference suggests another way psychology and neuroscience may usefully inform AI.&lt;/li&gt;
&lt;li&gt;This section discusses possible paths toward resolving the conflict between fast inference and structured representations, including Helmholtz machine–style approximate inference in generative models and cooperation between model-free and model-based reinforcement learning systems&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;approximate-inference-in-structured-models&#34;&gt;Approximate inference in structured models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In contrast, whereas representing intuitive theories and structured causal models is less natural in deep neural networks, recent progress has demonstrated the remarkable effectiveness of gradient-based learning in high-dimensional parameter spaces.&lt;/li&gt;
&lt;li&gt;A complete account of learning and inference must explain how the brain does so much with limited computational resources&lt;/li&gt;
&lt;li&gt;Popular algorithms for approximate inference in probabilistic machine learning have been proposed as psychological models&lt;/li&gt;
&lt;li&gt;Most prominently, it has been proposed that humans can approximate Bayesian inference using Monte Carlo methods, which stochastically sample the space of possible hypotheses and evaluate these samples according to their consistency with the data and prior knowledge&lt;/li&gt;
&lt;li&gt;Although Monte Carlo methods are powerful and come with asymptotic guarantees, it is challenging to make them work on complex problems like program induction and theory learning.&lt;/li&gt;
&lt;li&gt;Discovering new theories can be slow and arduous, as testified by the long time scale of cognitive development, and learning in a saltatory fashion (rather than through gradual adaptation) is characteristic of aspects of human intelligence, including discovery and insight during development (Schulz 2012b), problem-solving (Sternberg &amp;amp; Davidson 1995), and epoch-making discoveries in scientific research (Langley et al. 1987).&lt;/li&gt;
&lt;li&gt;Dis-covering new theories can also occur much more quickly. A person learning the rules of Frostbite will probably undergo a loosely ordered sequence of “Aha!” moments:&lt;/li&gt;
&lt;li&gt;These little fragments of a “Frostbite theory” are assembled to form a causal understanding of the game relatively quickly, in what seems more like a guided process than arbitrary proposals in a Monte Carlo inference scheme&lt;/li&gt;
&lt;li&gt;For domains where program or theory learning occurs quickly, it is possible that people employ inductive biases not only to evaluate hypotheses, but also to guide hypothesis selection.&lt;/li&gt;
&lt;li&gt;How might efficient mappings from questions to a plausible subset of answers be learned?  spanning both deep learning and graphical models, has attempted to tackle this challenge by “amortizing” probabilistic inference computations into an efficient feed-forward mapping&lt;/li&gt;
&lt;li&gt;These feed-forward mappings can be learned in various ways, for example, using paired generative/recognition networks and variational optimization, or nearest-neighbor density estimation&lt;/li&gt;
&lt;li&gt;One implication of amortization is that solutions to different problems will become correlated because of the sharing of amortized computations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model-based-and-model-free-reinforcement-learning&#34;&gt;Model-based and model-free reinforcement learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;There is indeed substantial evidence that the brain uses similar model-free learning algorithms in simple associative learning or discrimination learning tasks&lt;/li&gt;
&lt;li&gt;Considerable evidence suggests that the brain also has a model-based learning system, responsible for building a “cognitive map” of the environment and using it to plan action sequences for more complex tasks Model-based planning is an essential ingredient of human intelligence, enabling flexible adaptation to new tasks and goals;&lt;/li&gt;
&lt;li&gt;One boundary condition on this flexibility is the fact that the skills become “habitized” with routine application,  possibly reflecting a shift from model-based to model-free control. This shift may arise from a rational arbitration between learning systems to balance the trade-off between flexibility and speed&lt;/li&gt;
&lt;li&gt;Similarly to how probabilistic computations can be amortized for efficiency (see previous section), plans can be amortized into cached values by allowing the model-based system to simulate training data for the model-free system&lt;/li&gt;
&lt;li&gt;Intrinsic motivation also plays an important role in human learning and behavior&lt;/li&gt;
&lt;li&gt;all externally provided rewards are reinterpreted according to the “internal value” of the agent,&lt;/li&gt;
&lt;li&gt;There may also be an intrinsic drive to reduce uncertainty and construct models of the 8environment&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;responses-to-common-questionswhat-we-believe&#34;&gt;Responses to common questions(What we believe)&lt;/h1&gt;
&lt;h2 id=&#34;comparing-the-learning-speeds-of-humans-and-neural-networks-on-specific-tasks-is-not-meaningful-because-humans-have-extensive-prior-experience&#34;&gt;Comparing the learning speeds of humans and neural networks on specific tasks is not meaningful, because humans have extensive prior experience&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;If deep learning researchers see themselves as trying to capture the equivalent of humans’ collective evolutionary experience, this would be equivalent to a truly immense “pre-training” phase.&lt;/li&gt;
&lt;li&gt;We are less committed to a particular story regarding the origins of the ingredients,&lt;/li&gt;
&lt;li&gt;successful learning-to-learn – or, at least, human-level transfer learning – is enabled by having models with the right representational structure, including the other building blocks discussed in this article.&lt;/li&gt;
&lt;li&gt;To build these representations from scratch might require exploring fundamental structural variations in the network’s architecture, which gradient-based learning in weight space is not prepared to do. Although deep learning researchers do explore many such architectural variations&lt;/li&gt;
&lt;li&gt;dynamics of structure search may look much more like the slow random hill climbing of evolution than the smooth, methodical progress of stochastic gradient descent.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;biological-plausibility-suggests-theories-of-intelligence-should-start-with-neural-networks&#34;&gt;Biological plausibility suggests theories of intelligence should start with neural networks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We have focused on how cognitive science can motivate and guide efforts to engineer human-like AI, in contrast to some advocates of deep neural networks who cite neuro-science for inspiration.&lt;/li&gt;
&lt;li&gt;Unfortunately, what we “know” about the brain is not all that clear-cut.&lt;/li&gt;
&lt;li&gt;Hebbian learning is another case in point. In the form of long-term potentiation (LTP) and spike-timing dependent plasticity (STDP), Hebbian learning mechanisms are often cited as biologically supported.&lt;/li&gt;
&lt;li&gt;Most relevantly for our focus, it would be especially challenging to try to implement the ingredients described in this article using purely Hebbian mechanisms.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;language-is-essential-for-human-intelligence-why-is-it-not-more-prominent-here&#34;&gt;Language is essential for human intelligence. Why is it not more prominent here?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We have said little in this article about people’s ability to communicate and think in natural language, a distinctively human cognitive capacity where machine capabilities strikingly lag.&lt;/li&gt;
&lt;li&gt;We believe that understanding language and its role in intelligence goes hand-in-hand with understanding the building blocks discussed in this article.&lt;/li&gt;
&lt;li&gt;These capacities are in place before children master language, and they provide the building blocks for linguistic meaning and language acquisition&lt;/li&gt;
&lt;li&gt;Is it recursion, or some new kind of recursive structure uilding ability? Is it the ability to re-use symbols by name ? Is it the ability to understand others intentionally and build shared intentionality ? Is it some new version of these things, or is it just more of the aspects of these capacities that are already present in infants?&lt;/li&gt;
&lt;li&gt;But with language, older children become able to reason about a much wider range of physical and psychological situations . Language also facilitates more powerful learning-to-learn and compositionality.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;looking-forward&#34;&gt;Looking forward&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Machine performance may rival or exceed human performance on particular tasks, and algorithms may take inspiration from neuroscience or aspects of psychology, but it does not follow that the algorithm learns or thinks like a person. This is a higher bar worth reaching for, potentially leading to more powerful algorithms, while also helping unlock the mysteries of the human mind.&lt;/li&gt;
&lt;li&gt;When comparing people with the current best algorithms in AI and machine learning, people learn from fewer data and generalize in richer and more flexible ways.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;promising-directions-in-deep-learning&#34;&gt;Promising directions in deep learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;There has been recent interest in integrating psychological ingredients with deep neural networks, especially selective   attention, augmented working memory, and experience replay.&lt;/li&gt;
&lt;li&gt;Paralleling the human perceptual apparatus, selective  attention forces deep learning models to process raw, perceptual data as a series of high-resolution “foveal glimpses” rather than all at once.&lt;/li&gt;
&lt;li&gt;Attention may help these models in several ways. It helps to coordinate complex, often sequential, outputs by attending to only specific aspects of the input, allowing the model to focus on smaller sub-tasks rather than solving an entire problem in one shot.&lt;/li&gt;
&lt;li&gt;Attention also allows larger models to be trained without requiring every model parameter to affect every output or action.&lt;/li&gt;
&lt;li&gt;Researchers are also developing neural networks with “working memories” that augment the shorter-term memory provided by unit activation and the longer-term memory provided by the connection weights&lt;/li&gt;
&lt;li&gt;Each model seems to learn genuine programs from examples, albeit in a representation more like assembly language than a high-level programming language.&lt;/li&gt;
&lt;li&gt;differentiable programming suggests the intriguing possibility of combining the best of program induction and deep learning.&lt;/li&gt;
&lt;li&gt;Another example of combining pattern recognition and model-based search comes from recent AI research into the game Go.&lt;/li&gt;
&lt;li&gt;One worthy goal would be to build an AI system that beats a world-class player with the amount and kind of training human champions receive, rather than overpowering them with Google-scale computational resources.&lt;/li&gt;
&lt;li&gt;Although techniques for handling variable-sized inputs in ConvNets may help in playing on different board sizes, the value functions and policies that AlphaGo learns seem unlikely to generalize as flexibly and automatically as people.&lt;/li&gt;
&lt;li&gt;the fact that it cannot even conceive of these variants, let alone adapt to them autonomously, is a sign that it does not understand the game as humans do.&lt;/li&gt;
&lt;li&gt;Humans represent their strategies as a response to these constraints, such that if the game changes, they can begin to adjust their strategies accordingly.&lt;/li&gt;
&lt;li&gt;We believe it would be richly rewarding for AI and cognitive science to pursue this challenge together and that such systems could be a compelling testbed for the principles this article suggests, as well as building on all of the progress to date that AlphaGo represents.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;future-applications-to-practical-ai-problems&#34;&gt;Future applications to practical AI problems&lt;/h2&gt;
&lt;h3 id=&#34;scene-understanding&#34;&gt;Scene understanding&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Deep learning is moving beyond object recognition and toward scene understanding, as evidenced by a flurry of recent work focused on generating natural language captions for images&lt;/li&gt;
&lt;li&gt;Yet current algorithms are still better at recognizing objects than understanding scenes, often getting the key objects right but their causal relationships wrong&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;autonomous-agents-and-intelligent-devices&#34;&gt;Autonomous agents and intelligent devices&lt;/h3&gt;
&lt;h3 id=&#34;autonomous-driving&#34;&gt;Autonomous driving&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Similarly, other drivers on the road have similarly complex mental states underlying their behavior&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;creative-design&#34;&gt;Creative design&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Although we are still far from developing AI systems that can tackle these types of tasks, we see compositionality and causality as central to this goal.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;toward-more-human-like-learning-and-thinking-machines&#34;&gt;Toward more human-like learning and thinking machines&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;we suggest that deep learning and other computational paradigms should aim to tackle these tasks using as few training data as people need, and also to evaluate models on a range of human-like generalizations beyond the one task on which the model was trained.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;note&#34;&gt;Note&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;The Atari games are deterministic, raising the possibility that a learner can succeed by memorizing long sequences of actions without learning to generalize&lt;/li&gt;
&lt;li&gt;Although it is unclear if the DQN also memorizes action sequences, an alternative “human starts” metric provides a stronger test of generalization&lt;/li&gt;
&lt;li&gt;Although connectionist networks have been used to model the general transition that children undergo between the ages of 3 and 4 regarding false belief,&lt;/li&gt;
&lt;li&gt;A new approach using convolutional “matching networks” achieves good one-shot classification performance when discriminating between characters from different alphabets&lt;/li&gt;
&lt;li&gt;In the interest of brevity, we do not discuss here another important vein of work linking neural circuits to variational approximations (Bastos et al. 2012), which have received less attention in the psychological literature.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;the-three-major-paradigms-of-ai-symbolicism-connectionism-behaviorism&#34;&gt;The three major paradigms of AI: symbolicism, connectionism, behaviorism&lt;/h1&gt;
&lt;h2 id=&#34;symbolicism&#34;&gt;Symbolicism&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;symbolicism=Logicism=Psychlogism=Computerism&lt;/li&gt;
&lt;li&gt;The main principles are the hypothesis of a physical symbol system (i.e., a system that manipulates symbols) and the principle of limited rationality.&lt;/li&gt;
&lt;li&gt;This category included most of the pioneers of artificial intelligence research.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;main-points&#34;&gt;Main points&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Symbolism is the foundation of human cognitive and mental activities&lt;/li&gt;
&lt;li&gt;Computers operate as physical systems that manipulate symbols&lt;/li&gt;
&lt;li&gt;Cognition involves performing computations on symbolic representations&lt;/li&gt;
&lt;li&gt;Computers can emulate or approximate human cognitive functions.&lt;/li&gt;
&lt;li&gt;In 1957, Newell, Simon and their colleagues created a program named &amp;ldquo;Logic Theorist&amp;rdquo; that could prove mathematical theorems. It verified 38 out of the first 52 propositions in Whitehead and Russell&amp;rsquo;s &amp;ldquo;Principia Mathematica&amp;rdquo;, and subsequently confirmed some more.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;connectionism&#34;&gt;Connectionism&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Connectionism is an approach in cognitive science that hopes to explain mental phenomena with artificial neural networks (ANNs).&lt;/li&gt;
&lt;li&gt;The central principle of connectionism is to use simple and often consistent units interconnected networks, to describe psychological phenomena. Different models of connections and unit forms may vary. For example, the units and connections of the network can represent neurons and synapses, as in the human brain.&lt;/li&gt;
&lt;li&gt;Lack of neuroscientific rationality.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;parallel-distributed-processing-pdp&#34;&gt;Parallel distributed processing, PDP&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;It is a method of artificial neural networks that emphasizes the parallelism of neural processing and the distributedness of neural representations, providing researchers with a general mathematical framework. It mainly includes eight aspects:&lt;/li&gt;
&lt;li&gt;A set of processing units, represented by a set of integers.&lt;/li&gt;
&lt;li&gt;The activation of the units, represented by a vector of time-dependent functions.&lt;/li&gt;
&lt;li&gt;The output function of the units, represented by a vector of activation functions.&lt;/li&gt;
&lt;li&gt;The connectivity pattern between units, represented by a real matrix indicating connection strengths.&lt;/li&gt;
&lt;li&gt;The propagation rule for propagating activation through connections, expressed as a function on unit outputs.&lt;/li&gt;
&lt;li&gt;The activation rule for combining inputs sent to units to determine new activations for units, represented by current activations and propagation functions.&lt;/li&gt;
&lt;li&gt;The learning rule for modifying connections based on experience, expressed as weight changes based on any number of variables.&lt;/li&gt;
&lt;li&gt;The environment that provides experience for the system, represented by a set of activation vectors for some subsets of units.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;experimentalism&#34;&gt;Experimentalism&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We can learn almost everything we know from the statistical patterns of sensory input.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;actionism&#34;&gt;Actionism&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Actionism (Behaviorism), also known as evolutionary or cybernetic school, is based on cybernetics and perception-action control systems.&lt;/li&gt;
&lt;li&gt;It argues that artificial intelligence originates from cybernetics.&lt;/li&gt;
&lt;li&gt;Cybernetic ideas became an important part of the zeitgeist in the 1940s and 1950s, influencing early artificial intelligence researchers. The cybernetics and self-organizing systems proposed by Wiener, McCulloch and others, as well as the engineering cybernetics and biological cybernetics proposed by Qian Xuesen and others, affected many fields. Cybernetics linked the working principles of neural systems with information theory, control theory, logic and computer science.&lt;/li&gt;
&lt;li&gt;And it assumes that all behaviors are produced by stimuli from the environment or shaped by individual life history; especially individual punishment, incentives, stimuli and behavioral outcomes caused by reinforcement in environment and life history. Therefore, although behaviorists generally accept that genetic factors are important determinants of behavior, they still pay more attention to environmental influences.&lt;/li&gt;
&lt;li&gt;The early research work focused on simulating human intelligent behavior and role in control processes, such as research on cybernetic systems such as self-optimization, self-adaptation, self-stabilization, self-organization and self-learning , And carried out research on &amp;ldquo;cybernetic animals&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;By the 1960s and 1970s , some progress had been made in these studies of cybernetic systems , sowing seeds for intelligent control and intelligent robots , which gave birth to intelligent control systems . Intelligent robot system .&lt;/li&gt;
&lt;li&gt;Behaviorism did not appear until the end of the 20th century as a new school of artificial intelligence , attracting many people&amp;rsquo;s interest . The representative author of this school was Brooks&amp;rsquo;s six-legged walking robot , which was regarded as a new generation of &amp;ldquo;cybernetic animals&amp;rdquo; . It is a control system based on perception-action mode to simulate insect behavior .&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
